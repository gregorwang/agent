"""
Chatlog MCP Server for BENEDICTJUN Agent

Provides MCP tools for intelligent chatlog retrieval:
- get_chatlog_stats: Get statistics about loaded chatlog
- search_person: Search messages from a specific person
- atomic tools for topic/keyword/semantic retrieval
"""

import os
import json
import time
import asyncio
from typing import Optional, Dict, Any, List, Tuple

from claude_agent_sdk import tool, create_sdk_mcp_server

from .loader import ChatlogLoader, get_chatlog_loader
from .searcher import ChatlogSearcher, SearchResult
from .cleaner import ChatlogCleaner, CleanerConfig
from .metadata_index_loader import MetadataIndexLoader, get_index_loader
from .semantic_index import get_semantic_index


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Tool Definitions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global instances
_chatlog_loader: Optional[ChatlogLoader] = None
_chatlog_searcher: Optional[ChatlogSearcher] = None
_chatlog_cleaner: Optional[ChatlogCleaner] = None

_CHATLOG_MAX_RETURN_CHARS = int(os.getenv("CHATLOG_MAX_RETURN_CHARS", "4000"))
_CHATLOG_INDEX_MAX_RESULTS = int(os.getenv("CHATLOG_INDEX_MAX_RESULTS", "200"))
_CHATLOG_INDEX_CONTEXT_BEFORE = int(os.getenv("CHATLOG_INDEX_CONTEXT_BEFORE", "2"))
_CHATLOG_INDEX_CONTEXT_AFTER = int(os.getenv("CHATLOG_INDEX_CONTEXT_AFTER", "2"))


def _cap_text(text: str, max_chars: int) -> str:
    """Cap tool output to prevent context overflow."""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n...(å·²æˆªæ–­)"

def _build_response(
    ok: bool,
    data: Dict[str, Any],
    meta: Optional[Dict[str, Any]] = None,
    is_error: bool = False
) -> Dict[str, Any]:
    payload = {
        "ok": ok,
        "data": data,
        "meta": meta or {}
    }
    text = json.dumps(payload, ensure_ascii=False, indent=2)
    return {
        "content": [{"type": "text", "text": text}],
        **({"is_error": True} if is_error else {})
    }


def _success(data: Dict[str, Any], meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    return _build_response(True, data, meta=meta, is_error=False)


def _error(message: str, meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    payload = {"error": message}
    return _build_response(False, payload, meta=meta, is_error=True)


def _parse_sender_content(content: str) -> Tuple[str, str]:
    if ": " in content:
        sender, body = content.split(": ", 1)
        return sender, body
    return "", content



def _get_loader() -> ChatlogLoader:
    """Create a fresh ChatlogLoader (no caching)."""
    return ChatlogLoader()


def _get_searcher(loader: ChatlogLoader) -> ChatlogSearcher:
    """Create a fresh ChatlogSearcher (no caching)."""
    return ChatlogSearcher(
        loader=loader,
        context_before=int(os.getenv("CHATLOG_CONTEXT_BEFORE", "2")),
        context_after=int(os.getenv("CHATLOG_CONTEXT_AFTER", "2"))
    )


def _get_cleaner() -> ChatlogCleaner:
    """Get the chatlog cleaner instance."""
    global _chatlog_cleaner
    if _chatlog_cleaner is None:
        config = CleanerConfig(
            model=os.getenv("CHATLOG_CLEANER_MODEL", "Gemini-2.5-Flash-Lite"),
            char_threshold=int(os.getenv("CHATLOG_CHAR_THRESHOLD", "3000")),
            target_chars=int(os.getenv("CHATLOG_TARGET_CHARS", "2000"))
        )
        _chatlog_cleaner = ChatlogCleaner(config)
    return _chatlog_cleaner


# Internal implementations (undecorated, for sync use)

async def _query_chatlog_indexed_impl(args: dict) -> dict:
    """
    Optimized query implementation using pre-built metadata index.
    
    Uses O(1) topic lookups instead of linear scans.
    Returns compact results to avoid context explosion.
    """
    import time
    import datetime
    
    query_start_time = time.time()
    
    def log(msg: str, phase: str = ""):
        """Log with millisecond timestamp."""
        ts = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG INDEX] [{ts}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    max_results = min(int(args.get("max_results", 20)), _CHATLOG_INDEX_MAX_RESULTS)
    
    log(f"ğŸš€ å¼€å§‹ç´¢å¼•æŸ¥è¯¢", "START")
    log(f"ğŸ“ é—®é¢˜: '{question}' (äººç‰©: {target_person or 'æ— '})")
    
    if not question:
        return {"content": [{"type": "text", "text": "é”™è¯¯ï¼šè¯·æä¾›æŸ¥è¯¢é—®é¢˜ã€‚"}]}
    
    # Load index (fast, O(1) lookups)
    index_loader = get_index_loader()
    if not index_loader.load_index():
        log("âš ï¸ ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°æ—§å®ç°", "FALLBACK")
        return await _query_chatlog_impl(args)
    
    log(
        f"âœ“ ç´¢å¼•å·²åŠ è½½: {len(index_loader.available_topics)} è¯é¢˜ | æ–‡ä»¶: {index_loader.index_path}"
    )
    
    # Step 1: Use cleaner to identify topics from question
    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    
    keywords = []
    if poe_client and poe_client.is_configured:
        log(f"ğŸ”‘ ä½¿ç”¨å°æ¨¡å‹è¯†åˆ«è¯é¢˜: {cleaner.config.model}")
        start = time.time()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, index_loader.available_topics
        )
        selected_topics = query_metadata.get("topics", [])
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   âœ“ è¯†åˆ«è¯é¢˜({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'æ— '}",
            "TOPICS"
        )
        log(f"   âœ“ å…³é”®è¯({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
        log(f"   âœ“ æ‰©å±•è€—æ—¶: {time.time()-start:.2f}s")
    else:
        log("âš ï¸ Poe APIæœªé…ç½®ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…")
        # Fallback: fuzzy match topics based on question keywords
        selected_topics = []
        if "å€Ÿ" in question or "é’±" in question:
            for topic in ("å€Ÿè´·", "é‡‘é’±"):
                if topic in index_loader.available_topics:
                    selected_topics.append(topic)
        if target_person and target_person in index_loader.available_topics:
            selected_topics.append(target_person)
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, index_loader.available_topics
        )
        selected_topics = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=selected_topics,
            available_topics=index_loader.available_topics
        )
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   âœ“ è¯†åˆ«è¯é¢˜({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'æ— '}",
            "TOPICS"
        )
        log(f"   âœ“ å…³é”®è¯({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
    
    # Step 2: Search by topics using index (O(1) per topic)
    log("ğŸ” Step 2: ç´¢å¼•æœç´¢...", "SEARCH")
    start = time.time()

    matched_lines = set()

    # Search by selected topics
    log(f"   âœ“ ä½¿ç”¨è¯é¢˜æ£€ç´¢: {len(selected_topics)} ä¸ª", "SEARCH")
    for topic in selected_topics:
        lines = index_loader.search_by_topic_exact(topic)
        matched_lines.update(lines[:max_results])
    
    # Only search by selected topics (keywords are used for topic selection only)

    # Semantic recall (optional, uses local embeddings cache)
    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum
    sem_top_k = int(os.getenv("CHATLOG_SEM_TOP_K", "50"))
    semantic_scores: Dict[int, float] = {}

    semantic_index = get_semantic_index()
    if semantic_index.is_available():
        log("   âœ“ è¯­ä¹‰æ£€ç´¢: å·²å¯ç”¨", "SEARCH")
        semantic_matches = semantic_index.search(question, top_k=sem_top_k)
        for line_num, score in semantic_matches:
            # Normalize cosine (-1..1) -> (0..1)
            semantic_scores[line_num] = max(0.0, min(1.0, (score + 1.0) / 2.0))
        log(
            f"   âœ“ è¯­ä¹‰å‘½ä¸­: {len(semantic_scores)} æ¡ | top_k={sem_top_k}",
            "SEARCH"
        )
    else:
        log("   âš ï¸ è¯­ä¹‰æ£€ç´¢: æœªå¯ç”¨ (ç¼ºå°‘ embeddings ç¼“å­˜)", "SEARCH")

    log(f"   âœ“ åŒ¹é…æ¶ˆæ¯: {len(matched_lines)} æ¡ ({time.time()-start:.2f}s)")
    
    if not matched_lines and not semantic_scores:
        log("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ¶ˆæ¯", "RESULT")
        return {
            "content": [{
                "type": "text",
                "text": f"æœªæ‰¾åˆ°ä¸ã€Œ{question}ã€ç›¸å…³çš„èŠå¤©è®°å½•ã€‚\næœç´¢è¯é¢˜: {', '.join(selected_topics)}"
            }]
        }
    
    # Step 3: Load messages with context (only matched lines)
    log("ğŸ“„ Step 3: åŠ è½½æ¶ˆæ¯...", "LOAD")
    start = time.time()
    
    combined_lines = set(matched_lines) | set(semantic_scores.keys())
    if not combined_lines:
        combined_lines = set(matched_lines)

    def _score(line_num: int) -> float:
        score = 0.0
        if line_num in matched_lines:
            score += kw_weight * 1.0
        if line_num in semantic_scores:
            score += sem_weight * semantic_scores[line_num]
        return score

    scored_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
    sorted_lines = scored_lines[:max_results]
    messages = index_loader.get_messages_by_lines(
        sorted_lines,
        context_before=_CHATLOG_INDEX_CONTEXT_BEFORE,
        context_after=_CHATLOG_INDEX_CONTEXT_AFTER
    )
    
    log(f"   âœ“ åŠ è½½æ¶ˆæ¯: {len(messages)} æ¡ ({time.time()-start:.2f}s)")
    
    # Step 4: Format raw results for cleaning (hit-centered windows)
    log("ğŸ“¦ Step 4: æ ¼å¼åŒ–ç»“æœ...", "FORMAT")

    message_map = {msg.get("line_number"): msg for msg in messages}
    filtered_samples: List[str] = []
    def _window_mentions_other_person(line_num: int) -> bool:
        if not target_person:
            return False
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        persons = set()
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            facts = (msg.get("metadata") or {}).get("facts") or {}
            for key in ("äººç‰©", "å¯¹è±¡", "ä¸»ä½“", "äºº"):
                val = facts.get(key)
                if isinstance(val, str) and val.strip():
                    persons.add(val.strip())
        if not persons:
            return False
        if target_person not in persons:
            if len(filtered_samples) < 3:
                filtered_samples.append(
                    f"è¡Œ{line_num} persons={', '.join(sorted(persons))}"
                )
            return True
        return False

    if target_person:
        filtered_lines = [
            ln for ln in sorted_lines if not _window_mentions_other_person(ln)
        ]
        if filtered_lines:
            log(
                f"   âœ“ å‘½ä¸­çª—å£è¿‡æ»¤(åŸºäºfacts): {len(sorted_lines)} -> {len(filtered_lines)}",
                "FORMAT"
            )
            if filtered_samples:
                log(
                    "   âœ“ è¿‡æ»¤ç¤ºä¾‹: " + " | ".join(filtered_samples),
                    "FORMAT"
                )
            sorted_lines = filtered_lines

    result_parts = []
    result_parts.append(f"## æŸ¥è¯¢: {question}")
    result_parts.append(f"è¯é¢˜: {', '.join(selected_topics) if selected_topics else 'æ— '}")
    result_parts.append(f"åŒ¹é…: {len(sorted_lines)} æ¡ | è¿”å›: {len(messages)} æ¡")
    result_parts.append(f"å…³é”®è¯: {', '.join(keywords[:20]) if keywords else 'æ— '}")

    for idx, line_num in enumerate(sorted_lines, 1):
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        result_parts.append(
            f"--- å‘½ä¸­çª—å£ {idx} (è¡Œ {line_num}, Â±{_CHATLOG_INDEX_CONTEXT_BEFORE}/{_CHATLOG_INDEX_CONTEXT_AFTER}) ---"
        )
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            raw = msg.get("content", "")
            sender = "æœªçŸ¥"
            body = raw
            if ": " in raw:
                sender, body = raw.split(": ", 1)
            ts = msg.get("timestamp", "")[:19]
            tag = "å‘½ä¸­" if msg.get("is_match") else "ä¸Šä¸‹æ–‡"
            confidence = "é«˜" if msg.get("is_match") else "ä¸­"
            result_parts.append(
                f"[{ts}] {sender}: {body} (è¡Œ{ln} {tag} ç½®ä¿¡åº¦:{confidence})"
            )

    raw_text = "\n".join(result_parts)

    # Step 5: Second-pass selection (skip if already window-formatted)
    log("ğŸ§¹ Step 5: äºŒæ¬¡ç­›é€‰æ¸…æ´—...", "CLEAN")
    if target_person:
        raw_text, attr_stats = await cleaner.entity_attribution(
            raw_text,
            target_person,
            question
        )
        if not attr_stats.get("skipped"):
            log(
                f"   âœ“ å®ä½“å½’å› : ä¿ç•™ {attr_stats.get('keep_count', 0)} æ¡ | "
                f"æ’é™¤ {attr_stats.get('exclude_count', 0)} æ¡",
                "CLEAN"
            )
    if "å‘½ä¸­çª—å£" in raw_text:
        cleaned = raw_text
        log("   è·³è¿‡æ¸…æ´—ï¼šå·²åŒ…å«å‘½ä¸­çª—å£ä¸Šä¸‹æ–‡(å·²åšå®ä½“å½’å› )", "CLEAN")
    else:
        if poe_client and poe_client.is_configured:
            log(f"   è°ƒç”¨ {cleaner.config.model} è¿›è¡ŒäºŒæ¬¡ç­›é€‰...", "CLEAN")
        else:
            log("   ä½¿ç”¨ç®€å•æˆªæ–­ (Poeæœªé…ç½®)", "CLEAN")
        cleaned = await cleaner.clean_results(
            formatted_text=raw_text,
            question=question,
            target_person=target_person,
            force=True
        )
    log(f"   âœ“ æ¸…æ´—å: {len(cleaned)} å­—ç¬¦", "CLEAN")

    result_text = _cap_text(cleaned, _CHATLOG_MAX_RETURN_CHARS)
    
    total_time = time.time() - query_start_time
    log(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå‡†å¤‡è¿”å›ç»™ Agent", "DONE")
    log(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s | è¿”å›å­—ç¬¦: {len(result_text)}", "TIMING")

    return {
        "content": [{"type": "text", "text": result_text}]
    }


async def _query_chatlog_composed_impl(args: dict) -> dict:
    """Compose atomic tools to answer a chatlog query."""
    import datetime

    def log(msg: str, phase: str = ""):
        ts = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG MCP] [{ts}]{phase_str} {msg}")

    question = args.get("question", "")
    target_person = args.get("target_person")
    requested_max = args.get("max_results", 100)
    max_results = min(max(1, int(requested_max)), _CHATLOG_INDEX_MAX_RESULTS)

    if not question:
        return {
            "content": [{"type": "text", "text": "é”™è¯¯ï¼šè¯·æä¾›æŸ¥è¯¢é—®é¢˜ã€‚"}],
            "is_error": True,
        }

    query_start_time = time.time()
    log(f"ğŸš€ å¼€å§‹ç»„åˆæŸ¥è¯¢", "START")
    log(f"ğŸ“ æ”¶åˆ°æŸ¥è¯¢: '{question}' (äººç‰©: {target_person or 'æ— '}, é™åˆ¶: {max_results})")

    index_loader = get_index_loader()
    if not index_loader.load_index():
        log("âš ï¸ ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°æ—§å®ç°", "FALLBACK")
        return await _query_chatlog_impl(args)

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    log("ğŸ”‘ Step 1: æŸ¥è¯¢æ‰©å±•", "EXPAND")
    available_topics = index_loader.available_topics
    if llm_available:
        keywords, metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        method = "llm"
    else:
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, available_topics
        )
        metadata = cleaner._fallback_metadata_classification(
            question, available_topics
        )
        metadata["topics"] = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=metadata.get("topics", []),
            available_topics=available_topics,
        )
        method = "rule_based"
    topics = metadata.get("topics", []) or []
    log(f"   âœ“ method: {method} | keywords: {len(keywords)} | topics: {len(topics)}", "EXPAND")

    log("ğŸ” Step 2: è¯é¢˜ç´¢å¼•æ£€ç´¢ + è¯­ä¹‰æ£€ç´¢(å¹¶è¡Œ)", "SEARCH")

    async def _search_topics() -> set[int]:
        lines: set[int] = set()
        for topic in topics:
            lines.update(index_loader.search_by_topic_exact(topic))
        return lines

    async def _search_semantic() -> Dict[int, float]:
        semantic_index = get_semantic_index()
        if not semantic_index.is_available():
            log("   âš ï¸ è¯­ä¹‰æ£€ç´¢æœªå¯ç”¨ (ç¼ºå°‘ embeddings ç¼“å­˜)", "SEARCH")
            return {}
        log("   âœ“ è¯­ä¹‰æ£€ç´¢å¯ç”¨", "SEARCH")
        sem_top_k = int(os.getenv("CHATLOG_SEM_TOP_K", "50"))
        semantic_matches = await asyncio.to_thread(
            semantic_index.search,
            question,
            top_k=sem_top_k
        )
        scores: Dict[int, float] = {}
        for line_num, score in semantic_matches:
            scores[line_num] = max(0.0, min(1.0, (score + 1.0) / 2.0))
        return scores

    matched_lines, semantic_scores = await asyncio.gather(
        _search_topics(),
        _search_semantic()
    )

    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum

    if not matched_lines and not semantic_scores:
        log("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ¶ˆæ¯", "RESULT")
        return {
            "content": [{
                "type": "text",
                "text": f"æœªæ‰¾åˆ°ä¸ã€Œ{question}ã€ç›¸å…³çš„èŠå¤©è®°å½•ã€‚"
            }]
        }

    def _score(line_num: int) -> float:
        score = 0.0
        if line_num in matched_lines:
            score += kw_weight
        if line_num in semantic_scores:
            score += sem_weight * semantic_scores[line_num]
        return score

    combined_lines = set(matched_lines) | set(semantic_scores.keys())
    ranked_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
    ranked_lines = ranked_lines[:max_results]

    log(f"ğŸ“„ Step 3: åŠ è½½æ¶ˆæ¯ (å‘½ä¸­: {len(ranked_lines)})", "LOAD")
    messages = index_loader.get_messages_by_lines(
        ranked_lines,
        context_before=_CHATLOG_INDEX_CONTEXT_BEFORE,
        context_after=_CHATLOG_INDEX_CONTEXT_AFTER,
    )

    formatted_messages: List[Dict[str, Any]] = []
    for msg in messages:
        raw = msg.get("content", "")
        sender, body = _parse_sender_content(raw)
        formatted_messages.append({
            "line": msg.get("line_number"),
            "time": (msg.get("timestamp") or "")[:19],
            "sender": sender or "æœªçŸ¥",
            "content": body,
            "is_match": bool(msg.get("is_match")),
        })

    if target_person:
        if llm_available:
            filter_result = await _filter_by_person_impl({
                "messages": formatted_messages,
                "target_person": target_person,
                "use_llm": True,
            })
        else:
            filter_result = await _filter_by_person_impl({
                "messages": formatted_messages,
                "target_person": target_person,
                "use_llm": False,
            })
        if filter_result.get("content"):
            try:
                payload = json.loads(filter_result["content"][0]["text"])
                formatted_messages = payload.get("data", {}).get("filtered_messages", formatted_messages)
            except (ValueError, KeyError, TypeError):
                pass

    log("ğŸ§¾ Step 4: æ ¼å¼åŒ–è¾“å‡º", "FORMAT")
    formatted_lines = []
    for m in formatted_messages:
        tag = "âœ“" if m.get("is_match") else ""
        line = f"[{m.get('time', '')}] {m.get('sender', 'æœªçŸ¥')}: {m.get('content', '')} {tag}".strip()
        formatted_lines.append(line)

    header = [
        "## èŠå¤©è®°å½•æ£€ç´¢ç»“æœ",
        f"**é—®é¢˜**: {question}",
    ]
    if target_person:
        header.append(f"**ç›®æ ‡äººç‰©**: {target_person}")
    header.append(f"**è¯é¢˜**: {', '.join(topics) if topics else 'æ— '}")
    header.append(f"**å…³é”®è¯**: {', '.join(keywords) if keywords else 'æ— '}")
    header.append(f"**å‘½ä¸­æ¶ˆæ¯**: {len(ranked_lines)}")
    header.append("---")

    combined_text = "\n".join(header + formatted_lines)
    if len(combined_text) > cleaner.config.char_threshold:
        log("ğŸ§¹ Step 5: æ¸…æ´—å‹ç¼©", "CLEAN")
        combined_text = await cleaner.clean_results(
            formatted_text=combined_text,
            question=question,
            target_person=target_person,
            force=True,
        )

    result_text = _cap_text(combined_text, _CHATLOG_MAX_RETURN_CHARS)
    total_time = time.time() - query_start_time
    log(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶ {total_time:.2f}s | è¿”å›å­—ç¬¦: {len(result_text)}", "DONE")
    return {"content": [{"type": "text", "text": result_text}]}

async def _query_chatlog_impl(args: dict) -> dict:
    """Internal implementation of query_chatlog."""
    import time
    import datetime
    
    query_start_time = time.time()  # Track total query time
    
    def log(msg: str, phase: str = ""):
        """Add log entry with millisecond timestamp."""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG MCP] [{timestamp}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    # Enforce a reasonable minimum max_results to avoid excessive outputs
    requested_max = args.get("max_results", 100)
    max_results = min(max(1, int(requested_max)), 100)  # Cap to avoid huge output
    
    log(f"ğŸš€ å¼€å§‹æŸ¥è¯¢å¾ªç¯", "START")
    log(f"ğŸ“ æ”¶åˆ°æŸ¥è¯¢: '{question}' (äººç‰©: {target_person or 'æ— '}, é™åˆ¶: {max_results})")
    
    if not question:
        return {
            "content": [{"type": "text", "text": "é”™è¯¯ï¼šè¯·æä¾›æŸ¥è¯¢é—®é¢˜ã€‚"}]
        }
    
    loader = _get_loader()
    searcher = _get_searcher(loader)
    cleaner = _get_cleaner()

    log("ğŸ“‚ æ­£åœ¨åŠ è½½èŠå¤©è®°å½•...")
    start = time.time()
    if not loader.load():
        return {
            "content": [{
                "type": "text",
                "text": f"é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶ {loader.file_path}"
            }]
        }
    log(f"âœ“ åŠ è½½å®Œæˆ: {loader.message_count} æ¡æ¶ˆæ¯ ({time.time()-start:.2f}s)")
    
    try:
        # Step 1: Expand query
        log("ğŸ”‘ Step 1: å…ƒæ•°æ®ä¸å…³é”®è¯æ‰©å±•...")
        start = time.time()
        
        # Check if Poe is configured
        poe_client = cleaner._get_poe_client()
        if poe_client and poe_client.is_configured:
            log(f"   ä½¿ç”¨å°æ¨¡å‹: {cleaner.config.model}")
        else:
            log("   âš ï¸ Poe APIæœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™fallback")
        
        available_topics = loader.get_unique_topics()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        topics = query_metadata.get("topics", [])
        log(f"   âœ“ æœç´¢å…³é”®è¯: {', '.join(keywords)}", "KEYWORDS")
        log(f"   âœ“ è¯é¢˜æ ‡ç­¾: {', '.join(topics) if topics else 'æ— '}", "TOPICS")
        log(
            f"   âœ“ æƒ…æ„Ÿ: {query_metadata.get('sentiment')}, "
            f"ä¿¡æ¯å¯†åº¦: {query_metadata.get('information_density')}"
        )
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(available_topics)}")
        log(f"   âœ“ æ‰©å±•è€—æ—¶: {time.time()-start:.2f}s")
        
        if not keywords:
            result_text = "é”™è¯¯ï¼šæ— æ³•ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚"
            log(f"âŒ æŸ¥è¯¢å¤±è´¥: æ— æ³•æå–å…³é”®è¯", "ERROR")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 2: Search with metadata
        log("ğŸ” Step 2: å…ƒæ•°æ®æœç´¢...")
        start = time.time()

        if target_person:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=target_person,
                max_results=max_results
            )
        else:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=None,
                max_results=max_results
            )
        
        log(f"   âœ“ åŒ¹é…æ¶ˆæ¯: {len(result.messages)} æ¡")
        log(f"   ä¸Šä¸‹æ–‡çª—å£: Â±{searcher.context_before}/{searcher.context_after} æ¡")
        log(f"   æœç´¢è€—æ—¶: {time.time()-start:.2f}s")
        
        if not result.messages:
            result_text = f"æœªæ‰¾åˆ°ä¸ã€Œ{question}ã€ç›¸å…³çš„èŠå¤©è®°å½•ã€‚\næœç´¢å…³é”®è¯: {', '.join(keywords)}"
            log(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ¶ˆæ¯", "RESULT")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 3: Format results
        log("ğŸ“„ Step 3: æ ¼å¼åŒ–ç»“æœ...")
        start = time.time()
        formatted = searcher.format_segmented_output(result, gap_threshold=10)
        original_len = len(formatted)
        log(f"   åŸå§‹å¤§å°: {original_len} å­—ç¬¦")
        log(f"   æ ¼å¼åŒ–è€—æ—¶: {time.time()-start:.2f}s")
        
        # Step 4: Second-pass selection (skip if already window-formatted)
        log("ğŸ§¹ Step 4: äºŒæ¬¡ç­›é€‰æ¸…æ´—...")
        start = time.time()

        if target_person:
            formatted, attr_stats = await cleaner.entity_attribution(
                formatted,
                target_person,
                question
            )
            if not attr_stats.get("skipped"):
                log(
                    f"   âœ“ å®ä½“å½’å› : ä¿ç•™ {attr_stats.get('keep_count', 0)} æ¡ | "
                    f"æ’é™¤ {attr_stats.get('exclude_count', 0)} æ¡"
                )
        if "å‘½ä¸­çª—å£" in formatted:
            cleaned = formatted
            log("   è·³è¿‡æ¸…æ´—ï¼šå·²åŒ…å«å‘½ä¸­çª—å£ä¸Šä¸‹æ–‡(å·²åšå®ä½“å½’å› )")
        else:
            if poe_client and poe_client.is_configured:
                log(f"   è°ƒç”¨ {cleaner.config.model} è¿›è¡ŒäºŒæ¬¡ç­›é€‰...")
            else:
                log("   ä½¿ç”¨ç®€å•æˆªæ–­ (Poeæœªé…ç½®)")

            cleaned = await cleaner.clean_results(
                formatted_text=formatted,
                question=question,
                target_person=target_person,
                force=True
            )
        log(f"   âœ“ æ¸…æ´—å: {len(cleaned)} å­—ç¬¦ ({time.time()-start:.2f}s)")

        
        # Build response header
        header = f"## èŠå¤©è®°å½•æ£€ç´¢ç»“æœ\n\n"
        header += f"**é—®é¢˜**: {question}\n"
        if target_person:
            header += f"**ç›®æ ‡äººç‰©**: {target_person}\n"
        header += f"**æœç´¢å…³é”®è¯**: {', '.join(keywords)}\n"
        header += (
            f"**æŸ¥è¯¢å…ƒæ•°æ®**: topics={query_metadata.get('topics', [])}, "
            f"sentiment={query_metadata.get('sentiment')}, "
            f"information_density={query_metadata.get('information_density')}\n"
        )
        header += f"**æ‰¾åˆ°æ¶ˆæ¯æ•°**: {len(result.messages)}\n"
        header += f"**åŸå§‹å¤§å°**: {original_len} å­—ç¬¦\n"
        header += f"**æœ€ç»ˆå¤§å°**: {len(cleaned)} å­—ç¬¦\n"
        header += f"---\n\n"
        
        # Log completion (no footer in return to reduce agent context)
        total_time = time.time() - query_start_time
        log(f"ğŸ“¦ æ­£åœ¨åŒ…è£…ç»“æœ...", "WRAP")
        log(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå‡†å¤‡è¿”å›ç»™ Agent", "DONE")
        log(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s", "TIMING")
        
        # Return without operation logs to reduce agent context size
        final_text = _cap_text(header + cleaned, _CHATLOG_MAX_RETURN_CHARS)
        return {
            "content": [{
                "type": "text",
                "text": final_text
            }]
        }
        
    except Exception as e:
        log(f"âŒ é”™è¯¯: {str(e)}", "ERROR")
        import traceback
        log(f"   {traceback.format_exc()}")
        return {
            "content": [{
                "type": "text",
                "text": f"æŸ¥è¯¢é”™è¯¯: {str(e)}"
            }]
        }



async def _get_chatlog_stats_impl(args: dict) -> dict:
    """Internal implementation of get_chatlog_stats."""
    loader = _get_loader()
    
    if not loader.is_loaded:
        if not loader.load():
            return {
                "content": [{
                    "type": "text",
                    "text": f"é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶ {loader.file_path}"
                }]
            }
    
    stats = loader.get_stats()
    
    output = "## èŠå¤©è®°å½•ç»Ÿè®¡\n\n"
    output += f"**æ–‡ä»¶è·¯å¾„**: {stats['file_path']}\n"
    output += f"**æ€»æ¶ˆæ¯æ•°**: {stats['total_messages']}\n"
    output += f"\n### å‘é€è€…ç»Ÿè®¡\n\n"
    
    for sender, count in stats['sender_message_counts'].items():
        output += f"- **{sender}**: {count} æ¡æ¶ˆæ¯\n"
    
    output = _cap_text(output, _CHATLOG_MAX_RETURN_CHARS)
    return {
        "content": [{
            "type": "text",
            "text": output
        }]
    }


async def _list_topics_impl(args: dict) -> dict:
    started = time.time()
    limit = int(args.get("limit", 100))
    pattern = (args.get("pattern") or "").strip()

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "æ— æ³•åŠ è½½ç´¢å¼•",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            }
        )

    topics = index_loader.available_topics
    if pattern:
        pattern_lower = pattern.lower()
        topics = [t for t in topics if pattern_lower in t.lower()]

    topics_sorted = sorted(topics)
    data = {
        "topics": topics_sorted[:limit],
        "total_count": len(index_loader.available_topics),
        "returned_count": min(len(topics_sorted), limit),
        "pattern": pattern or None,
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _search_by_topics_impl(args: dict) -> dict:
    started = time.time()
    topics = args.get("topics") or []
    max_results = min(int(args.get("max_results", 100)), 500)

    if not topics:
        return _error("è¯·æä¾›è‡³å°‘ä¸€ä¸ªè¯é¢˜", meta={"source": "index"})

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "æ— æ³•åŠ è½½ç´¢å¼•",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            }
        )

    all_lines: set[int] = set()
    breakdown: Dict[str, int] = {}
    for topic in topics:
        lines = index_loader.search_by_topic_exact(topic)
        breakdown[topic] = len(lines)
        all_lines.update(lines)

    line_numbers = sorted(all_lines)[:max_results]
    data = {
        "line_numbers": line_numbers,
        "total_matches": len(all_lines),
        "topic_breakdown": breakdown,
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _search_by_keywords_impl(args: dict) -> dict:
    started = time.time()
    keywords = args.get("keywords") or []
    target_person = args.get("target_person")
    max_results = min(int(args.get("max_results", 100)), 500)
    match_all = bool(args.get("match_all", False))

    if not keywords:
        return _error("è¯·æä¾›è‡³å°‘ä¸€ä¸ªå…³é”®è¯", meta={"source": "scan"})

    loader = _get_loader()
    if not loader.load():
        return _error(
            "æ— æ³•åŠ è½½èŠå¤©è®°å½•",
            meta={
                "available": False,
                "source": "scan",
                "timing_ms": int((time.time() - started) * 1000)
            }
        )

    normalized_keywords = [k.lower() for k in keywords if isinstance(k, str)]
    keyword_hits = {k: 0 for k in normalized_keywords}
    matched_lines: List[int] = []

    target_lower = target_person.lower() if isinstance(target_person, str) else None

    for msg in loader.get_all_messages():
        if target_lower and target_lower not in (msg.sender or "").lower():
            continue
        content_lower = msg.content.lower()
        matches = [kw for kw in normalized_keywords if kw and kw in content_lower]
        if (match_all and len(matches) == len(normalized_keywords)) or (not match_all and matches):
            matched_lines.append(msg.line_number)
            for kw in matches:
                keyword_hits[kw] += 1

    data = {
        "line_numbers": matched_lines[:max_results],
        "total_matches": len(matched_lines),
        "keyword_breakdown": keyword_hits,
        "person_filter": target_person,
        "match_all": match_all,
    }
    meta = {
        "available": True,
        "source": "scan",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _load_messages_impl(args: dict) -> dict:
    started = time.time()
    line_numbers = args.get("line_numbers") or []
    context_before = min(int(args.get("context_before", 0)), 10)
    context_after = min(int(args.get("context_after", 0)), 10)
    include_metadata = bool(args.get("include_metadata", False))

    if not line_numbers:
        return _error("è¯·æä¾›è¡Œå·åˆ—è¡¨", meta={"source": "index"})

    cleaned_lines = []
    for ln in line_numbers[:200]:
        try:
            cleaned_lines.append(int(ln))
        except (TypeError, ValueError):
            continue
    if not cleaned_lines:
        return _error("è¡Œå·æ ¼å¼æ— æ•ˆ", meta={"source": "index"})

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "æ— æ³•åŠ è½½ç´¢å¼•",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            }
        )

    messages = index_loader.get_messages_by_lines(
        cleaned_lines,
        context_before=context_before,
        context_after=context_after,
    )
    result = []
    for msg in messages:
        raw = msg.get("content", "")
        sender, body = _parse_sender_content(raw)
        item = {
            "line": msg.get("line_number"),
            "time": (msg.get("timestamp") or "")[:19],
            "sender": sender or "æœªçŸ¥",
            "content": body,
            "is_match": bool(msg.get("is_match")),
        }
        if include_metadata:
            item["metadata"] = msg.get("metadata", {})
        result.append(item)

    data = {
        "messages": result,
        "count": len(result),
        "context": f"Â±{context_before}/{context_after}",
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _expand_query_impl(args: dict) -> dict:
    started = time.time()
    question = args.get("question", "")
    target_person = args.get("target_person")
    use_llm = bool(args.get("use_llm", True))

    if not question:
        return _error("è¯·æä¾›é—®é¢˜", meta={"source": "llm"})

    index_loader = get_index_loader()
    available_topics = index_loader.available_topics if index_loader.load_index() else []

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    if use_llm and llm_available:
        keywords, metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        method = "llm"
        model = cleaner.config.model
        llm_used = True
    else:
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, available_topics
        )
        metadata = cleaner._fallback_metadata_classification(
            question, available_topics
        )
        metadata["topics"] = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=metadata.get("topics", []),
            available_topics=available_topics,
        )
        method = "rule_based"
        model = None
        llm_used = False

    data = {
        "keywords": keywords,
        "topics": metadata.get("topics", []),
        "sentiment": metadata.get("sentiment"),
        "information_density": metadata.get("information_density"),
        "method": method,
        "model": model,
        "llm_available": llm_available,
    }
    meta = {
        "available": True,
        "source": "llm" if method == "llm" else "rule_based",
        "llm_used": llm_used,
        "model": model,
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _search_semantic_impl(args: dict) -> dict:
    started = time.time()
    query = args.get("query", "")
    top_k = min(int(args.get("top_k", 50)), 200)

    if not query:
        return _error("è¯·æä¾›æŸ¥è¯¢æ–‡æœ¬", meta={"source": "semantic"})

    semantic_index = get_semantic_index()
    if not semantic_index.is_available():
        data = {
            "available": False,
            "reason": "ç¼ºå°‘ embeddings ç¼“å­˜æ–‡ä»¶",
            "suggestion": "è¿è¡Œ python -m src.chatlog.semantic_index æ„å»ºç´¢å¼•",
            "results": [],
        }
        meta = {
            "available": False,
            "source": "semantic",
            "timing_ms": int((time.time() - started) * 1000),
        }
        return _success(data, meta=meta)

    raw_results = semantic_index.search(query, top_k=top_k)
    results = [
        {"line": ln, "score": round((score + 1.0) / 2.0, 4)}
        for ln, score in raw_results
    ]
    data = {
        "available": True,
        "results": results,
        "count": len(results),
        "query": query,
    }
    meta = {
        "available": True,
        "source": "semantic",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _filter_by_person_impl(args: dict) -> dict:
    started = time.time()
    messages = args.get("messages") or []
    target_person = args.get("target_person", "")
    use_llm = bool(args.get("use_llm", True))

    if not messages:
        return _error("è¯·æä¾›æ¶ˆæ¯åˆ—è¡¨", meta={"source": "llm"})
    if not target_person:
        return _error("è¯·æä¾›ç›®æ ‡äººç‰©", meta={"source": "llm"})

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    kept: List[Dict[str, Any]] = []
    excluded: List[Dict[str, Any]] = []

    if use_llm and llm_available:
        formatted_lines = [
            f"[{m.get('time', '')}] {m.get('sender', 'æœªçŸ¥')}: {m.get('content', '')}"
            for m in messages
        ]
        formatted_text = "\n".join(formatted_lines)
        filtered_text, attr_stats = await cleaner.entity_attribution(
            formatted_text,
            target_person,
            ""
        )
        filtered_set = {line.strip() for line in filtered_text.splitlines() if line.strip()}
        for msg, line in zip(messages, formatted_lines):
            if line.strip() in filtered_set:
                kept.append(msg)
            else:
                excluded.append(msg)
        method = "llm_attribution"
        meta = {
            "available": True,
            "source": "llm",
            "llm_used": True,
            "model": cleaner.config.model,
            "timing_ms": int((time.time() - started) * 1000),
            "attr_stats": attr_stats,
        }
    else:
        for msg in messages:
            content = msg.get("content", "")
            sender = msg.get("sender", "")
            if target_person in content or target_person == sender:
                kept.append(msg)
            else:
                excluded.append(msg)
        method = "name_match"
        meta = {
            "available": True,
            "source": "rule_based",
            "llm_used": False,
            "model": None,
            "timing_ms": int((time.time() - started) * 1000),
        }

    data = {
        "filtered_messages": kept,
        "kept_count": len(kept),
        "excluded_count": len(excluded),
        "method": method,
        "target_person": target_person,
        "llm_available": llm_available,
    }
    return _success(data, meta=meta)


async def _format_messages_impl(args: dict) -> dict:
    started = time.time()
    messages = args.get("messages") or []
    fmt = args.get("format", "compact")
    max_chars = min(int(args.get("max_chars", _CHATLOG_MAX_RETURN_CHARS)), 10000)

    if not messages:
        return _error("è¯·æä¾›æ¶ˆæ¯åˆ—è¡¨", meta={"source": "format"})

    lines: List[str] = []
    if fmt == "timeline":
        lines.append("## æ—¶é—´çº¿")
        current_date = None
        for m in messages:
            time_str = m.get("time", "")
            date = time_str[:10] if time_str else "æœªçŸ¥æ—¥æœŸ"
            if date != current_date:
                current_date = date
                lines.append("")
                lines.append(f"### {date}")
            clock = time_str[11:16] if len(time_str) >= 16 else ""
            sender = m.get("sender", "æœªçŸ¥")
            content = m.get("content", "")
            lines.append(f"- **{clock}** [{sender}]: {content}")
    elif fmt == "detailed":
        for m in messages:
            lines.append("---")
            lines.append(f"**è¡Œå·**: {m.get('line')}")
            lines.append(f"**æ—¶é—´**: {m.get('time')}")
            lines.append(f"**å‘é€è€…**: {m.get('sender')}")
            lines.append(f"**å†…å®¹**: {m.get('content')}")
    else:
        for m in messages:
            tag = "âœ“" if m.get("is_match") else ""
            line = f"[{m.get('time', '')}] {m.get('sender', 'æœªçŸ¥')}: {m.get('content', '')} {tag}".strip()
            lines.append(line)

    text = "\n".join(lines)
    truncated = len(text) > max_chars
    if truncated:
        text = _cap_text(text, max_chars)

    data = {
        "text": text,
        "chars": len(text),
        "messages": len(messages),
        "format": fmt,
        "truncated": truncated,
    }
    meta = {
        "available": True,
        "source": "format",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta)


async def _search_person_impl(args: dict) -> dict:
    """Internal implementation of search_person."""
    person = args.get("person", "")
    include_context = args.get("include_context", True)
    
    if not person:
        return {
            "content": [{
                "type": "text",
                "text": "é”™è¯¯ï¼šè¯·æä¾›äººç‰©åç§°ã€‚"
            }]
        }
    
    loader = _get_loader()
    
    if not loader.is_loaded:
        if not loader.load():
            return {
                "content": [{
                    "type": "text",
                    "text": "é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶"
                }]
            }
    
    # Get messages from this person
    person_messages = loader.get_messages_by_sender(person)
    
    if not person_messages:
        return {
            "content": [{
                "type": "text",
                "text": f"æœªæ‰¾åˆ°ã€Œ{person}ã€çš„æ¶ˆæ¯è®°å½•ã€‚"
            }]
        }
    
    # Build result
    output = f"## å…³äºã€Œ{person}ã€çš„æ¶ˆæ¯è®°å½•\n\n"
    output += f"**æ€»æ¶ˆæ¯æ•°**: {len(person_messages)}\n"
    output += f"---\n\n"
    
    if include_context:
        # Get context for each message
        all_line_numbers = set()
        for msg in person_messages[:50]:  # Limit to avoid too much data
            for ln in range(max(1, msg.line_number - 2), msg.line_number + 3):
                all_line_numbers.add(ln)
        
        for ln in sorted(all_line_numbers):
            msg = loader.get_message(ln)
            if msg:
                output += msg.format_simple() + "\n"
    else:
        # Just the person's messages
        for msg in person_messages[:100]:
            output += msg.format_simple() + "\n"
    
    return {
        "content": [{
            "type": "text",
            "text": output
        }]
    }


# Tool-decorated versions (for MCP)
@tool(
    "get_chatlog_stats",
    "è·å–èŠå¤©è®°å½•çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»æ¶ˆæ¯æ•°ã€å‘é€è€…åˆ—è¡¨ç­‰ã€‚",
    {}
)
async def get_chatlog_stats(args: dict) -> dict:
    """Get statistics about the loaded chatlog."""
    return await _get_chatlog_stats_impl(args)


@tool(
    "search_person",
    "æœç´¢ç‰¹å®šäººç‰©çš„æ‰€æœ‰ç›¸å…³æ¶ˆæ¯è®°å½•ã€‚",
    {
        "person": str,            # äººç‰©åç§°
        "include_context": bool   # å¯é€‰ï¼šæ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤trueï¼‰
    }
)
async def search_person(args: dict) -> dict:
    """Search for all messages related to a specific person."""
    return await _search_person_impl(args)


@tool(
    "list_topics",
    "åˆ—å‡ºèŠå¤©è®°å½•ç´¢å¼•ä¸­çš„è¯é¢˜æ ‡ç­¾ã€‚",
    {
        "limit": int,
        "pattern": str
    }
)
async def list_topics(args: dict) -> dict:
    return await _list_topics_impl(args)


@tool(
    "search_by_topics",
    "æ ¹æ®è¯é¢˜æ ‡ç­¾æ£€ç´¢æ¶ˆæ¯è¡Œå·ã€‚",
    {
        "topics": list,
        "max_results": int
    }
)
async def search_by_topics(args: dict) -> dict:
    return await _search_by_topics_impl(args)


@tool(
    "search_by_keywords",
    "æ ¹æ®å…³é”®è¯å…¨æ–‡æ£€ç´¢æ¶ˆæ¯è¡Œå·ã€‚å¯é™å®šå‘é€è€…ã€‚",
    {
        "keywords": list,
        "target_person": str,
        "max_results": int,
        "match_all": bool
    }
)
async def search_by_keywords(args: dict) -> dict:
    return await _search_by_keywords_impl(args)


@tool(
    "load_messages",
    "æ ¹æ®è¡Œå·åŠ è½½æ¶ˆæ¯å†…å®¹ï¼Œå¯é€‰åŒ…å«ä¸Šä¸‹æ–‡ä¸å…ƒæ•°æ®ã€‚",
    {
        "line_numbers": list,
        "context_before": int,
        "context_after": int,
        "include_metadata": bool
    }
)
async def load_messages(args: dict) -> dict:
    return await _load_messages_impl(args)


@tool(
    "expand_query",
    "å°†é—®é¢˜æ‰©å±•ä¸ºå…³é”®è¯å’Œè¯é¢˜æ ‡ç­¾ï¼ˆLLM å¯é€‰ï¼‰ã€‚",
    {
        "question": str,
        "target_person": str,
        "use_llm": bool
    }
)
async def expand_query(args: dict) -> dict:
    return await _expand_query_impl(args)


@tool(
    "search_semantic",
    "ä½¿ç”¨è¯­ä¹‰å‘é‡å¬å›ç›¸ä¼¼æ¶ˆæ¯ã€‚",
    {
        "query": str,
        "top_k": int
    }
)
async def search_semantic(args: dict) -> dict:
    return await _search_semantic_impl(args)


@tool(
    "filter_by_person",
    "è¿‡æ»¤æ¶ˆæ¯ï¼Œç¡®ä¿å†…å®¹ä¸ç›®æ ‡äººç‰©ç›¸å…³ã€‚",
    {
        "messages": list,
        "target_person": str,
        "use_llm": bool
    }
)
async def filter_by_person(args: dict) -> dict:
    return await _filter_by_person_impl(args)


@tool(
    "format_messages",
    "æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨ä¸ºæ–‡æœ¬ã€‚",
    {
        "messages": list,
        "format": str,
        "max_chars": int
    }
)
async def format_messages(args: dict) -> dict:
    return await _format_messages_impl(args)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Server Creation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_chatlog_mcp_server(chatlog_path: Optional[str] = None):
    """
    Create the Chatlog MCP server.
    
    Args:
        chatlog_path: Optional path to chatlog JSONL file
        
    Returns:
        An MCP server that can be passed to ClaudeAgentOptions.mcp_servers
    """
    global _chatlog_loader
    
    # Initialize loader with custom path if provided
    if chatlog_path:
        _chatlog_loader = ChatlogLoader(chatlog_path)
    
    return create_sdk_mcp_server(
        name="chatlog",
        version="1.0.0",
        tools=[
            get_chatlog_stats,
            search_person,
            list_topics,
            search_by_topics,
            search_by_keywords,
            load_messages,
            expand_query,
            search_semantic,
            filter_by_person,
            format_messages,
        ]
    )


def get_chatlog_tools_info() -> List[Dict[str, str]]:
    """Get information about available chatlog tools for documentation."""      
    return [
        {
            "name": "mcp__chatlog__get_chatlog_stats",
            "description": "è·å–èŠå¤©è®°å½•ç»Ÿè®¡ä¿¡æ¯",
            "usage": "æŸ¥çœ‹èŠå¤©è®°å½•æ¦‚å†µæ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__search_person",
            "description": "æœç´¢ç‰¹å®šäººç‰©çš„æ¶ˆæ¯è®°å½•",
            "usage": "éœ€è¦äº†è§£æŸä¸ªäººçš„å†å²æ¶ˆæ¯æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__list_topics",
            "description": "åˆ—å‡ºèŠå¤©è®°å½•ç´¢å¼•ä¸­çš„è¯é¢˜æ ‡ç­¾",
            "usage": "äº†è§£å¯ç”¨è¯é¢˜èŒƒå›´æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__search_by_topics",
            "description": "æŒ‰è¯é¢˜æ ‡ç­¾è¿”å›åŒ¹é…è¡Œå·",
            "usage": "å·²æœ‰è¯é¢˜æ ‡ç­¾æ—¶å¿«é€Ÿç¼©å°èŒƒå›´"
        },
        {
            "name": "mcp__chatlog__search_by_keywords",
            "description": "æŒ‰å…³é”®è¯æ£€ç´¢æ¶ˆæ¯è¡Œå·",
            "usage": "éœ€è¦ç²¾ç¡®å…³é”®è¯åŒ¹é…æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__load_messages",
            "description": "æŒ‰è¡Œå·åŠ è½½æ¶ˆæ¯ä¸ä¸Šä¸‹æ–‡",
            "usage": "åœ¨å·²æœ‰è¡Œå·æ—¶è·å–åŸå§‹å†…å®¹"
        },
        {
            "name": "mcp__chatlog__expand_query",
            "description": "å°†é—®é¢˜æ‰©å±•ä¸ºå…³é”®è¯å’Œè¯é¢˜",
            "usage": "é—®é¢˜æ¨¡ç³Šæˆ–éœ€è¦è¯é¢˜å»ºè®®æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__search_semantic",
            "description": "è¯­ä¹‰å‘é‡å¬å›ç›¸ä¼¼æ¶ˆæ¯",
            "usage": "è¯­ä¹‰æ£€ç´¢æˆ–å®½æ³›é—®é¢˜å¬å›æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__filter_by_person",
            "description": "è¿‡æ»¤ä¸ç›®æ ‡äººç‰©ç›¸å…³çš„æ¶ˆæ¯",
            "usage": "éœ€è¦ä¿è¯äººåå½’å› æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__format_messages",
            "description": "æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨ä¸ºæ–‡æœ¬",
            "usage": "éœ€è¦å›ºå®šæ ¼å¼è¾“å‡ºæ—¶è°ƒç”¨"
        }
    ]


async def close_chatlog_clients() -> None:
    """Close any chatlog-related async clients (e.g., Poe session)."""
    global _chatlog_cleaner
    if _chatlog_cleaner is None:
        return
    try:
        await _chatlog_cleaner.close()
    except Exception:
        pass
    _chatlog_cleaner = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Synchronous API for direct usage
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compose_chatlog_query_sync(
    question: str,
    target_person: Optional[str] = None,
    max_results: int = 100
) -> str:
    """Synchronous wrapper for composed chatlog query (internal use)."""
    args = {
        "question": question,
        "target_person": target_person,
        "max_results": max_results
    }
    
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_query_chatlog_composed_impl(args))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)


def get_chatlog_stats_sync() -> str:
    """Synchronous wrapper for get_chatlog_stats."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_get_chatlog_stats_impl({}))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)

