"""
Chatlog MCP Server for BENEDICTJUN Agent

Provides MCP tools for intelligent chatlog retrieval:
- query_chatlog: Main query tool with keyword expansion and cleaning
- get_chatlog_stats: Get statistics about loaded chatlog
- search_person: Search messages from a specific person
"""

import os
import asyncio
from typing import Optional, Dict, Any, List

from claude_agent_sdk import tool, create_sdk_mcp_server

from .loader import ChatlogLoader, get_chatlog_loader
from .searcher import ChatlogSearcher, SearchResult
from .cleaner import ChatlogCleaner, CleanerConfig
from .metadata_index_loader import MetadataIndexLoader, get_index_loader
from .semantic_index import get_semantic_index


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Tool Definitions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global instances
_chatlog_loader: Optional[ChatlogLoader] = None
_chatlog_searcher: Optional[ChatlogSearcher] = None
_chatlog_cleaner: Optional[ChatlogCleaner] = None

_CHATLOG_MAX_RETURN_CHARS = int(os.getenv("CHATLOG_MAX_RETURN_CHARS", "4000"))
_CHATLOG_INDEX_MAX_RESULTS = int(os.getenv("CHATLOG_INDEX_MAX_RESULTS", "200"))
_CHATLOG_INDEX_CONTEXT_BEFORE = int(os.getenv("CHATLOG_INDEX_CONTEXT_BEFORE", "2"))
_CHATLOG_INDEX_CONTEXT_AFTER = int(os.getenv("CHATLOG_INDEX_CONTEXT_AFTER", "2"))


def _cap_text(text: str, max_chars: int) -> str:
    """Cap tool output to prevent context overflow."""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n...(å·²æˆªæ–­)"



def _get_loader() -> ChatlogLoader:
    """Create a fresh ChatlogLoader (no caching)."""
    return ChatlogLoader()


def _get_searcher(loader: ChatlogLoader) -> ChatlogSearcher:
    """Create a fresh ChatlogSearcher (no caching)."""
    return ChatlogSearcher(
        loader=loader,
        context_before=int(os.getenv("CHATLOG_CONTEXT_BEFORE", "2")),
        context_after=int(os.getenv("CHATLOG_CONTEXT_AFTER", "2"))
    )


def _get_cleaner() -> ChatlogCleaner:
    """Get the chatlog cleaner instance."""
    global _chatlog_cleaner
    if _chatlog_cleaner is None:
        config = CleanerConfig(
            model=os.getenv("CHATLOG_CLEANER_MODEL", "Gemini-2.5-Flash-Lite"),
            char_threshold=int(os.getenv("CHATLOG_CHAR_THRESHOLD", "3000")),
            target_chars=int(os.getenv("CHATLOG_TARGET_CHARS", "2000"))
        )
        _chatlog_cleaner = ChatlogCleaner(config)
    return _chatlog_cleaner


# Internal implementations (undecorated, for sync use)

async def _query_chatlog_indexed_impl(args: dict) -> dict:
    """
    Optimized query implementation using pre-built metadata index.
    
    Uses O(1) topic lookups instead of linear scans.
    Returns compact results to avoid context explosion.
    """
    import time
    import datetime
    
    query_start_time = time.time()
    
    def log(msg: str, phase: str = ""):
        """Log with millisecond timestamp."""
        ts = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG INDEX] [{ts}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    max_results = min(int(args.get("max_results", 20)), _CHATLOG_INDEX_MAX_RESULTS)
    
    log(f"ðŸš€ å¼€å§‹ç´¢å¼•æŸ¥è¯¢", "START")
    log(f"ðŸ“ é—®é¢˜: '{question}' (äººç‰©: {target_person or 'æ— '})")
    
    if not question:
        return {"content": [{"type": "text", "text": "é”™è¯¯ï¼šè¯·æä¾›æŸ¥è¯¢é—®é¢˜ã€‚"}]}
    
    # Load index (fast, O(1) lookups)
    index_loader = get_index_loader()
    if not index_loader.load_index():
        log("âš ï¸ ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå›žé€€åˆ°æ—§å®žçŽ°", "FALLBACK")
        return await _query_chatlog_impl(args)
    
    log(
        f"âœ“ ç´¢å¼•å·²åŠ è½½: {len(index_loader.available_topics)} è¯é¢˜ | æ–‡ä»¶: {index_loader.index_path}"
    )
    
    # Step 1: Use cleaner to identify topics from question
    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    
    keywords = []
    if poe_client and poe_client.is_configured:
        log(f"ðŸ”‘ ä½¿ç”¨å°æ¨¡åž‹è¯†åˆ«è¯é¢˜: {cleaner.config.model}")
        start = time.time()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, index_loader.available_topics
        )
        selected_topics = query_metadata.get("topics", [])
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   âœ“ è¯†åˆ«è¯é¢˜({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'æ— '}",
            "TOPICS"
        )
        log(f"   âœ“ å…³é”®è¯({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
        log(f"   âœ“ æ‰©å±•è€—æ—¶: {time.time()-start:.2f}s")
    else:
        log("âš ï¸ Poe APIæœªé…ç½®ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…")
        # Fallback: fuzzy match topics based on question keywords
        selected_topics = []
        if "å€Ÿ" in question or "é’±" in question:
            for topic in ("å€Ÿè´·", "é‡‘é’±"):
                if topic in index_loader.available_topics:
                    selected_topics.append(topic)
        if target_person and target_person in index_loader.available_topics:
            selected_topics.append(target_person)
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, index_loader.available_topics
        )
        selected_topics = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=selected_topics,
            available_topics=index_loader.available_topics
        )
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   âœ“ è¯†åˆ«è¯é¢˜({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'æ— '}",
            "TOPICS"
        )
        log(f"   âœ“ å…³é”®è¯({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
    
    # Step 2: Search by topics using index (O(1) per topic)
    log("ðŸ” Step 2: ç´¢å¼•æœç´¢...", "SEARCH")
    start = time.time()

    matched_lines = set()

    # Search by selected topics
    log(f"   âœ“ ä½¿ç”¨è¯é¢˜æ£€ç´¢: {len(selected_topics)} ä¸ª", "SEARCH")
    for topic in selected_topics:
        lines = index_loader.search_by_topic_exact(topic)
        matched_lines.update(lines[:max_results])
    
    # Only search by selected topics (keywords are used for topic selection only)

    # Semantic recall (optional, uses local embeddings cache)
    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum
    sem_top_k = int(os.getenv("CHATLOG_SEM_TOP_K", "50"))
    semantic_scores: Dict[int, float] = {}

    semantic_index = get_semantic_index()
    if semantic_index.is_available():
        log("   âœ“ è¯­ä¹‰æ£€ç´¢: å·²å¯ç”¨", "SEARCH")
        semantic_matches = semantic_index.search(question, top_k=sem_top_k)
        for line_num, score in semantic_matches:
            # Normalize cosine (-1..1) -> (0..1)
            semantic_scores[line_num] = max(0.0, min(1.0, (score + 1.0) / 2.0))
        log(
            f"   âœ“ è¯­ä¹‰å‘½ä¸­: {len(semantic_scores)} æ¡ | top_k={sem_top_k}",
            "SEARCH"
        )
    else:
        log("   âš ï¸ è¯­ä¹‰æ£€ç´¢: æœªå¯ç”¨ (ç¼ºå°‘ embeddings ç¼“å­˜)", "SEARCH")

    log(f"   âœ“ åŒ¹é…æ¶ˆæ¯: {len(matched_lines)} æ¡ ({time.time()-start:.2f}s)")
    
    if not matched_lines and not semantic_scores:
        log("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ¶ˆæ¯", "RESULT")
        return {
            "content": [{
                "type": "text",
                "text": f"æœªæ‰¾åˆ°ä¸Žã€Œ{question}ã€ç›¸å…³çš„èŠå¤©è®°å½•ã€‚\næœç´¢è¯é¢˜: {', '.join(selected_topics)}"
            }]
        }
    
    # Step 3: Load messages with context (only matched lines)
    log("ðŸ“„ Step 3: åŠ è½½æ¶ˆæ¯...", "LOAD")
    start = time.time()
    
    combined_lines = set(matched_lines) | set(semantic_scores.keys())
    if not combined_lines:
        combined_lines = set(matched_lines)

    def _score(line_num: int) -> float:
        score = 0.0
        if line_num in matched_lines:
            score += kw_weight * 1.0
        if line_num in semantic_scores:
            score += sem_weight * semantic_scores[line_num]
        return score

    scored_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
    sorted_lines = scored_lines[:max_results]
    messages = index_loader.get_messages_by_lines(
        sorted_lines,
        context_before=_CHATLOG_INDEX_CONTEXT_BEFORE,
        context_after=_CHATLOG_INDEX_CONTEXT_AFTER
    )
    
    log(f"   âœ“ åŠ è½½æ¶ˆæ¯: {len(messages)} æ¡ ({time.time()-start:.2f}s)")
    
    # Step 4: Format raw results for cleaning (hit-centered windows)
    log("ðŸ“¦ Step 4: æ ¼å¼åŒ–ç»“æžœ...", "FORMAT")

    message_map = {msg.get("line_number"): msg for msg in messages}
    filtered_samples: List[str] = []
    def _window_mentions_other_person(line_num: int) -> bool:
        if not target_person:
            return False
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        persons = set()
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            facts = (msg.get("metadata") or {}).get("facts") or {}
            for key in ("äººç‰©", "å¯¹è±¡", "ä¸»ä½“", "äºº"):
                val = facts.get(key)
                if isinstance(val, str) and val.strip():
                    persons.add(val.strip())
        if not persons:
            return False
        if target_person not in persons:
            if len(filtered_samples) < 3:
                filtered_samples.append(
                    f"è¡Œ{line_num} persons={', '.join(sorted(persons))}"
                )
            return True
        return False

    if target_person:
        filtered_lines = [
            ln for ln in sorted_lines if not _window_mentions_other_person(ln)
        ]
        if filtered_lines:
            log(
                f"   âœ“ å‘½ä¸­çª—å£è¿‡æ»¤(åŸºäºŽfacts): {len(sorted_lines)} -> {len(filtered_lines)}",
                "FORMAT"
            )
            if filtered_samples:
                log(
                    "   âœ“ è¿‡æ»¤ç¤ºä¾‹: " + " | ".join(filtered_samples),
                    "FORMAT"
                )
            sorted_lines = filtered_lines

    result_parts = []
    result_parts.append(f"## æŸ¥è¯¢: {question}")
    result_parts.append(f"è¯é¢˜: {', '.join(selected_topics) if selected_topics else 'æ— '}")
    result_parts.append(f"åŒ¹é…: {len(sorted_lines)} æ¡ | è¿”å›ž: {len(messages)} æ¡")
    result_parts.append(f"å…³é”®è¯: {', '.join(keywords[:20]) if keywords else 'æ— '}")

    for idx, line_num in enumerate(sorted_lines, 1):
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        result_parts.append(
            f"--- å‘½ä¸­çª—å£ {idx} (è¡Œ {line_num}, Â±{_CHATLOG_INDEX_CONTEXT_BEFORE}/{_CHATLOG_INDEX_CONTEXT_AFTER}) ---"
        )
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            raw = msg.get("content", "")
            sender = "æœªçŸ¥"
            body = raw
            if ": " in raw:
                sender, body = raw.split(": ", 1)
            ts = msg.get("timestamp", "")[:19]
            tag = "å‘½ä¸­" if msg.get("is_match") else "ä¸Šä¸‹æ–‡"
            confidence = "é«˜" if msg.get("is_match") else "ä¸­"
            result_parts.append(
                f"[{ts}] {sender}: {body} (è¡Œ{ln} {tag} ç½®ä¿¡åº¦:{confidence})"
            )

    raw_text = "\n".join(result_parts)

    # Step 5: Second-pass selection (skip if already window-formatted)
    log("ðŸ§¹ Step 5: äºŒæ¬¡ç­›é€‰æ¸…æ´—...", "CLEAN")
    if target_person:
        raw_text, attr_stats = await cleaner.entity_attribution(
            raw_text,
            target_person,
            question
        )
        if not attr_stats.get("skipped"):
            log(
                f"   âœ“ å®žä½“å½’å› : ä¿ç•™ {attr_stats.get('keep_count', 0)} æ¡ | "
                f"æŽ’é™¤ {attr_stats.get('exclude_count', 0)} æ¡",
                "CLEAN"
            )
    if "å‘½ä¸­çª—å£" in raw_text:
        cleaned = raw_text
        log("   è·³è¿‡æ¸…æ´—ï¼šå·²åŒ…å«å‘½ä¸­çª—å£ä¸Šä¸‹æ–‡(å·²åšå®žä½“å½’å› )", "CLEAN")
    else:
        if poe_client and poe_client.is_configured:
            log(f"   è°ƒç”¨ {cleaner.config.model} è¿›è¡ŒäºŒæ¬¡ç­›é€‰...", "CLEAN")
        else:
            log("   ä½¿ç”¨ç®€å•æˆªæ–­ (Poeæœªé…ç½®)", "CLEAN")
        cleaned = await cleaner.clean_results(
            formatted_text=raw_text,
            question=question,
            target_person=target_person,
            force=True
        )
    log(f"   âœ“ æ¸…æ´—åŽ: {len(cleaned)} å­—ç¬¦", "CLEAN")

    result_text = _cap_text(cleaned, _CHATLOG_MAX_RETURN_CHARS)
    
    total_time = time.time() - query_start_time
    log(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå‡†å¤‡è¿”å›žç»™ Agent", "DONE")
    log(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s | è¿”å›žå­—ç¬¦: {len(result_text)}", "TIMING")
    
    return {
        "content": [{"type": "text", "text": result_text}]
    }


async def _query_chatlog_impl(args: dict) -> dict:
    """Internal implementation of query_chatlog."""
    import time
    import datetime
    
    query_start_time = time.time()  # Track total query time
    
    def log(msg: str, phase: str = ""):
        """Add log entry with millisecond timestamp."""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG MCP] [{timestamp}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    # Enforce a reasonable minimum max_results to avoid excessive outputs
    requested_max = args.get("max_results", 100)
    max_results = min(max(1, int(requested_max)), 100)  # Cap to avoid huge output
    
    log(f"ðŸš€ å¼€å§‹æŸ¥è¯¢å¾ªçŽ¯", "START")
    log(f"ðŸ“ æ”¶åˆ°æŸ¥è¯¢: '{question}' (äººç‰©: {target_person or 'æ— '}, é™åˆ¶: {max_results})")
    
    if not question:
        return {
            "content": [{"type": "text", "text": "é”™è¯¯ï¼šè¯·æä¾›æŸ¥è¯¢é—®é¢˜ã€‚"}]
        }
    
    loader = _get_loader()
    searcher = _get_searcher(loader)
    cleaner = _get_cleaner()

    log("ðŸ“‚ æ­£åœ¨åŠ è½½èŠå¤©è®°å½•...")
    start = time.time()
    if not loader.load():
        return {
            "content": [{
                "type": "text",
                "text": f"é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶ {loader.file_path}"
            }]
        }
    log(f"âœ“ åŠ è½½å®Œæˆ: {loader.message_count} æ¡æ¶ˆæ¯ ({time.time()-start:.2f}s)")
    
    try:
        # Step 1: Expand query
        log("ðŸ”‘ Step 1: å…ƒæ•°æ®ä¸Žå…³é”®è¯æ‰©å±•...")
        start = time.time()
        
        # Check if Poe is configured
        poe_client = cleaner._get_poe_client()
        if poe_client and poe_client.is_configured:
            log(f"   ä½¿ç”¨å°æ¨¡åž‹: {cleaner.config.model}")
        else:
            log("   âš ï¸ Poe APIæœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™fallback")
        
        available_topics = loader.get_unique_topics()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        topics = query_metadata.get("topics", [])
        log(f"   âœ“ æœç´¢å…³é”®è¯: {', '.join(keywords)}", "KEYWORDS")
        log(f"   âœ“ è¯é¢˜æ ‡ç­¾: {', '.join(topics) if topics else 'æ— '}", "TOPICS")
        log(
            f"   âœ“ æƒ…æ„Ÿ: {query_metadata.get('sentiment')}, "
            f"ä¿¡æ¯å¯†åº¦: {query_metadata.get('information_density')}"
        )
        log(f"   âœ“ å¯ç”¨è¯é¢˜æ ‡ç­¾æ•°: {len(available_topics)}")
        log(f"   âœ“ æ‰©å±•è€—æ—¶: {time.time()-start:.2f}s")
        
        if not keywords:
            result_text = "é”™è¯¯ï¼šæ— æ³•ä»Žé—®é¢˜ä¸­æå–å…³é”®è¯ã€‚"
            log(f"âŒ æŸ¥è¯¢å¤±è´¥: æ— æ³•æå–å…³é”®è¯", "ERROR")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 2: Search with metadata
        log("ðŸ” Step 2: å…ƒæ•°æ®æœç´¢...")
        start = time.time()

        if target_person:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=target_person,
                max_results=max_results
            )
        else:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=None,
                max_results=max_results
            )
        
        log(f"   âœ“ åŒ¹é…æ¶ˆæ¯: {len(result.messages)} æ¡")
        log(f"   ä¸Šä¸‹æ–‡çª—å£: Â±{searcher.context_before}/{searcher.context_after} æ¡")
        log(f"   æœç´¢è€—æ—¶: {time.time()-start:.2f}s")
        
        if not result.messages:
            result_text = f"æœªæ‰¾åˆ°ä¸Žã€Œ{question}ã€ç›¸å…³çš„èŠå¤©è®°å½•ã€‚\næœç´¢å…³é”®è¯: {', '.join(keywords)}"
            log(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ¶ˆæ¯", "RESULT")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 3: Format results
        log("ðŸ“„ Step 3: æ ¼å¼åŒ–ç»“æžœ...")
        start = time.time()
        formatted = searcher.format_segmented_output(result, gap_threshold=10)
        original_len = len(formatted)
        log(f"   åŽŸå§‹å¤§å°: {original_len} å­—ç¬¦")
        log(f"   æ ¼å¼åŒ–è€—æ—¶: {time.time()-start:.2f}s")
        
        # Step 4: Second-pass selection (skip if already window-formatted)
        log("ðŸ§¹ Step 4: äºŒæ¬¡ç­›é€‰æ¸…æ´—...")
        start = time.time()

        if target_person:
            formatted, attr_stats = await cleaner.entity_attribution(
                formatted,
                target_person,
                question
            )
            if not attr_stats.get("skipped"):
                log(
                    f"   âœ“ å®žä½“å½’å› : ä¿ç•™ {attr_stats.get('keep_count', 0)} æ¡ | "
                    f"æŽ’é™¤ {attr_stats.get('exclude_count', 0)} æ¡"
                )
        if "å‘½ä¸­çª—å£" in formatted:
            cleaned = formatted
            log("   è·³è¿‡æ¸…æ´—ï¼šå·²åŒ…å«å‘½ä¸­çª—å£ä¸Šä¸‹æ–‡(å·²åšå®žä½“å½’å› )")
        else:
            if poe_client and poe_client.is_configured:
                log(f"   è°ƒç”¨ {cleaner.config.model} è¿›è¡ŒäºŒæ¬¡ç­›é€‰...")
            else:
                log("   ä½¿ç”¨ç®€å•æˆªæ–­ (Poeæœªé…ç½®)")

            cleaned = await cleaner.clean_results(
                formatted_text=formatted,
                question=question,
                target_person=target_person,
                force=True
            )
        log(f"   âœ“ æ¸…æ´—åŽ: {len(cleaned)} å­—ç¬¦ ({time.time()-start:.2f}s)")

        
        # Build response header
        header = f"## èŠå¤©è®°å½•æ£€ç´¢ç»“æžœ\n\n"
        header += f"**é—®é¢˜**: {question}\n"
        if target_person:
            header += f"**ç›®æ ‡äººç‰©**: {target_person}\n"
        header += f"**æœç´¢å…³é”®è¯**: {', '.join(keywords)}\n"
        header += (
            f"**æŸ¥è¯¢å…ƒæ•°æ®**: topics={query_metadata.get('topics', [])}, "
            f"sentiment={query_metadata.get('sentiment')}, "
            f"information_density={query_metadata.get('information_density')}\n"
        )
        header += f"**æ‰¾åˆ°æ¶ˆæ¯æ•°**: {len(result.messages)}\n"
        header += f"**åŽŸå§‹å¤§å°**: {original_len} å­—ç¬¦\n"
        header += f"**æœ€ç»ˆå¤§å°**: {len(cleaned)} å­—ç¬¦\n"
        header += f"---\n\n"
        
        # Log completion (no footer in return to reduce agent context)
        total_time = time.time() - query_start_time
        log(f"ðŸ“¦ æ­£åœ¨åŒ…è£…ç»“æžœ...", "WRAP")
        log(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå‡†å¤‡è¿”å›žç»™ Agent", "DONE")
        log(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s", "TIMING")
        
        # Return without operation logs to reduce agent context size
        final_text = _cap_text(header + cleaned, _CHATLOG_MAX_RETURN_CHARS)
        return {
            "content": [{
                "type": "text",
                "text": final_text
            }]
        }
        
    except Exception as e:
        log(f"âŒ é”™è¯¯: {str(e)}", "ERROR")
        import traceback
        log(f"   {traceback.format_exc()}")
        return {
            "content": [{
                "type": "text",
                "text": f"æŸ¥è¯¢é”™è¯¯: {str(e)}"
            }]
        }



async def _get_chatlog_stats_impl(args: dict) -> dict:
    """Internal implementation of get_chatlog_stats."""
    loader = _get_loader()
    
    if not loader.is_loaded:
        if not loader.load():
            return {
                "content": [{
                    "type": "text",
                    "text": f"é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶ {loader.file_path}"
                }]
            }
    
    stats = loader.get_stats()
    
    output = "## èŠå¤©è®°å½•ç»Ÿè®¡\n\n"
    output += f"**æ–‡ä»¶è·¯å¾„**: {stats['file_path']}\n"
    output += f"**æ€»æ¶ˆæ¯æ•°**: {stats['total_messages']}\n"
    output += f"\n### å‘é€è€…ç»Ÿè®¡\n\n"
    
    for sender, count in stats['sender_message_counts'].items():
        output += f"- **{sender}**: {count} æ¡æ¶ˆæ¯\n"
    
    output = _cap_text(output, _CHATLOG_MAX_RETURN_CHARS)
    return {
        "content": [{
            "type": "text",
            "text": output
        }]
    }


async def _search_person_impl(args: dict) -> dict:
    """Internal implementation of search_person."""
    person = args.get("person", "")
    include_context = args.get("include_context", True)
    
    if not person:
        return {
            "content": [{
                "type": "text",
                "text": "é”™è¯¯ï¼šè¯·æä¾›äººç‰©åç§°ã€‚"
            }]
        }
    
    loader = _get_loader()
    
    if not loader.is_loaded:
        if not loader.load():
            return {
                "content": [{
                    "type": "text",
                    "text": "é”™è¯¯ï¼šæ— æ³•åŠ è½½èŠå¤©è®°å½•æ–‡ä»¶"
                }]
            }
    
    # Get messages from this person
    person_messages = loader.get_messages_by_sender(person)
    
    if not person_messages:
        return {
            "content": [{
                "type": "text",
                "text": f"æœªæ‰¾åˆ°ã€Œ{person}ã€çš„æ¶ˆæ¯è®°å½•ã€‚"
            }]
        }
    
    # Build result
    output = f"## å…³äºŽã€Œ{person}ã€çš„æ¶ˆæ¯è®°å½•\n\n"
    output += f"**æ€»æ¶ˆæ¯æ•°**: {len(person_messages)}\n"
    output += f"---\n\n"
    
    if include_context:
        # Get context for each message
        all_line_numbers = set()
        for msg in person_messages[:50]:  # Limit to avoid too much data
            for ln in range(max(1, msg.line_number - 2), msg.line_number + 3):
                all_line_numbers.add(ln)
        
        for ln in sorted(all_line_numbers):
            msg = loader.get_message(ln)
            if msg:
                output += msg.format_simple() + "\n"
    else:
        # Just the person's messages
        for msg in person_messages[:100]:
            output += msg.format_simple() + "\n"
    
    return {
        "content": [{
            "type": "text",
            "text": output
        }]
    }


# Tool-decorated versions (for MCP)
@tool(
    "query_chatlog",
    "åŸºäºŽé—®é¢˜æ™ºèƒ½æ£€ç´¢èŠå¤©è®°å½•ã€‚ä¼šè‡ªåŠ¨æ‰©å±•å…³é”®è¯ã€æå–ä¸Šä¸‹æ–‡ã€æ¸…æ´—ç»“æžœã€‚é€‚åˆå›žç­”éœ€è¦åŸºäºŽåŽ†å²èŠå¤©è®°å½•çš„é—®é¢˜ã€‚",
    {
        "question": str,       # ç”¨æˆ·çš„é—®é¢˜
        "target_person": str,  # å¯é€‰ï¼šç›®æ ‡äººç‰©åç§°
        "max_results": int     # å¯é€‰ï¼šæœ€å¤§ç»“æžœæ•°ï¼ˆé»˜è®¤100ï¼‰
    }
)
async def query_chatlog(args: dict) -> dict:
    """Query the chatlog based on a question (uses indexed search)."""
    return await _query_chatlog_indexed_impl(args)


@tool(
    "get_chatlog_stats",
    "èŽ·å–èŠå¤©è®°å½•çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»æ¶ˆæ¯æ•°ã€å‘é€è€…åˆ—è¡¨ç­‰ã€‚",
    {}
)
async def get_chatlog_stats(args: dict) -> dict:
    """Get statistics about the loaded chatlog."""
    return await _get_chatlog_stats_impl(args)


@tool(
    "search_person",
    "æœç´¢ç‰¹å®šäººç‰©çš„æ‰€æœ‰ç›¸å…³æ¶ˆæ¯è®°å½•ã€‚",
    {
        "person": str,            # äººç‰©åç§°
        "include_context": bool   # å¯é€‰ï¼šæ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤trueï¼‰
    }
)
async def search_person(args: dict) -> dict:
    """Search for all messages related to a specific person."""
    return await _search_person_impl(args)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Server Creation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_chatlog_mcp_server(chatlog_path: Optional[str] = None):
    """
    Create the Chatlog MCP server.
    
    Args:
        chatlog_path: Optional path to chatlog JSONL file
        
    Returns:
        An MCP server that can be passed to ClaudeAgentOptions.mcp_servers
    """
    global _chatlog_loader
    
    # Initialize loader with custom path if provided
    if chatlog_path:
        _chatlog_loader = ChatlogLoader(chatlog_path)
    
    return create_sdk_mcp_server(
        name="chatlog",
        version="1.0.0",
        tools=[
            query_chatlog,
            get_chatlog_stats,
            search_person,
        ]
    )


def get_chatlog_tools_info() -> List[Dict[str, str]]:
    """Get information about available chatlog tools for documentation."""
    return [
        {
            "name": "mcp__chatlog__query_chatlog",
            "description": "åŸºäºŽé—®é¢˜æ™ºèƒ½æ£€ç´¢èŠå¤©è®°å½•",
            "usage": "å½“éœ€è¦äº†è§£åŽ†å²å¯¹è¯å†…å®¹æ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__get_chatlog_stats",
            "description": "èŽ·å–èŠå¤©è®°å½•ç»Ÿè®¡ä¿¡æ¯",
            "usage": "æŸ¥çœ‹èŠå¤©è®°å½•æ¦‚å†µæ—¶è°ƒç”¨"
        },
        {
            "name": "mcp__chatlog__search_person",
            "description": "æœç´¢ç‰¹å®šäººç‰©çš„æ¶ˆæ¯è®°å½•",
            "usage": "éœ€è¦äº†è§£æŸä¸ªäººçš„åŽ†å²æ¶ˆæ¯æ—¶è°ƒç”¨"
        }
    ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Synchronous API for direct usage
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def query_chatlog_sync(
    question: str,
    target_person: Optional[str] = None,
    max_results: int = 100
) -> str:
    """Synchronous wrapper for query_chatlog."""
    args = {
        "question": question,
        "target_person": target_person,
        "max_results": max_results
    }
    
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_query_chatlog_indexed_impl(args))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)


def get_chatlog_stats_sync() -> str:
    """Synchronous wrapper for get_chatlog_stats."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_get_chatlog_stats_impl({}))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)

