"""
Chatlog MCP Server for BENEDICTJUN Agent

Provides MCP tools for intelligent chatlog retrieval:
- get_chatlog_stats: Get statistics about loaded chatlog
- search_person: Search messages from a specific person
- atomic tools for topic/keyword/semantic retrieval
"""

import os
import re
import json
import time
import uuid
import asyncio
from typing import Optional, Dict, Any, List, Tuple

from claude_agent_sdk import tool, create_sdk_mcp_server

from .loader import ChatlogLoader, get_chatlog_loader
from .searcher import ChatlogSearcher, SearchResult
from .cleaner import ChatlogCleaner, CleanerConfig
from .metadata_index_loader import MetadataIndexLoader, get_index_loader
from .semantic_index import get_semantic_index


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MCP Tool Definitions
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Global instances
_chatlog_loader: Optional[ChatlogLoader] = None
_chatlog_searcher: Optional[ChatlogSearcher] = None
_chatlog_cleaner: Optional[ChatlogCleaner] = None

_CHATLOG_MAX_RETURN_CHARS = int(os.getenv("CHATLOG_MAX_RETURN_CHARS", "6000"))  # ÊèêÂçáÔºöÊúâÂéãÁº©ÂèØ‰ª•ËøîÂõûÊõ¥Â§ö
_CHATLOG_INDEX_MAX_RESULTS = int(os.getenv("CHATLOG_INDEX_MAX_RESULTS", "200"))
_CHATLOG_INDEX_CONTEXT_BEFORE = int(os.getenv("CHATLOG_INDEX_CONTEXT_BEFORE", "2"))
_CHATLOG_INDEX_CONTEXT_AFTER = int(os.getenv("CHATLOG_INDEX_CONTEXT_AFTER", "2"))
_CHATLOG_MAX_MESSAGES = int(os.getenv("CHATLOG_MAX_MESSAGES", "200"))
_CHATLOG_MAX_CONTENT_CHARS = int(os.getenv("CHATLOG_MAX_CONTENT_CHARS", "500"))
_CHATLOG_MAX_TOOL_CHARS = int(os.getenv("CHATLOG_MAX_TOOL_CHARS", "15000"))  # ÊèêÂçáÔºöÂ∑•ÂÖ∑ËøîÂõû‰∏äÈôê
_CHATLOG_MAX_LIST_ITEMS = int(os.getenv("CHATLOG_MAX_LIST_ITEMS", "80"))  # ÊèêÂçáÔºöÂàóË°®È°π‰∏äÈôê
_CHATLOG_MAX_EVIDENCE_MESSAGES = int(os.getenv("CHATLOG_MAX_EVIDENCE_MESSAGES", "80"))  # ÊèêÂçáÔºö40‚Üí80
_CHATLOG_MAX_EVIDENCE_PER_DIM = int(os.getenv("CHATLOG_MAX_EVIDENCE_PER_DIM", "25"))  # ÊèêÂçáÔºö10‚Üí25
_CHATLOG_EVIDENCE_SNIPPET_CHARS = int(os.getenv("CHATLOG_EVIDENCE_SNIPPET_CHARS", "150"))  # Á®çÂæÆÊîæÂÆΩ
_CHATLOG_EVIDENCE_CACHE_SIZE = int(os.getenv("CHATLOG_EVIDENCE_CACHE_SIZE", "20"))
_CHATLOG_LOAD_CONTEXT_BEFORE = int(os.getenv("CHATLOG_LOAD_CONTEXT_BEFORE", "2"))  # ÊèêÂçáÔºö‰∏ä‰∏ãÊñá
_CHATLOG_LOAD_CONTEXT_AFTER = int(os.getenv("CHATLOG_LOAD_CONTEXT_AFTER", "2"))  # ÊèêÂçáÔºö‰∏ä‰∏ãÊñá
_CHATLOG_LOAD_MAX_MESSAGES = int(os.getenv("CHATLOG_LOAD_MAX_MESSAGES", "60"))  # ÊèêÂçáÔºö20‚Üí60
_CHATLOG_SNIPPET_CHARS = int(os.getenv("CHATLOG_SNIPPET_CHARS", "150"))  # Á®çÂæÆÊîæÂÆΩ

_EVIDENCE_STORE: Dict[str, Dict[str, Any]] = {}
_EVIDENCE_STORE_ORDER: List[str] = []


def _cap_text(text: str, max_chars: int) -> str:
    """Cap tool output to prevent context overflow."""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n...(Â∑≤Êà™Êñ≠)"

def _approx_tokens(chars: int) -> int:
    if chars <= 0:
        return 0
    return max(1, int(chars / 3.6))

def _log_tool_payload(tool_name: str, payload: Dict[str, Any], chars: int) -> None:
    """Log tool result with token estimation and alert for large payloads."""
    approx_tokens = _approx_tokens(chars)
    threshold_chars = int(os.getenv("CHATLOG_TOOL_ALERT_CHARS", "12000"))
    
    # Extract field sizes
    data = payload.get("data", {}) if isinstance(payload, dict) else {}
    key_sizes: Dict[str, int] = {}
    if isinstance(data, dict):
        for k, v in data.items():
            try:
                key_sizes[k] = len(json.dumps(v, ensure_ascii=False))
            except (TypeError, ValueError):
                key_sizes[k] = 0
    
    largest_key = max(key_sizes.items(), key=lambda x: x[1], default=("", 0))
    
    if chars > threshold_chars:
        print(f"[TOOL ALERT] ‚ö†Ô∏è {tool_name}: {chars} chars (~{approx_tokens} tokens) OVER THRESHOLD")
        if largest_key[0]:
            print(f"  ‚îî‚îÄ Largest field: '{largest_key[0]}' = {largest_key[1]} chars")
        print(f"  ‚îî‚îÄ Fields: {list(key_sizes.keys())}")
    else:
        print(f"[TOOL] {tool_name}: {chars} chars (~{approx_tokens} tokens)")

def _truncate_list(items: List[Any], limit: int, cursor_prefix: str) -> Tuple[List[Any], int, Optional[str]]:
    if limit <= 0:
        return [], len(items), f"{cursor_prefix}#offset=0"
    if len(items) <= limit:
        return items, 0, None
    omitted = len(items) - limit
    next_cursor = f"{cursor_prefix}#offset={limit}"
    return items[:limit], omitted, next_cursor

def _build_snippet(text: str, max_chars: int) -> str:
    if not isinstance(text, str):
        return ""
    if max_chars <= 0 or len(text) <= max_chars:
        return text
    return text[:max_chars].rstrip() + "‚Ä¶"

# Slim data limits for preventing token explosion
_SLIM_MAX_LIST = int(os.getenv("CHATLOG_SLIM_MAX_LIST", "50"))
_SLIM_MAX_SNIPPET = int(os.getenv("CHATLOG_SLIM_MAX_SNIPPET", "200"))

def _slim_data(data: Dict[str, Any], depth: int = 0) -> Dict[str, Any]:
    """
    Recursively slim down data structure to prevent token explosion.
    
    - Lists: truncated to _SLIM_MAX_LIST items with omitted_count
    - Long strings: truncated to _SLIM_MAX_SNIPPET chars
    - Nested dicts: recursively processed
    """
    if depth > 5:  # Prevent infinite recursion
        return data
    
    slimmed: Dict[str, Any] = {}
    for key, value in data.items():
        if isinstance(value, list):
            limited, omitted, cursor = _truncate_list(
                value, _SLIM_MAX_LIST, f"field:{key}"
            )
            # Slim each item if it's a dict
            slimmed_list = []
            for item in limited:
                if isinstance(item, dict):
                    slimmed_list.append(_slim_data(item, depth + 1))
                elif isinstance(item, str) and len(item) > _SLIM_MAX_SNIPPET:
                    slimmed_list.append(_build_snippet(item, _SLIM_MAX_SNIPPET))
                else:
                    slimmed_list.append(item)
            slimmed[key] = slimmed_list
            if omitted > 0:
                slimmed[f"_{key}_omitted"] = omitted
                slimmed[f"_{key}_cursor"] = cursor
        elif isinstance(value, str) and len(value) > _SLIM_MAX_SNIPPET:
            slimmed[key] = _build_snippet(value, _SLIM_MAX_SNIPPET)
        elif isinstance(value, dict):
            slimmed[key] = _slim_data(value, depth + 1)
        else:
            slimmed[key] = value
    return slimmed


def _store_evidence(payload: Dict[str, Any]) -> str:
    evidence_id = f"evi_{uuid.uuid4().hex[:12]}"
    _EVIDENCE_STORE[evidence_id] = payload
    _EVIDENCE_STORE_ORDER.append(evidence_id)
    if len(_EVIDENCE_STORE_ORDER) > _CHATLOG_EVIDENCE_CACHE_SIZE:
        expired = _EVIDENCE_STORE_ORDER.pop(0)
        _EVIDENCE_STORE.pop(expired, None)
    return evidence_id

def _get_evidence(evidence_id: str) -> Optional[Dict[str, Any]]:
    if not evidence_id:
        return None
    return _EVIDENCE_STORE.get(evidence_id)

def _build_response(
    ok: bool,
    data: Dict[str, Any],
    meta: Optional[Dict[str, Any]] = None,
    is_error: bool = False,
    tool_name: str = "unknown",
    slim: bool = True,
) -> Dict[str, Any]:
    """Build standardized tool response with automatic data slimming."""
    meta = meta or {}
    meta.setdefault("tool", tool_name)
    
    # Apply data slimming before serialization to prevent token explosion
    if slim and isinstance(data, dict):
        data = _slim_data(data)
    
    payload = {
        "ok": ok,
        "data": data,
        "meta": meta
    }
    text = json.dumps(payload, ensure_ascii=False, indent=2)
    _log_tool_payload(tool_name, payload, len(text))
    if len(text) > _CHATLOG_MAX_TOOL_CHARS:
        meta["truncated"] = True
        meta["max_chars"] = _CHATLOG_MAX_TOOL_CHARS
        payload["meta"] = meta
        text = json.dumps(payload, ensure_ascii=False, indent=2)
        text = _cap_text(text, _CHATLOG_MAX_TOOL_CHARS)
    return {
        "content": [{"type": "text", "text": text}],
        **({"is_error": True} if is_error else {})
    }


def _success(
    data: Dict[str, Any],
    meta: Optional[Dict[str, Any]] = None,
    tool_name: str = "unknown"
) -> Dict[str, Any]:
    return _build_response(True, data, meta=meta, is_error=False, tool_name=tool_name)


def _error(
    message: str,
    meta: Optional[Dict[str, Any]] = None,
    tool_name: str = "unknown"
) -> Dict[str, Any]:
    payload = {"error": message}
    return _build_response(False, payload, meta=meta, is_error=True, tool_name=tool_name)


def _parse_sender_content(content: str) -> Tuple[str, str]:
    if ": " in content:
        sender, body = content.split(": ", 1)
        return sender, body
    return "", content


def _extract_payload(result: Dict[str, Any]) -> Dict[str, Any]:
    """Extract structured payload from a tool result."""
    if not result or "content" not in result or not result["content"]:
        return {}
    text = result["content"][0].get("text", "")
    try:
        payload = json.loads(text)
    except (TypeError, ValueError):
        return {}
    if isinstance(payload, dict):
        return payload
    return {"data": payload}


def _coerce_list(value: Any) -> List[str]:
    if value is None:
        return []
    if isinstance(value, list):
        items: List[str] = []
        for item in value:
            if isinstance(item, str) and "," in item:
                items.extend([p.strip() for p in item.replace("Ôºå", ",").split(",") if p.strip()])
            elif isinstance(item, str):
                items.append(item.strip())
            else:
                items.append(str(item))
        return [i for i in items if i]
    if isinstance(value, str):
        parts = [p.strip() for p in value.replace("Ôºå", ",").split(",")]
        return [p for p in parts if p]
    return [str(value)]


def _coerce_int_list(value: Any) -> List[int]:
    raw_items = _coerce_list(value)
    cleaned: List[int] = []
    for item in raw_items:
        try:
            cleaned.append(int(item))
        except (TypeError, ValueError):
            continue
    return cleaned


def _infer_task_type(question: str) -> str:
    """Infer a high-level task type from the question."""
    if not question:
        return "analysis"
    q = question.lower()
    decision_cues = ("ËØ•‰∏çËØ•", "Ë¶Å‰∏çË¶Å", "ÊòØÂê¶Â∫îËØ•", "ËÉΩ‰∏çËÉΩ", "ÂÄºÂæó‰∏ç", "should i", "should we")
    compare_cues = ("ÂØπÊØî", "ÊØîËæÉ", "Âì™‰∏™", "Êõ¥Â•Ω", "Âå∫Âà´", "difference")
    cause_cues = ("‰∏∫‰ªÄ‰πà", "ÂéüÂõ†", "ÂØºËá¥", "Âõ†‰∏∫", "why")
    plan_cues = ("‰ªÄ‰πàÊó∂ÂÄô", "Êó∂Èó¥", "ÂÆâÊéí", "ËÆ°Âàí", "Êó•Á®ã", "when")
    summary_cues = ("ÊÄªÁªì", "Ê¶ÇÊã¨", "ÂõûÈ°æ", "Ê¢≥ÁêÜ", "ÊÄªÁªì‰∏Ä‰∏ã", "summarize", "summary")
    retrieval_cues = ("ÊúâÊ≤°Êúâ", "Êâæ", "Êü•Êâæ", "ÊêúÁ¥¢", "Âì™Èáå", "look up", "find")

    if any(cue in q for cue in decision_cues):
        return "decision"
    if any(cue in q for cue in compare_cues):
        return "comparison"
    if any(cue in q for cue in cause_cues):
        return "attribution"
    if any(cue in q for cue in plan_cues):
        return "planning"
    if any(cue in q for cue in summary_cues):
        return "summary"
    if any(cue in q for cue in retrieval_cues):
        return "retrieval"
    return "analysis"


def _task_sub_questions(task_type: str) -> List[str]:
    if task_type == "decision":
        return [
            "Áõ∏ÂÖ≥ÂéÜÂè≤‰∫ã‰ª∂‰∏éËØÅÊçÆÊúâÂì™‰∫õÔºü",
            "Ê≠£Âêë/Ë¥üÂêë‰ø°Âè∑ÂêÑÊòØ‰ªÄ‰πàÔºü",
            "ÂÖ≥ÈîÆ‰ø°ÊÅØÁº∫Âè£Êàñ‰∏çÁ°ÆÂÆöÊÄßÊòØ‰ªÄ‰πàÔºü",
        ]
    if task_type == "comparison":
        return [
            "ÂØπÊØîÂØπË±°ÁöÑÂÖ≥ÈîÆÂ∑ÆÂºÇÊòØ‰ªÄ‰πàÔºü",
            "ÊúâÂì™‰∫õÁõ¥Êé•ËØÅÊçÆÊîØÊåÅÂ∑ÆÂºÇÔºü",
            "ÈúÄË¶ÅË°•ÂÖÖÂì™‰∫õ‰ø°ÊÅØÔºü",
        ]
    if task_type == "attribution":
        return [
            "Áõ∏ÂÖ≥‰∫ã‰ª∂ÈìæÊù°ÊòØ‰ªÄ‰πàÔºü",
            "ÂèØËÉΩÁöÑÂéüÂõ†ÊàñËß¶ÂèëÂõ†Á¥†ÊúâÂì™‰∫õÔºü",
            "Âì™‰∫õËØÅÊçÆÊîØÊåÅÊàñÂèçÈ©≥Ôºü",
        ]
    if task_type == "planning":
        return [
            "ÂéÜÂè≤ÊâøËØ∫ÊàñÊó∂Èó¥ÁÇπÊòØ‰ªÄ‰πàÔºü",
            "ÂèØË°åÁöÑÂÆâÊéíÁ™óÂè£ÊòØ‰ªÄ‰πàÔºü",
            "ÊΩúÂú®ÂÜ≤Á™ÅÊàñÈ£éÈô©ÊòØ‰ªÄ‰πàÔºü",
        ]
    if task_type == "summary":
        return [
            "ÂÖ≥ÈîÆ‰∫ã‰ª∂‰∏é‰∫∫Áâ©ÊúâÂì™‰∫õÔºü",
            "‰∏ªË¶ÅÂèòÂåñÊàñËΩ¨ÊäòÊòØ‰ªÄ‰πàÔºü",
            "ÈúÄË¶Å‰øùÁïôÁöÑËØÅÊçÆÁÇπÊúâÂì™‰∫õÔºü",
        ]
    if task_type == "retrieval":
        return [
            "ÊòéÁ°ÆÁöÑÂÖ≥ÈîÆËØç/ËØùÈ¢òÊòØ‰ªÄ‰πàÔºü",
            "ÊòØÂê¶ÈúÄË¶Å‰∫∫Áâ©ÊàñÊó∂Èó¥ËøáÊª§Ôºü",
            "ÊòØÂê¶ÈúÄË¶Å‰∏ä‰∏ãÊñáÁ™óÂè£Ôºü",
        ]
    return [
        "ÂÖ≥ÈîÆ‰∫ãÂÆû‰∏éËØÅÊçÆÊúâÂì™‰∫õÔºü",
        "ÊòØÂê¶Â≠òÂú®Ê®°ÂºèÊàñË∂ãÂäøÔºü",
        "ÈúÄË¶ÅË°•ÂÖÖÂì™‰∫õ‰ø°ÊÅØÔºü",
    ]



def _get_loader() -> ChatlogLoader:
    """Create a fresh ChatlogLoader (no caching)."""
    return ChatlogLoader()


def _get_searcher(loader: ChatlogLoader) -> ChatlogSearcher:
    """Create a fresh ChatlogSearcher (no caching)."""
    return ChatlogSearcher(
        loader=loader,
        context_before=int(os.getenv("CHATLOG_CONTEXT_BEFORE", "2")),
        context_after=int(os.getenv("CHATLOG_CONTEXT_AFTER", "2"))
    )


def _get_cleaner() -> ChatlogCleaner:
    """Get the chatlog cleaner instance."""
    global _chatlog_cleaner
    if _chatlog_cleaner is None:
        config = CleanerConfig(
            model=os.getenv("CHATLOG_CLEANER_MODEL", "Gemini-2.5-Flash-Lite"),
            char_threshold=int(os.getenv("CHATLOG_CHAR_THRESHOLD", "3000")),
            target_chars=int(os.getenv("CHATLOG_TARGET_CHARS", "2000"))
        )
        _chatlog_cleaner = ChatlogCleaner(config)
    return _chatlog_cleaner


# Internal implementations (undecorated, for sync use)

async def _query_chatlog_indexed_impl(args: dict) -> dict:
    """
    Optimized query implementation using pre-built metadata index.
    
    Uses O(1) topic lookups instead of linear scans.
    Returns compact results to avoid context explosion.
    """
    import time
    import datetime
    
    query_start_time = time.time()
    
    def log(msg: str, phase: str = ""):
        """Log with millisecond timestamp."""
        ts = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG INDEX] [{ts}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    max_results = min(int(args.get("max_results", 20)), _CHATLOG_INDEX_MAX_RESULTS)
    
    log(f"üöÄ ÂºÄÂßãÁ¥¢ÂºïÊü•ËØ¢", "START")
    log(f"üìù ÈóÆÈ¢ò: '{question}' (‰∫∫Áâ©: {target_person or 'Êó†'})")
    
    if not question:
        return {"content": [{"type": "text", "text": "ÈîôËØØÔºöËØ∑Êèê‰æõÊü•ËØ¢ÈóÆÈ¢ò„ÄÇ"}]}
    
    # Load index (fast, O(1) lookups)
    index_loader = get_index_loader()
    if not index_loader.load_index():
        log("‚ö†Ô∏è Á¥¢ÂºïÊú™ÊâæÂà∞ÔºåÂõûÈÄÄÂà∞ÊóßÂÆûÁé∞", "FALLBACK")
        return await _query_chatlog_impl(args)
    
    log(
        f"‚úì Á¥¢ÂºïÂ∑≤Âä†ËΩΩ: {len(index_loader.available_topics)} ËØùÈ¢ò | Êñá‰ª∂: {index_loader.index_path}"
    )
    
    # Step 1: Use cleaner to identify topics from question
    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    
    keywords = []
    if poe_client and poe_client.is_configured:
        log(f"üîë ‰ΩøÁî®Â∞èÊ®°ÂûãËØÜÂà´ËØùÈ¢ò: {cleaner.config.model}")
        start = time.time()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, index_loader.available_topics
        )
        selected_topics = query_metadata.get("topics", [])
        log(f"   ‚úì ÂèØÁî®ËØùÈ¢òÊ†áÁ≠æÊï∞: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   ‚úì ËØÜÂà´ËØùÈ¢ò({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'Êó†'}",
            "TOPICS"
        )
        log(f"   ‚úì ÂÖ≥ÈîÆËØç({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
        log(f"   ‚úì Êâ©Â±ïËÄóÊó∂: {time.time()-start:.2f}s")
    else:
        log("‚ö†Ô∏è Poe APIÊú™ÈÖçÁΩÆÔºå‰ΩøÁî®Ê®°Á≥äÂåπÈÖç")
        # Fallback: fuzzy match topics based on question keywords
        selected_topics = []
        if "ÂÄü" in question or "Èí±" in question:
            for topic in ("ÂÄüË¥∑", "ÈáëÈí±"):
                if topic in index_loader.available_topics:
                    selected_topics.append(topic)
        if target_person and target_person in index_loader.available_topics:
            selected_topics.append(target_person)
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, index_loader.available_topics
        )
        selected_topics = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=selected_topics,
            available_topics=index_loader.available_topics
        )
        log(f"   ‚úì ÂèØÁî®ËØùÈ¢òÊ†áÁ≠æÊï∞: {len(index_loader.available_topics)}", "TOPICS")
        log(
            f"   ‚úì ËØÜÂà´ËØùÈ¢ò({len(selected_topics)}): {', '.join(selected_topics) if selected_topics else 'Êó†'}",
            "TOPICS"
        )
        log(f"   ‚úì ÂÖ≥ÈîÆËØç({len(keywords)}): {', '.join(keywords)}", "KEYWORDS")
    
    # Step 2: Search by topics using index (O(1) per topic)
    log("üîç Step 2: Á¥¢ÂºïÊêúÁ¥¢...", "SEARCH")
    start = time.time()

    matched_lines = set()

    # Search by selected topics
    log(f"   ‚úì ‰ΩøÁî®ËØùÈ¢òÊ£ÄÁ¥¢: {len(selected_topics)} ‰∏™", "SEARCH")
    for topic in selected_topics:
        lines = index_loader.search_by_topic_exact(topic)
        matched_lines.update(lines[:max_results])
    
    # Only search by selected topics (keywords are used for topic selection only)

    # Semantic recall (optional, uses local embeddings cache)
    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum
    sem_top_k = int(os.getenv("CHATLOG_SEM_TOP_K", "100"))  # ÊèêÂçáÔºöÊúâÂéãÁº©ÂèØ‰ª•Âè¨ÂõûÊõ¥Â§ö
    semantic_scores: Dict[int, float] = {}

    semantic_index = get_semantic_index()
    if semantic_index.is_available():
        log("   ‚úì ËØ≠‰πâÊ£ÄÁ¥¢: Â∑≤ÂêØÁî®", "SEARCH")
        semantic_matches = semantic_index.search(question, top_k=sem_top_k)
        for line_num, score in semantic_matches:
            # Normalize cosine (-1..1) -> (0..1)
            semantic_scores[line_num] = max(0.0, min(1.0, (score + 1.0) / 2.0))
        log(
            f"   ‚úì ËØ≠‰πâÂëΩ‰∏≠: {len(semantic_scores)} Êù° | top_k={sem_top_k}",
            "SEARCH"
        )
    else:
        log("   ‚ö†Ô∏è ËØ≠‰πâÊ£ÄÁ¥¢: Êú™ÂêØÁî® (Áº∫Â∞ë embeddings ÁºìÂ≠ò)", "SEARCH")

    log(f"   ‚úì ÂåπÈÖçÊ∂àÊÅØ: {len(matched_lines)} Êù° ({time.time()-start:.2f}s)")
    
    if not matched_lines and not semantic_scores:
        log("‚ö†Ô∏è Êú™ÊâæÂà∞ÂåπÈÖçÊ∂àÊÅØ", "RESULT")
        return {
            "content": [{
                "type": "text",
                "text": f"Êú™ÊâæÂà∞‰∏é„Äå{question}„ÄçÁõ∏ÂÖ≥ÁöÑËÅäÂ§©ËÆ∞ÂΩï„ÄÇ\nÊêúÁ¥¢ËØùÈ¢ò: {', '.join(selected_topics)}"
            }]
        }
    
    # Step 3: Load messages with context (only matched lines)
    log("üìÑ Step 3: Âä†ËΩΩÊ∂àÊÅØ...", "LOAD")
    start = time.time()
    
    combined_lines = set(matched_lines) | set(semantic_scores.keys())
    if not combined_lines:
        combined_lines = set(matched_lines)

    def _score(line_num: int) -> float:
        score = 0.0
        if line_num in matched_lines:
            score += kw_weight * 1.0
        if line_num in semantic_scores:
            score += sem_weight * semantic_scores[line_num]
        return score

    scored_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
    sorted_lines = scored_lines[:max_results]
    messages = index_loader.get_messages_by_lines(
        sorted_lines,
        context_before=_CHATLOG_INDEX_CONTEXT_BEFORE,
        context_after=_CHATLOG_INDEX_CONTEXT_AFTER
    )
    
    log(f"   ‚úì Âä†ËΩΩÊ∂àÊÅØ: {len(messages)} Êù° ({time.time()-start:.2f}s)")
    
    # Step 4: Format raw results for cleaning (hit-centered windows)
    log("üì¶ Step 4: Ê†ºÂºèÂåñÁªìÊûú...", "FORMAT")

    message_map = {msg.get("line_number"): msg for msg in messages}
    filtered_samples: List[str] = []
    def _window_mentions_other_person(line_num: int) -> bool:
        if not target_person:
            return False
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        persons = set()
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            facts = (msg.get("metadata") or {}).get("facts") or {}
            for key in ("‰∫∫Áâ©", "ÂØπË±°", "‰∏ª‰Ωì", "‰∫∫"):
                val = facts.get(key)
                if isinstance(val, str) and val.strip():
                    persons.add(val.strip())
        if not persons:
            return False
        if target_person not in persons:
            if len(filtered_samples) < 3:
                filtered_samples.append(
                    f"Ë°å{line_num} persons={', '.join(sorted(persons))}"
                )
            return True
        return False

    if target_person:
        filtered_lines = [
            ln for ln in sorted_lines if not _window_mentions_other_person(ln)
        ]
        if filtered_lines:
            log(
                f"   ‚úì ÂëΩ‰∏≠Á™óÂè£ËøáÊª§(Âü∫‰∫éfacts): {len(sorted_lines)} -> {len(filtered_lines)}",
                "FORMAT"
            )
            if filtered_samples:
                log(
                    "   ‚úì ËøáÊª§Á§∫‰æã: " + " | ".join(filtered_samples),
                    "FORMAT"
                )
            sorted_lines = filtered_lines

    result_parts = []
    result_parts.append(f"## Êü•ËØ¢: {question}")
    result_parts.append(f"ËØùÈ¢ò: {', '.join(selected_topics) if selected_topics else 'Êó†'}")
    result_parts.append(f"ÂåπÈÖç: {len(sorted_lines)} Êù° | ËøîÂõû: {len(messages)} Êù°")
    result_parts.append(f"ÂÖ≥ÈîÆËØç: {', '.join(keywords[:20]) if keywords else 'Êó†'}")

    for idx, line_num in enumerate(sorted_lines, 1):
        start = max(1, line_num - _CHATLOG_INDEX_CONTEXT_BEFORE)
        end = line_num + _CHATLOG_INDEX_CONTEXT_AFTER
        result_parts.append(
            f"--- ÂëΩ‰∏≠Á™óÂè£ {idx} (Ë°å {line_num}, ¬±{_CHATLOG_INDEX_CONTEXT_BEFORE}/{_CHATLOG_INDEX_CONTEXT_AFTER}) ---"
        )
        for ln in range(start, end + 1):
            msg = message_map.get(ln)
            if not msg:
                continue
            raw = msg.get("content", "")
            sender = "Êú™Áü•"
            body = raw
            if ": " in raw:
                sender, body = raw.split(": ", 1)
            ts = msg.get("timestamp", "")[:19]
            tag = "ÂëΩ‰∏≠" if msg.get("is_match") else "‰∏ä‰∏ãÊñá"
            confidence = "È´ò" if msg.get("is_match") else "‰∏≠"
            result_parts.append(
                f"[{ts}] {sender}: {body} (Ë°å{ln} {tag} ÁΩÆ‰ø°Â∫¶:{confidence})"
            )

    raw_text = "\n".join(result_parts)

    # Step 5: Second-pass selection (skip if already window-formatted)
    log("üßπ Step 5: ‰∫åÊ¨°Á≠õÈÄâÊ∏ÖÊ¥ó...", "CLEAN")
    if target_person:
        raw_text, attr_stats = await cleaner.entity_attribution(
            raw_text,
            target_person,
            question
        )
        if not attr_stats.get("skipped"):
            log(
                f"   ‚úì ÂÆû‰ΩìÂΩíÂõ†: ‰øùÁïô {attr_stats.get('keep_count', 0)} Êù° | "
                f"ÊéíÈô§ {attr_stats.get('exclude_count', 0)} Êù°",
                "CLEAN"
            )
    if "ÂëΩ‰∏≠Á™óÂè£" in raw_text:
        cleaned = raw_text
        log("   Ë∑≥ËøáÊ∏ÖÊ¥óÔºöÂ∑≤ÂåÖÂê´ÂëΩ‰∏≠Á™óÂè£‰∏ä‰∏ãÊñá(Â∑≤ÂÅöÂÆû‰ΩìÂΩíÂõ†)", "CLEAN")
    else:
        if poe_client and poe_client.is_configured:
            log(f"   Ë∞ÉÁî® {cleaner.config.model} ËøõË°å‰∫åÊ¨°Á≠õÈÄâ...", "CLEAN")
        else:
            log("   ‰ΩøÁî®ÁÆÄÂçïÊà™Êñ≠ (PoeÊú™ÈÖçÁΩÆ)", "CLEAN")
        cleaned = await cleaner.clean_results(
            formatted_text=raw_text,
            question=question,
            target_person=target_person,
            force=True
        )
    log(f"   ‚úì Ê∏ÖÊ¥óÂêé: {len(cleaned)} Â≠óÁ¨¶", "CLEAN")

    result_text = _cap_text(cleaned, _CHATLOG_MAX_RETURN_CHARS)
    
    total_time = time.time() - query_start_time
    log(f"‚úÖ Êü•ËØ¢ÂÆåÊàêÔºåÂáÜÂ§áËøîÂõûÁªô Agent", "DONE")
    log(f"‚è±Ô∏è ÊÄªËÄóÊó∂: {total_time:.2f}s | ËøîÂõûÂ≠óÁ¨¶: {len(result_text)}", "TIMING")

    return {
        "content": [{"type": "text", "text": result_text}]
    }


async def _query_chatlog_composed_impl(args: dict) -> dict:
    """Compose atomic tools to answer a chatlog query."""
    import datetime

    def log(msg: str, phase: str = ""):
        ts = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG MCP] [{ts}]{phase_str} {msg}")

    question = args.get("question", "")
    target_person = args.get("target_person")
    requested_max = args.get("max_results", 100)
    max_results = min(max(1, int(requested_max)), _CHATLOG_INDEX_MAX_RESULTS)

    if not question:
        return {
            "content": [{"type": "text", "text": "ÈîôËØØÔºöËØ∑Êèê‰æõÊü•ËØ¢ÈóÆÈ¢ò„ÄÇ"}],
            "is_error": True,
        }

    query_start_time = time.time()
    log(f"üöÄ ÂºÄÂßãÁªÑÂêàÊü•ËØ¢", "START")
    log(f"üìù Êî∂Âà∞Êü•ËØ¢: '{question}' (‰∫∫Áâ©: {target_person or 'Êó†'}, ÈôêÂà∂: {max_results})")

    index_loader = get_index_loader()
    if not index_loader.load_index():
        log("‚ö†Ô∏è Á¥¢ÂºïÊú™ÊâæÂà∞ÔºåÂõûÈÄÄÂà∞ÊóßÂÆûÁé∞", "FALLBACK")
        return await _query_chatlog_impl(args)

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    log("üîë Step 1: Êü•ËØ¢Êâ©Â±ï", "EXPAND")
    available_topics = index_loader.available_topics
    if llm_available:
        keywords, metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        method = "llm"
    else:
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, available_topics
        )
        metadata = cleaner._fallback_metadata_classification(
            question, available_topics
        )
        metadata["topics"] = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=metadata.get("topics", []),
            available_topics=available_topics,
        )
        method = "rule_based"
    topics = metadata.get("topics", []) or []
    log(f"   ‚úì method: {method} | keywords: {len(keywords)} | topics: {len(topics)}", "EXPAND")

    log("üîç Step 2: ËØùÈ¢òÁ¥¢ÂºïÊ£ÄÁ¥¢ + ËØ≠‰πâÊ£ÄÁ¥¢(Âπ∂Ë°å)", "SEARCH")

    async def _search_topics() -> set[int]:
        lines: set[int] = set()
        for topic in topics:
            lines.update(index_loader.search_by_topic_exact(topic))
        return lines

    async def _search_semantic() -> Dict[int, float]:
        semantic_index = get_semantic_index()
        if not semantic_index.is_available():
            log("   ‚ö†Ô∏è ËØ≠‰πâÊ£ÄÁ¥¢Êú™ÂêØÁî® (Áº∫Â∞ë embeddings ÁºìÂ≠ò)", "SEARCH")
            return {}
        log("   ‚úì ËØ≠‰πâÊ£ÄÁ¥¢ÂêØÁî®", "SEARCH")
        sem_top_k = int(os.getenv("CHATLOG_SEM_TOP_K", "100"))  # ÊèêÂçáÔºöÊúâÂéãÁº©ÂèØ‰ª•Âè¨ÂõûÊõ¥Â§ö
        semantic_matches = await asyncio.to_thread(
            semantic_index.search,
            question,
            top_k=sem_top_k
        )
        scores: Dict[int, float] = {}
        for line_num, score in semantic_matches:
            scores[line_num] = max(0.0, min(1.0, (score + 1.0) / 2.0))
        return scores

    matched_lines, semantic_scores = await asyncio.gather(
        _search_topics(),
        _search_semantic()
    )

    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum

    if not matched_lines and not semantic_scores:
        log("‚ö†Ô∏è Êú™ÊâæÂà∞ÂåπÈÖçÊ∂àÊÅØ", "RESULT")
        return {
            "content": [{
                "type": "text",
                "text": f"Êú™ÊâæÂà∞‰∏é„Äå{question}„ÄçÁõ∏ÂÖ≥ÁöÑËÅäÂ§©ËÆ∞ÂΩï„ÄÇ"
            }]
        }

    def _score(line_num: int) -> float:
        score = 0.0
        if line_num in matched_lines:
            score += kw_weight
        if line_num in semantic_scores:
            score += sem_weight * semantic_scores[line_num]
        return score

    combined_lines = set(matched_lines) | set(semantic_scores.keys())
    ranked_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
    ranked_lines = ranked_lines[:max_results]

    log(f"üìÑ Step 3: Âä†ËΩΩÊ∂àÊÅØ (ÂëΩ‰∏≠: {len(ranked_lines)})", "LOAD")
    messages = index_loader.get_messages_by_lines(
        ranked_lines,
        context_before=_CHATLOG_INDEX_CONTEXT_BEFORE,
        context_after=_CHATLOG_INDEX_CONTEXT_AFTER,
    )

    formatted_messages: List[Dict[str, Any]] = []
    for msg in messages:
        raw = msg.get("content", "")
        sender, body = _parse_sender_content(raw)
        formatted_messages.append({
            "line": msg.get("line_number"),
            "time": (msg.get("timestamp") or "")[:19],
            "sender": sender or "Êú™Áü•",
            "content": body,
            "is_match": bool(msg.get("is_match")),
        })

    if target_person:
        if llm_available:
            filter_result = await _filter_by_person_impl({
                "messages": formatted_messages,
                "target_person": target_person,
                "use_llm": True,
            })
        else:
            filter_result = await _filter_by_person_impl({
                "messages": formatted_messages,
                "target_person": target_person,
                "use_llm": False,
            })
        if filter_result.get("content"):
            try:
                payload = json.loads(filter_result["content"][0]["text"])
                formatted_messages = payload.get("data", {}).get("filtered_messages", formatted_messages)
            except (ValueError, KeyError, TypeError):
                pass

    log("üßæ Step 4: Ê†ºÂºèÂåñËæìÂá∫", "FORMAT")
    formatted_lines = []
    for m in formatted_messages:
        tag = "‚úì" if m.get("is_match") else ""
        line = f"[{m.get('time', '')}] {m.get('sender', 'Êú™Áü•')}: {m.get('content', '')} {tag}".strip()
        formatted_lines.append(line)

    header = [
        "## ËÅäÂ§©ËÆ∞ÂΩïÊ£ÄÁ¥¢ÁªìÊûú",
        f"**ÈóÆÈ¢ò**: {question}",
    ]
    if target_person:
        header.append(f"**ÁõÆÊ†á‰∫∫Áâ©**: {target_person}")
    header.append(f"**ËØùÈ¢ò**: {', '.join(topics) if topics else 'Êó†'}")
    header.append(f"**ÂÖ≥ÈîÆËØç**: {', '.join(keywords) if keywords else 'Êó†'}")
    header.append(f"**ÂëΩ‰∏≠Ê∂àÊÅØ**: {len(ranked_lines)}")
    header.append("---")

    combined_text = "\n".join(header + formatted_lines)
    if len(combined_text) > cleaner.config.char_threshold:
        log("üßπ Step 5: Ê∏ÖÊ¥óÂéãÁº©", "CLEAN")
        combined_text = await cleaner.clean_results(
            formatted_text=combined_text,
            question=question,
            target_person=target_person,
            force=True,
        )

    result_text = _cap_text(combined_text, _CHATLOG_MAX_RETURN_CHARS)
    total_time = time.time() - query_start_time
    log(f"‚úÖ Êü•ËØ¢ÂÆåÊàêÔºåËÄóÊó∂ {total_time:.2f}s | ËøîÂõûÂ≠óÁ¨¶: {len(result_text)}", "DONE")
    return {"content": [{"type": "text", "text": result_text}]}

async def _query_chatlog_impl(args: dict) -> dict:
    """Internal implementation of query_chatlog."""
    import time
    import datetime
    
    query_start_time = time.time()  # Track total query time
    
    def log(msg: str, phase: str = ""):
        """Add log entry with millisecond timestamp."""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        phase_str = f" [{phase}]" if phase else ""
        print(f"[CHATLOG MCP] [{timestamp}]{phase_str} {msg}")
    
    question = args.get("question", "")
    target_person = args.get("target_person")
    # Enforce a reasonable minimum max_results to avoid excessive outputs
    requested_max = args.get("max_results", 100)
    max_results = min(max(1, int(requested_max)), 100)  # Cap to avoid huge output
    
    log(f"üöÄ ÂºÄÂßãÊü•ËØ¢Âæ™ÁéØ", "START")
    log(f"üìù Êî∂Âà∞Êü•ËØ¢: '{question}' (‰∫∫Áâ©: {target_person or 'Êó†'}, ÈôêÂà∂: {max_results})")
    
    if not question:
        return {
            "content": [{"type": "text", "text": "ÈîôËØØÔºöËØ∑Êèê‰æõÊü•ËØ¢ÈóÆÈ¢ò„ÄÇ"}]
        }
    
    loader = _get_loader()
    searcher = _get_searcher(loader)
    cleaner = _get_cleaner()

    log("üìÇ Ê≠£Âú®Âä†ËΩΩËÅäÂ§©ËÆ∞ÂΩï...")
    start = time.time()
    if not loader.load():
        return {
            "content": [{
                "type": "text",
                "text": f"ÈîôËØØÔºöÊó†Ê≥ïÂä†ËΩΩËÅäÂ§©ËÆ∞ÂΩïÊñá‰ª∂ {loader.file_path}"
            }]
        }
    log(f"‚úì Âä†ËΩΩÂÆåÊàê: {loader.message_count} Êù°Ê∂àÊÅØ ({time.time()-start:.2f}s)")
    
    try:
        # Step 1: Expand query
        log("üîë Step 1: ÂÖÉÊï∞ÊçÆ‰∏éÂÖ≥ÈîÆËØçÊâ©Â±ï...")
        start = time.time()
        
        # Check if Poe is configured
        poe_client = cleaner._get_poe_client()
        if poe_client and poe_client.is_configured:
            log(f"   ‰ΩøÁî®Â∞èÊ®°Âûã: {cleaner.config.model}")
        else:
            log("   ‚ö†Ô∏è Poe APIÊú™ÈÖçÁΩÆÔºå‰ΩøÁî®ËßÑÂàôfallback")
        
        available_topics = loader.get_unique_topics()
        keywords, query_metadata = await cleaner.expand_query(
            question, target_person, available_topics
        )
        topics = query_metadata.get("topics", [])
        log(f"   ‚úì ÊêúÁ¥¢ÂÖ≥ÈîÆËØç: {', '.join(keywords)}", "KEYWORDS")
        log(f"   ‚úì ËØùÈ¢òÊ†áÁ≠æ: {', '.join(topics) if topics else 'Êó†'}", "TOPICS")
        log(
            f"   ‚úì ÊÉÖÊÑü: {query_metadata.get('sentiment')}, "
            f"‰ø°ÊÅØÂØÜÂ∫¶: {query_metadata.get('information_density')}"
        )
        log(f"   ‚úì ÂèØÁî®ËØùÈ¢òÊ†áÁ≠æÊï∞: {len(available_topics)}")
        log(f"   ‚úì Êâ©Â±ïËÄóÊó∂: {time.time()-start:.2f}s")
        
        if not keywords:
            result_text = "ÈîôËØØÔºöÊó†Ê≥ï‰ªéÈóÆÈ¢ò‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØç„ÄÇ"
            log(f"‚ùå Êü•ËØ¢Â§±Ë¥•: Êó†Ê≥ïÊèêÂèñÂÖ≥ÈîÆËØç", "ERROR")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 2: Search with metadata
        log("üîç Step 2: ÂÖÉÊï∞ÊçÆÊêúÁ¥¢...")
        start = time.time()

        if target_person:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=target_person,
                max_results=max_results
            )
        else:
            result = searcher.search_by_metadata(
                metadata=query_metadata,
                keywords=keywords,
                target_person=None,
                max_results=max_results
            )
        
        log(f"   ‚úì ÂåπÈÖçÊ∂àÊÅØ: {len(result.messages)} Êù°")
        log(f"   ‰∏ä‰∏ãÊñáÁ™óÂè£: ¬±{searcher.context_before}/{searcher.context_after} Êù°")
        log(f"   ÊêúÁ¥¢ËÄóÊó∂: {time.time()-start:.2f}s")
        
        if not result.messages:
            result_text = f"Êú™ÊâæÂà∞‰∏é„Äå{question}„ÄçÁõ∏ÂÖ≥ÁöÑËÅäÂ§©ËÆ∞ÂΩï„ÄÇ\nÊêúÁ¥¢ÂÖ≥ÈîÆËØç: {', '.join(keywords)}"
            log(f"‚ö†Ô∏è Êú™ÊâæÂà∞ÂåπÈÖçÊ∂àÊÅØ", "RESULT")
            return {
                "content": [{"type": "text", "text": result_text}]
            }
        
        # Step 3: Format results
        log("üìÑ Step 3: Ê†ºÂºèÂåñÁªìÊûú...")
        start = time.time()
        formatted = searcher.format_segmented_output(result, gap_threshold=10)
        original_len = len(formatted)
        log(f"   ÂéüÂßãÂ§ßÂ∞è: {original_len} Â≠óÁ¨¶")
        log(f"   Ê†ºÂºèÂåñËÄóÊó∂: {time.time()-start:.2f}s")
        
        # Step 4: Second-pass selection (skip if already window-formatted)
        log("üßπ Step 4: ‰∫åÊ¨°Á≠õÈÄâÊ∏ÖÊ¥ó...")
        start = time.time()

        if target_person:
            formatted, attr_stats = await cleaner.entity_attribution(
                formatted,
                target_person,
                question
            )
            if not attr_stats.get("skipped"):
                log(
                    f"   ‚úì ÂÆû‰ΩìÂΩíÂõ†: ‰øùÁïô {attr_stats.get('keep_count', 0)} Êù° | "
                    f"ÊéíÈô§ {attr_stats.get('exclude_count', 0)} Êù°"
                )
        if "ÂëΩ‰∏≠Á™óÂè£" in formatted:
            cleaned = formatted
            log("   Ë∑≥ËøáÊ∏ÖÊ¥óÔºöÂ∑≤ÂåÖÂê´ÂëΩ‰∏≠Á™óÂè£‰∏ä‰∏ãÊñá(Â∑≤ÂÅöÂÆû‰ΩìÂΩíÂõ†)")
        else:
            if poe_client and poe_client.is_configured:
                log(f"   Ë∞ÉÁî® {cleaner.config.model} ËøõË°å‰∫åÊ¨°Á≠õÈÄâ...")
            else:
                log("   ‰ΩøÁî®ÁÆÄÂçïÊà™Êñ≠ (PoeÊú™ÈÖçÁΩÆ)")

            cleaned = await cleaner.clean_results(
                formatted_text=formatted,
                question=question,
                target_person=target_person,
                force=True
            )
        log(f"   ‚úì Ê∏ÖÊ¥óÂêé: {len(cleaned)} Â≠óÁ¨¶ ({time.time()-start:.2f}s)")

        
        # Build response header
        header = f"## ËÅäÂ§©ËÆ∞ÂΩïÊ£ÄÁ¥¢ÁªìÊûú\n\n"
        header += f"**ÈóÆÈ¢ò**: {question}\n"
        if target_person:
            header += f"**ÁõÆÊ†á‰∫∫Áâ©**: {target_person}\n"
        header += f"**ÊêúÁ¥¢ÂÖ≥ÈîÆËØç**: {', '.join(keywords)}\n"
        header += (
            f"**Êü•ËØ¢ÂÖÉÊï∞ÊçÆ**: topics={query_metadata.get('topics', [])}, "
            f"sentiment={query_metadata.get('sentiment')}, "
            f"information_density={query_metadata.get('information_density')}\n"
        )
        header += f"**ÊâæÂà∞Ê∂àÊÅØÊï∞**: {len(result.messages)}\n"
        header += f"**ÂéüÂßãÂ§ßÂ∞è**: {original_len} Â≠óÁ¨¶\n"
        header += f"**ÊúÄÁªàÂ§ßÂ∞è**: {len(cleaned)} Â≠óÁ¨¶\n"
        header += f"---\n\n"
        
        # Log completion (no footer in return to reduce agent context)
        total_time = time.time() - query_start_time
        log(f"üì¶ Ê≠£Âú®ÂåÖË£ÖÁªìÊûú...", "WRAP")
        log(f"‚úÖ Êü•ËØ¢ÂÆåÊàêÔºåÂáÜÂ§áËøîÂõûÁªô Agent", "DONE")
        log(f"‚è±Ô∏è ÊÄªËÄóÊó∂: {total_time:.2f}s", "TIMING")
        
        # Return without operation logs to reduce agent context size
        final_text = _cap_text(header + cleaned, _CHATLOG_MAX_RETURN_CHARS)
        return {
            "content": [{
                "type": "text",
                "text": final_text
            }]
        }
        
    except Exception as e:
        log(f"‚ùå ÈîôËØØ: {str(e)}", "ERROR")
        import traceback
        log(f"   {traceback.format_exc()}")
        return {
            "content": [{
                "type": "text",
                "text": f"Êü•ËØ¢ÈîôËØØ: {str(e)}"
            }]
        }



async def _get_chatlog_stats_impl(args: dict) -> dict:
    """Internal implementation of get_chatlog_stats."""
    loader = _get_loader()
    
    if not loader.is_loaded:
        if not loader.load():
            return _error(
                f"ÈîôËØØÔºöÊó†Ê≥ïÂä†ËΩΩËÅäÂ§©ËÆ∞ÂΩïÊñá‰ª∂ {loader.file_path}",
                meta={"source": "stats"},
                tool_name="get_chatlog_stats"
            )
    
    stats = loader.get_stats()

    data = {
        "stats": stats,
    }
    meta = {
        "available": True,
        "source": "stats",
    }
    return _success(data, meta=meta, tool_name="get_chatlog_stats")


async def _list_topics_impl(args: dict) -> dict:
    started = time.time()
    limit = min(int(args.get("limit", _CHATLOG_MAX_LIST_ITEMS)), _CHATLOG_MAX_LIST_ITEMS)
    pattern = (args.get("pattern") or "").strip()

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "Êó†Ê≥ïÂä†ËΩΩÁ¥¢Âºï",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            },
            tool_name="list_topics"
        )

    topics = index_loader.available_topics
    if pattern:
        pattern_lower = pattern.lower()
        topics = [t for t in topics if pattern_lower in t.lower()]

    topics_sorted = sorted(topics)
    limited, omitted_count, next_cursor = _truncate_list(
        topics_sorted,
        limit,
        cursor_prefix=f"topics:{pattern or 'all'}"
    )
    data = {
        "topics": limited,
        "total_count": len(index_loader.available_topics),
        "returned_count": len(limited),
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
        "pattern": pattern or None,
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="list_topics")


async def _search_by_topics_impl(args: dict) -> dict:
    started = time.time()
    topics = _coerce_list(args.get("topics"))
    max_results = min(int(args.get("max_results", _CHATLOG_MAX_LIST_ITEMS)), 500)

    if not topics:
        return _error(
            "ËØ∑Êèê‰æõËá≥Â∞ë‰∏Ä‰∏™ËØùÈ¢ò",
            meta={"source": "index"},
            tool_name="search_by_topics"
        )

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "Êó†Ê≥ïÂä†ËΩΩÁ¥¢Âºï",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            },
            tool_name="search_by_topics"
        )

    all_lines: set[int] = set()
    breakdown: Dict[str, int] = {}
    for topic in topics:
        lines = index_loader.search_by_topic_exact(topic)
        breakdown[topic] = len(lines)
        all_lines.update(lines)

    line_numbers = sorted(all_lines)
    limited, omitted_count, next_cursor = _truncate_list(
        line_numbers,
        max_results,
        cursor_prefix="topics"
    )
    data = {
        "line_numbers": limited,
        "total_matches": len(all_lines),
        "topic_breakdown": breakdown,
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="search_by_topics")


async def _search_by_keywords_impl(args: dict) -> dict:
    started = time.time()
    keywords = _coerce_list(args.get("keywords"))
    target_person = args.get("target_person")
    max_results = min(int(args.get("max_results", _CHATLOG_MAX_LIST_ITEMS)), 500)
    match_all = bool(args.get("match_all", False))

    if not keywords:
        return _error(
            "ËØ∑Êèê‰æõËá≥Â∞ë‰∏Ä‰∏™ÂÖ≥ÈîÆËØç",
            meta={"source": "scan"},
            tool_name="search_by_keywords"
        )

    loader = _get_loader()
    if not loader.load():
        return _error(
            "Êó†Ê≥ïÂä†ËΩΩËÅäÂ§©ËÆ∞ÂΩï",
            meta={
                "available": False,
                "source": "scan",
                "timing_ms": int((time.time() - started) * 1000)
            },
            tool_name="search_by_keywords"
        )

    normalized_keywords = [k.lower() for k in keywords if isinstance(k, str)]
    keyword_hits = {k: 0 for k in normalized_keywords}
    matched_lines: List[int] = []

    target_lower = target_person.lower() if isinstance(target_person, str) else None

    for msg in loader.get_all_messages():
        if target_lower and target_lower not in (msg.sender or "").lower():
            continue
        content_lower = msg.content.lower()
        matches = [kw for kw in normalized_keywords if kw and kw in content_lower]
        if (match_all and len(matches) == len(normalized_keywords)) or (not match_all and matches):
            matched_lines.append(msg.line_number)
            for kw in matches:
                keyword_hits[kw] += 1

    limited, omitted_count, next_cursor = _truncate_list(
        matched_lines,
        max_results,
        cursor_prefix="keywords"
    )
    data = {
        "line_numbers": limited,
        "total_matches": len(matched_lines),
        "keyword_breakdown": keyword_hits,
        "person_filter": target_person,
        "match_all": match_all,
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
    }
    meta = {
        "available": True,
        "source": "scan",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="search_by_keywords")


async def _load_messages_impl(args: dict) -> dict:
    started = time.time()
    line_numbers = _coerce_int_list(args.get("line_numbers"))
    context_before = min(
        int(args.get("context_before", _CHATLOG_LOAD_CONTEXT_BEFORE)),
        5,
    )
    context_after = min(
        int(args.get("context_after", _CHATLOG_LOAD_CONTEXT_AFTER)),
        5,
    )
    include_metadata = bool(args.get("include_metadata", False))
    max_messages = min(
        int(args.get("max_messages", _CHATLOG_LOAD_MAX_MESSAGES)),
        200,
    )
    max_content_chars = min(
        int(args.get("max_content_chars", _CHATLOG_MAX_CONTENT_CHARS)),
        2000
    )
    snippet_chars = min(
        int(args.get("snippet_chars", _CHATLOG_SNIPPET_CHARS)),
        500,
    )
    fields = args.get("fields") or ["line", "time", "sender", "content"]

    if not line_numbers:
        return _error(
            "ËØ∑Êèê‰æõË°åÂè∑ÂàóË°®",
            meta={"source": "index"},
            tool_name="load_messages"
        )

    context_span = max(1, context_before + context_after + 1)
    max_lines = max(1, int(max_messages / context_span))
    cleaned_lines = line_numbers[:max_lines]
    if not cleaned_lines:
        return _error(
            "Ë°åÂè∑Ê†ºÂºèÊó†Êïà",
            meta={"source": "index"},
            tool_name="load_messages"
        )

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "Êó†Ê≥ïÂä†ËΩΩÁ¥¢Âºï",
            meta={
                "available": False,
                "source": "index",
                "timing_ms": int((time.time() - started) * 1000)
            },
            tool_name="load_messages"
        )

    messages = index_loader.get_messages_by_lines(
        cleaned_lines,
        context_before=context_before,
        context_after=context_after,
    )
    limited_messages, omitted_count, next_cursor = _truncate_list(
        messages,
        max_messages,
        cursor_prefix="messages"
    )
    truncated = omitted_count > 0
    result = []
    normalized_fields = [f for f in fields if isinstance(f, str) and f.strip()]
    if "line" not in normalized_fields:
        normalized_fields.insert(0, "line")
    for msg in limited_messages:
        raw = msg.get("content", "")
        sender, body = _parse_sender_content(raw)
        if max_content_chars > 0 and len(body) > max_content_chars:
            body = body[:max_content_chars] + "‚Ä¶"
        snippet = _build_snippet(body, snippet_chars)
        item = {
            "line": msg.get("line_number"),
            "time": (msg.get("timestamp") or "")[:19],
            "sender": sender or "Êú™Áü•",
            "content": snippet,
            "is_match": bool(msg.get("is_match")),
        }
        if include_metadata:
            item["metadata"] = msg.get("metadata", {})
        if "topics" in msg:
            item["topics"] = msg.get("topics")
        item = {k: v for k, v in item.items() if k in normalized_fields or k == "metadata"}
        result.append(item)

    data = {
        "messages": result,
        "count": len(result),
        "context": f"¬±{context_before}/{context_after}",
        "truncated": truncated,
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
        "max_messages": max_messages,
        "max_content_chars": max_content_chars,
        "snippet_chars": snippet_chars,
        "fields": normalized_fields,
    }
    meta = {
        "available": True,
        "source": "index",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="load_messages")


async def _expand_query_impl(args: dict) -> dict:
    started = time.time()
    question = args.get("question", "")
    target_person = args.get("target_person")
    use_llm = bool(args.get("use_llm", True))

    if not question:
        return _error(
            "ËØ∑Êèê‰æõÈóÆÈ¢ò",
            meta={"source": "llm"},
            tool_name="expand_query"
        )

    index_loader = get_index_loader()
    available_topics = index_loader.available_topics if index_loader.load_index() else []
    # Only pass first 50 topics as preview to LLM to prevent token explosion
    # Full available_topics list (1771+) would consume ~8k tokens
    topics_preview = available_topics[:50] if available_topics else []

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    if use_llm and llm_available:
        keywords, metadata = await cleaner.expand_query(
            question, target_person, topics_preview  # Pass preview, not full list
        )
        # Server-side filtering: ensure LLM-suggested topics exist in available_topics
        llm_topics = metadata.get("topics", [])
        metadata["topics"] = [t for t in llm_topics if t in available_topics]
        method = "llm"
        model = cleaner.config.model
        llm_used = True
    else:
        keywords = cleaner._fallback_keyword_extraction(
            question, target_person, available_topics
        )
        metadata = cleaner._fallback_metadata_classification(
            question, available_topics
        )
        metadata["topics"] = cleaner._ensure_topic_coverage(
            question=question,
            target_person=target_person,
            keywords=keywords,
            topics=metadata.get("topics", []),
            available_topics=available_topics,
        )
        method = "rule_based"
        model = None
        llm_used = False

    limited_keywords, kw_omitted, kw_cursor = _truncate_list(
        keywords,
        _CHATLOG_MAX_LIST_ITEMS,
        cursor_prefix="keywords"
    )
    raw_topics = metadata.get("topics", []) or []
    limited_topics, topic_omitted, topic_cursor = _truncate_list(
        raw_topics,
        _CHATLOG_MAX_LIST_ITEMS,
        cursor_prefix="topics"
    )
    data = {
        "keywords": limited_keywords,
        "topics": limited_topics,
        "sentiment": metadata.get("sentiment"),
        "information_density": metadata.get("information_density"),
        "method": method,
        "model": model,
        "llm_available": llm_available,
        "omitted_count": {
            "keywords": kw_omitted,
            "topics": topic_omitted,
        },
        "next_cursor": {
            "keywords": kw_cursor,
            "topics": topic_cursor,
        },
    }
    meta = {
        "available": True,
        "source": "llm" if method == "llm" else "rule_based",
        "llm_used": llm_used,
        "model": model,
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="expand_query")


async def _search_semantic_impl(args: dict) -> dict:
    started = time.time()
    query = args.get("query", "")
    top_k = min(int(args.get("top_k", _CHATLOG_MAX_LIST_ITEMS)), 200)

    if not query:
        return _error(
            "ËØ∑Êèê‰æõÊü•ËØ¢ÊñáÊú¨",
            meta={"source": "semantic"},
            tool_name="search_semantic"
        )

    semantic_index = get_semantic_index()
    if not semantic_index.is_available():
        data = {
            "available": False,
            "reason": "Áº∫Â∞ë embeddings ÁºìÂ≠òÊñá‰ª∂",
            "suggestion": "ËøêË°å python -m src.chatlog.semantic_index ÊûÑÂª∫Á¥¢Âºï",
            "results": [],
        }
        meta = {
            "available": False,
            "source": "semantic",
            "timing_ms": int((time.time() - started) * 1000),
        }
        return _success(data, meta=meta, tool_name="search_semantic")

    raw_results = semantic_index.search(query, top_k=top_k)
    results = [
        {"line": ln, "score": round((score + 1.0) / 2.0, 4)}
        for ln, score in raw_results
    ]
    limited, omitted_count, next_cursor = _truncate_list(
        results,
        top_k,
        cursor_prefix="semantic"
    )
    data = {
        "available": True,
        "results": limited,
        "count": len(limited),
        "query": query,
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
    }
    meta = {
        "available": True,
        "source": "semantic",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="search_semantic")


async def _filter_by_person_impl(args: dict) -> dict:
    started = time.time()
    messages = args.get("messages") or []
    target_person = args.get("target_person", "")
    use_llm = bool(args.get("use_llm", True))

    if not messages:
        return _error(
            "ËØ∑Êèê‰æõÊ∂àÊÅØÂàóË°®",
            meta={"source": "llm"},
            tool_name="filter_by_person"
        )
    if not target_person:
        return _error(
            "ËØ∑Êèê‰æõÁõÆÊ†á‰∫∫Áâ©",
            meta={"source": "llm"},
            tool_name="filter_by_person"
        )

    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    kept: List[Dict[str, Any]] = []
    excluded: List[Dict[str, Any]] = []

    if use_llm and llm_available:
        formatted_lines = [
            f"[{m.get('time', '')}] {m.get('sender', 'Êú™Áü•')}: {m.get('content', '')}"
            for m in messages
        ]
        formatted_text = "\n".join(formatted_lines)
        filtered_text, attr_stats = await cleaner.entity_attribution(
            formatted_text,
            target_person,
            ""
        )
        filtered_set = {line.strip() for line in filtered_text.splitlines() if line.strip()}
        for msg, line in zip(messages, formatted_lines):
            if line.strip() in filtered_set:
                kept.append(msg)
            else:
                excluded.append(msg)
        method = "llm_attribution"
        meta = {
            "available": True,
            "source": "llm",
            "llm_used": True,
            "model": cleaner.config.model,
            "timing_ms": int((time.time() - started) * 1000),
            "attr_stats": attr_stats,
        }
    else:
        for msg in messages:
            content = msg.get("content", "")
            sender = msg.get("sender", "")
            if target_person in content or target_person == sender:
                kept.append(msg)
            else:
                excluded.append(msg)
        method = "name_match"
        meta = {
            "available": True,
            "source": "rule_based",
            "llm_used": False,
            "model": None,
            "timing_ms": int((time.time() - started) * 1000),
        }

    data = {
        "filtered_messages": kept,
        "kept_count": len(kept),
        "excluded_count": len(excluded),
        "method": method,
        "target_person": target_person,
        "llm_available": llm_available,
    }
    return _success(data, meta=meta, tool_name="filter_by_person")


async def _format_messages_impl(args: dict) -> dict:
    started = time.time()
    messages = args.get("messages") or []
    fmt = args.get("format", "compact")
    max_chars = min(int(args.get("max_chars", _CHATLOG_MAX_RETURN_CHARS)), 10000)

    if not messages:
        return _error(
            "ËØ∑Êèê‰æõÊ∂àÊÅØÂàóË°®",
            meta={"source": "format"},
            tool_name="format_messages"
        )

    lines: List[str] = []
    if fmt == "timeline":
        lines.append("## Êó∂Èó¥Á∫ø")
        current_date = None
        for m in messages:
            time_str = m.get("time", "")
            date = time_str[:10] if time_str else "Êú™Áü•Êó•Êúü"
            if date != current_date:
                current_date = date
                lines.append("")
                lines.append(f"### {date}")
            clock = time_str[11:16] if len(time_str) >= 16 else ""
            sender = m.get("sender", "Êú™Áü•")
            content = m.get("content", "")
            lines.append(f"- **{clock}** [{sender}]: {content}")
    elif fmt == "detailed":
        for m in messages:
            lines.append("---")
            lines.append(f"**Ë°åÂè∑**: {m.get('line')}")
            lines.append(f"**Êó∂Èó¥**: {m.get('time')}")
            lines.append(f"**ÂèëÈÄÅËÄÖ**: {m.get('sender')}")
            lines.append(f"**ÂÜÖÂÆπ**: {m.get('content')}")
    else:
        for m in messages:
            tag = "‚úì" if m.get("is_match") else ""
            line = f"[{m.get('time', '')}] {m.get('sender', 'Êú™Áü•')}: {m.get('content', '')} {tag}".strip()
            lines.append(line)

    text = "\n".join(lines)
    truncated = len(text) > max_chars
    if truncated:
        text = _cap_text(text, max_chars)

    data = {
        "text": text,
        "chars": len(text),
        "messages": len(messages),
        "format": fmt,
        "truncated": truncated,
    }
    meta = {
        "available": True,
        "source": "format",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="format_messages")


def _extract_amounts(text: str) -> List[str]:
    if not text:
        return []
    amounts: List[str] = []
    pattern = re.compile(r"(\d+(?:\.\d+)?)\s*(ÂÖÉ|Âùó|Ôø•|¬•|rmb|‰∫∫Ê∞ëÂ∏Å)", re.IGNORECASE)
    for match in pattern.finditer(text):
        amount = match.group(1)
        unit = match.group(2)
        amounts.append(f"{amount}{unit}")
    return amounts


def _classify_signal(content: str) -> Tuple[bool, bool]:
    repay_keywords = ("Ëøò", "ËøòÈí±", "ËøòÊ¨æ", "Ëøò‰Ω†", "ËøòÊàë", "Â∑≤Ëøò", "ËΩ¨Ë¥¶Áªô")
    negative_keywords = ("Ê≤°Ëøò", "Êú™Ëøò", "Êãñ", "Êé®Ëøü", "‰∏ãÊ¨°", "ÊîπÂ§©", "ÊôöÁÇπ")

    has_repay = any(k in content for k in repay_keywords)
    has_negative = any(k in content for k in negative_keywords)
    return has_repay, has_negative


async def _parse_task_impl(args: dict) -> dict:
    started = time.time()
    question = args.get("question", "")
    target_person = args.get("target_person")
    use_llm = bool(args.get("use_llm", True))
    max_dimensions = min(int(args.get("max_dimensions", 4)), 6)

    if not question:
        return _error(
            "ËØ∑Êèê‰æõÈóÆÈ¢ò",
            meta={"source": "parse"},
            tool_name="parse_task"
        )

    index_loader = get_index_loader()
    available_topics = index_loader.available_topics if index_loader.load_index() else []
    cleaner = _get_cleaner()
    poe_client = cleaner._get_poe_client()
    llm_available = bool(poe_client and poe_client.is_configured)

    if use_llm and llm_available:
        plan = await cleaner.plan_evidence_dimensions(
            question,
            target_person=target_person,
            available_topics=available_topics,
            max_dimensions=max_dimensions,
        )
        method = plan.get("method", "llm")
        model = plan.get("model")
    else:
        plan = cleaner._fallback_dimension_plan(
            question,
            target_person=target_person,
            available_topics=available_topics,
            max_dimensions=max_dimensions,
        )
        method = plan.get("method", "rule_based")
        model = plan.get("model")

    task_type = _infer_task_type(question)

    output = {
        "task_type": task_type,
        "question_type": plan.get("question_type", "analysis"),
        "target_person": target_person,
        "dimensions": plan.get("dimensions", []),
        "method": method,
        "model": model,
    }
    result_meta = {
        "available": True,
        "source": "parse",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(output, meta=result_meta, tool_name="parse_task")


async def _retrieve_evidence_impl(args: dict) -> dict:
    started = time.time()
    question = args.get("question", "")
    target_person = args.get("target_person")
    dimensions = args.get("dimensions") or []
    max_per_dimension = min(
        int(args.get("max_per_dimension", _CHATLOG_MAX_EVIDENCE_PER_DIM)),
        _CHATLOG_MAX_EVIDENCE_PER_DIM,
    )
    max_total_messages = min(
        int(args.get("max_total_messages", _CHATLOG_MAX_EVIDENCE_MESSAGES)),
        _CHATLOG_MAX_EVIDENCE_MESSAGES,
    )
    snippet_chars = min(
        int(args.get("snippet_chars", _CHATLOG_EVIDENCE_SNIPPET_CHARS)),
        300,
    )
    context_before = min(
        int(args.get("context_before", _CHATLOG_LOAD_CONTEXT_BEFORE)),
        3,
    )
    context_after = min(
        int(args.get("context_after", _CHATLOG_LOAD_CONTEXT_AFTER)),
        3,
    )
    use_semantic = bool(args.get("use_semantic", True))
    use_llm_plan = bool(args.get("use_llm_plan", True))

    if not question and not dimensions:
        return _error(
            "ËØ∑Êèê‰æõÈóÆÈ¢òÊàñÁª¥Â∫¶ËÆ°Âàí",
            meta={"source": "retrieve"},
            tool_name="retrieve_evidence"
        )

    index_loader = get_index_loader()
    if not index_loader.load_index():
        return _error(
            "Êó†Ê≥ïÂä†ËΩΩÁ¥¢Âºï",
            meta={"source": "index"},
            tool_name="retrieve_evidence"
        )

    available_topics = index_loader.available_topics

    if not dimensions:
        cleaner = _get_cleaner()
        poe_client = cleaner._get_poe_client()
        llm_available = bool(poe_client and poe_client.is_configured)
        if use_llm_plan and llm_available:
            plan = await cleaner.plan_evidence_dimensions(
                question,
                target_person=target_person,
                available_topics=available_topics,
                max_dimensions=4,
            )
        else:
            plan = cleaner._fallback_dimension_plan(
                question,
                target_person=target_person,
                available_topics=available_topics,
                max_dimensions=4,
            )
        dimensions = plan.get("dimensions", [])

    if not dimensions:
        data = {
            "evidence_id": None,
            "dimensions": [],
            "limits": {
                "max_per_dimension": max_per_dimension,
                "max_total_messages": max_total_messages,
            },
        }
        meta = {
            "available": True,
            "source": "retrieve",
            "timing_ms": int((time.time() - started) * 1000),
        }
        return _success(data, meta=meta, tool_name="retrieve_evidence")

    semantic_index = get_semantic_index()
    sem_weight = float(os.getenv("CHATLOG_SEM_WEIGHT", "0.6"))
    kw_weight = float(os.getenv("CHATLOG_KW_WEIGHT", "0.4"))
    weight_sum = sem_weight + kw_weight if (sem_weight + kw_weight) > 0 else 1.0
    sem_weight /= weight_sum
    kw_weight /= weight_sum
    high_info_lines = set(index_loader.get_high_value_messages())

    evidence_store: List[Dict[str, Any]] = []
    dimension_outputs: List[Dict[str, Any]] = []
    remaining_budget = max_total_messages

    for dim in dimensions:
        if remaining_budget <= 0:
            break
        name = dim.get("name") or "Êú™ÂëΩÂêçÁª¥Â∫¶"
        intent = dim.get("intent") or ""
        topic_seeds = _coerce_list(dim.get("topic_seeds"))
        keyword_seeds = _coerce_list(dim.get("keyword_seeds"))
        semantic_queries = _coerce_list(dim.get("semantic_queries"))
        counter_queries = _coerce_list(dim.get("counter_queries"))
        min_evidence = int(dim.get("min_evidence", 3))

        topic_seeds = [t for t in topic_seeds if t in available_topics]
        topic_lines: Dict[int, int] = {}
        for topic in topic_seeds:
            for ln in index_loader.search_by_topic_exact(topic):
                topic_lines[ln] = topic_lines.get(ln, 0) + 1

        semantic_lines: Dict[int, float] = {}
        if use_semantic and semantic_queries and semantic_index.is_available():
            sem_top_k = min(_CHATLOG_MAX_LIST_ITEMS, max_per_dimension * 4)
            for query in semantic_queries:
                for line_num, score in semantic_index.search(query, top_k=sem_top_k):
                    semantic_lines[line_num] = max(
                        semantic_lines.get(line_num, 0.0),
                        max(0.0, min(1.0, (score + 1.0) / 2.0)),
                    )

        keyword_lines: Dict[int, int] = {}
        if keyword_seeds and not topic_lines and not semantic_lines:
            keyword_result = await _search_by_keywords_impl({
                "keywords": keyword_seeds,
                "target_person": target_person,
                "max_results": _CHATLOG_MAX_LIST_ITEMS,
                "match_all": False,
            })
            payload = _extract_payload(keyword_result)
            keyword_data = payload.get("data", {})
            for ln in keyword_data.get("line_numbers", []) or []:
                if isinstance(ln, int):
                    keyword_lines[ln] = keyword_lines.get(ln, 0) + 1

        combined_lines = set(topic_lines.keys()) | set(keyword_lines.keys()) | set(semantic_lines.keys())
        if not combined_lines:
            dimension_outputs.append({
                "name": name,
                "intent": intent,
                "evidence": [],
                "counter_evidence": [],
                "coverage": {
                    "topic_seeds": topic_seeds,
                    "keyword_seeds": keyword_seeds,
                    "semantic_queries": semantic_queries,
                    "counter_queries": counter_queries,
                },
                "omitted_count": 0,
                "next_cursor": None,
                "min_evidence": min_evidence,
            })
            continue

        def _score(line_num: int) -> float:
            score = 0.0
            if line_num in topic_lines or line_num in keyword_lines:
                score += kw_weight
            if line_num in semantic_lines:
                score += sem_weight * semantic_lines[line_num]
            if line_num in high_info_lines:
                score += 0.15
            return score

        ranked_lines = sorted(combined_lines, key=lambda ln: (_score(ln), -ln), reverse=True)
        desired = min(max_per_dimension, remaining_budget)
        selected_lines = ranked_lines[:desired]
        omitted_count = max(0, len(combined_lines) - len(selected_lines))

        messages = index_loader.get_messages_by_lines(
            selected_lines,
            context_before=context_before,
            context_after=context_after,
        )
        formatted_messages: List[Dict[str, Any]] = []
        for msg in messages:
            if not msg.get("is_match"):
                continue
            raw = msg.get("content", "")
            sender, body = _parse_sender_content(raw)
            full_content = body
            if _CHATLOG_MAX_CONTENT_CHARS > 0 and len(full_content) > _CHATLOG_MAX_CONTENT_CHARS:
                full_content = full_content[:_CHATLOG_MAX_CONTENT_CHARS] + "‚Ä¶"
            snippet = _build_snippet(full_content, snippet_chars)
            mentions_target = False
            if target_person:
                mentions_target = target_person in (sender or "") or target_person in full_content
            score = _score(msg.get("line_number", 0))
            formatted_messages.append({
                "line": msg.get("line_number"),
                "time": (msg.get("timestamp") or "")[:19],
                "sender": sender or "Êú™Áü•",
                "content": full_content,
                "snippet": snippet,
                "topics": msg.get("topics", []),
                "metadata": msg.get("metadata", {}),
                "score": round(score, 4),
                "dimension": name,
                "mentions_target": mentions_target,
                "is_counter": False,
            })

        formatted_messages.sort(key=lambda m: (m.get("mentions_target"), m.get("score")), reverse=True)
        formatted_messages = formatted_messages[:desired]
        remaining_budget -= len(formatted_messages)

        counter_evidence: List[Dict[str, Any]] = []
        counter_store: List[Dict[str, Any]] = []
        if use_semantic and counter_queries and semantic_index.is_available():
            counter_lines: Dict[int, float] = {}
            counter_top_k = min(_CHATLOG_MAX_LIST_ITEMS, max(5, int(max_per_dimension / 2)))
            for query in counter_queries:
                for line_num, score in semantic_index.search(query, top_k=counter_top_k):
                    counter_lines[line_num] = max(
                        counter_lines.get(line_num, 0.0),
                        max(0.0, min(1.0, (score + 1.0) / 2.0)),
                    )
            counter_candidates = [ln for ln in counter_lines.keys() if ln not in selected_lines]
            if counter_candidates:
                counter_messages = index_loader.get_messages_by_lines(
                    counter_candidates[:counter_top_k],
                    context_before=0,
                    context_after=0,
                )
                for msg in counter_messages:
                    if not msg.get("is_match"):
                        continue
                    raw = msg.get("content", "")
                    sender, body = _parse_sender_content(raw)
                    full_content = body
                    if _CHATLOG_MAX_CONTENT_CHARS > 0 and len(full_content) > _CHATLOG_MAX_CONTENT_CHARS:
                        full_content = full_content[:_CHATLOG_MAX_CONTENT_CHARS] + "‚Ä¶"
                    snippet = _build_snippet(full_content, snippet_chars)
                    counter_store.append({
                        "line": msg.get("line_number"),
                        "time": (msg.get("timestamp") or "")[:19],
                        "sender": sender or "Êú™Áü•",
                        "content": full_content,
                        "snippet": snippet,
                        "topics": msg.get("topics", []),
                        "metadata": msg.get("metadata", {}),
                        "score": round(counter_lines.get(msg.get("line_number"), 0.0), 4),
                        "dimension": name,
                        "mentions_target": (
                            target_person in (sender or "") or target_person in full_content
                        ) if target_person else False,
                        "is_counter": True,
                    })
                    counter_evidence.append({
                        "line": msg.get("line_number"),
                        "time": (msg.get("timestamp") or "")[:19],
                        "sender": sender or "Êú™Áü•",
                        "snippet": snippet,
                        "score": round(counter_lines.get(msg.get("line_number"), 0.0), 4),
                        "is_counter": True,
                    })
            counter_evidence = counter_evidence[:max(1, int(max_per_dimension / 3))]

        evidence_store.extend(formatted_messages)
        evidence_store.extend(counter_store)

        dimension_outputs.append({
            "name": name,
            "intent": intent,
            "evidence": [
                {
                    "line": m.get("line"),
                    "time": m.get("time"),
                    "sender": m.get("sender"),
                    "snippet": m.get("snippet"),
                    "topics": m.get("topics", []),
                    "score": m.get("score"),
                }
                for m in formatted_messages
            ],
            "counter_evidence": counter_evidence,
            "coverage": {
                "topic_seeds": topic_seeds,
                "keyword_seeds": keyword_seeds,
                "semantic_queries": semantic_queries,
                "counter_queries": counter_queries,
            },
            "omitted_count": omitted_count,
            "next_cursor": None if omitted_count == 0 else f"dimension:{name}#offset={desired}",
            "min_evidence": min_evidence,
        })

    # Step: Optionally compress messages using Poe small model
    use_compression = bool(args.get("use_compression", True))
    if use_compression and evidence_store:
        cleaner = _get_cleaner()
        poe_client = cleaner._get_poe_client()
        if poe_client and poe_client.is_configured:
            try:
                evidence_store = await cleaner.compress_messages(
                    evidence_store,
                    question,
                    target_person=target_person,
                    max_output_messages=max_total_messages,
                    compression_ratio=0.5,
                )
                print(f"[RETRIEVE] ‚úì Êô∫ËÉΩÂéãÁº©: {len(evidence_store)} Êù°Ê∂àÊÅØ")
            except Exception as e:
                print(f"[RETRIEVE] ÂéãÁº©Â§±Ë¥•, ‰ΩøÁî®ÂéüÂßãÊï∞ÊçÆ: {e}")

    evidence_id = _store_evidence({
        "question": question,
        "target_person": target_person,
        "dimensions": dimensions,
        "messages": evidence_store,
    })

    data = {
        "evidence_id": evidence_id,
        "dimensions": dimension_outputs,
        "limits": {
            "max_per_dimension": max_per_dimension,
            "max_total_messages": max_total_messages,
            "snippet_chars": snippet_chars,
            "context_window": f"¬±{context_before}/{context_after}",
        },
        "inputs": {
            "question": question,
            "target_person": target_person,
        },
    }
    meta = {
        "available": True,
        "source": "retrieve",
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="retrieve_evidence")


async def _analyze_evidence_impl(args: dict) -> dict:
    started = time.time()
    evidence_id = args.get("evidence_id")
    messages = args.get("messages") or []
    question = args.get("question", "")
    target_person = args.get("target_person")
    max_examples = min(int(args.get("max_examples", 3)), 5)
    use_llm_analysis = bool(args.get("use_llm_analysis", True))  # ‰ΩøÁî® Poe Â∞èÊ®°ÂûãÁîüÊàêÊô∫ËÉΩÂàÜÊûê

    stored = _get_evidence(evidence_id) if evidence_id else None
    if stored:
        messages = stored.get("messages", []) or []
        question = question or stored.get("question", "")
        target_person = target_person or stored.get("target_person")
        dimensions = stored.get("dimensions", []) or []
    else:
        dimensions = args.get("dimensions") or []

    if not messages:
        return _error(
            "ËØ∑Êèê‰æõ evidence_id ÊàñÊ∂àÊÅØÂàóË°®",
            meta={"source": "analysis"},
            tool_name="analyze_evidence"
        )

    if not dimensions:
        dimensions = [{
            "name": "ÁªºÂêàËØÅÊçÆ",
            "intent": "",
            "min_evidence": 3,
        }]
        for msg in messages:
            msg.setdefault("dimension", "ÁªºÂêàËØÅÊçÆ")

    matrix: List[Dict[str, Any]] = []
    sender_counts: Dict[str, int] = {}
    for msg in messages:
        sender = msg.get("sender", "Êú™Áü•")
        sender_counts[sender] = sender_counts.get(sender, 0) + 1

    for dim in dimensions:
        name = dim.get("name") or "Êú™ÂëΩÂêçÁª¥Â∫¶"
        intent = dim.get("intent") or ""
        min_evidence = int(dim.get("min_evidence", 3))
        dim_messages = [m for m in messages if m.get("dimension") == name and not m.get("is_counter")]
        counter_messages = [m for m in messages if m.get("dimension") == name and m.get("is_counter")]

        dim_messages.sort(key=lambda m: (m.get("mentions_target"), m.get("score", 0)), reverse=True)
        counter_messages.sort(key=lambda m: m.get("score", 0), reverse=True)

        selected = dim_messages[:max_examples]
        counter_selected = counter_messages[:max(1, int(max_examples / 2))] if counter_messages else []

        topics_seen: List[str] = []
        for msg in selected:
            for topic in msg.get("topics", []) or []:
                if topic not in topics_seen:
                    topics_seen.append(topic)
            if len(topics_seen) >= 4:
                break

        if not selected:
            conclusion = "ËØ•Áª¥Â∫¶ËØÅÊçÆ‰∏çË∂≥ÔºåÊöÇÊó†Ê≥ïÂΩ¢ÊàêÁ®≥ÂÆöÁªìËÆ∫„ÄÇ"
        elif counter_selected:
            conclusion = "ËØ•Áª¥Â∫¶Â≠òÂú®‰∫íÁõ∏ÁüõÁõæÁöÑ‰ø°Âè∑ÔºåÈúÄË¶ÅÊõ¥Â§ö‰∏ä‰∏ãÊñáÁ°ÆËÆ§ÂÄæÂêë„ÄÇ"
        else:
            conclusion = "ËØ•Áª¥Â∫¶ËØÅÊçÆÁõ∏ÂØπÈõÜ‰∏≠ÔºåÂëàÁé∞Âá∫‰∏ÄËá¥ÁöÑÂÄæÂêëÊÄß„ÄÇ"

        gaps: List[str] = []
        if len(dim_messages) < min_evidence:
            gaps.append("ËØÅÊçÆÊï∞Èáè‰∏çË∂≥")
        if target_person and not any(m.get("mentions_target") for m in dim_messages):
            gaps.append("ËØÅÊçÆ‰∏≠ÁõÆÊ†á‰∫∫Áâ©Âá∫Áé∞ËæÉÂ∞ë")
        if not counter_messages:
            gaps.append("Áº∫Â∞ëÊòéÁ°ÆÂèçËØÅ")

        if len(dim_messages) >= min_evidence + 2:
            confidence = "high"
        elif len(dim_messages) >= min_evidence:
            confidence = "medium"
        else:
            confidence = "low"

        matrix.append({
            "dimension": name,
            "intent": intent,
            "conclusion": conclusion,
            "evidence": [
                {
                    "line": m.get("line"),
                    "time": m.get("time"),
                    "sender": m.get("sender"),
                    "snippet": m.get("snippet") or _build_snippet(m.get("content", ""), _CHATLOG_EVIDENCE_SNIPPET_CHARS),
                }
                for m in selected
            ],
            "counter_evidence": [
                {
                    "line": m.get("line"),
                    "time": m.get("time"),
                    "sender": m.get("sender"),
                    "snippet": m.get("snippet") or _build_snippet(m.get("content", ""), _CHATLOG_EVIDENCE_SNIPPET_CHARS),
                }
                for m in counter_selected
            ],
            "reasoning": f"ËØÅÊçÆ‰∏ªË¶ÅÈõÜ‰∏≠Âú®: {', '.join(topics_seen) or 'Áõ∏ÂÖ≥ÂØπËØù'}„ÄÇ",
            "gaps": gaps,
            "confidence": confidence,
        })

    # Step: Optionally use LLM for intelligent analysis
    llm_matrix = None
    if use_llm_analysis and matrix:
        cleaner = _get_cleaner()
        poe_client = cleaner._get_poe_client()
        if poe_client and poe_client.is_configured:
            try:
                llm_matrix = await cleaner.generate_evidence_matrix(
                    matrix,  # Pass the basic matrix as dimension_evidence
                    question,
                    target_person,
                )
                if llm_matrix and llm_matrix.get("method") == "llm":
                    # Merge LLM analysis into matrix
                    llm_dims = {d.get("name"): d for d in llm_matrix.get("dimensions", [])}
                    for m in matrix:
                        llm_dim = llm_dims.get(m.get("dimension"))
                        if llm_dim:
                            m["conclusion"] = llm_dim.get("conclusion", m.get("conclusion"))
                            m["reasoning"] = llm_dim.get("reasoning_chain", m.get("reasoning"))
                            if llm_dim.get("gaps"):
                                m["gaps"] = llm_dim.get("gaps")
                            if llm_dim.get("confidence"):
                                m["confidence"] = llm_dim.get("confidence")
            except Exception as e:
                print(f"[ANALYZE] LLM matrix generation failed: {e}")

    data = {
        "evidence_id": evidence_id,
        "matrix": matrix,
        "overview": {
            "message_count": len(messages),
            "sender_counts": sender_counts,
            "target_person": target_person,
        },
        "framework": _task_sub_questions(_infer_task_type(question)),
        "overall_conclusion": llm_matrix.get("overall_conclusion") if llm_matrix else None,
        "evidence_quality": llm_matrix.get("evidence_quality") if llm_matrix else None,
        "analysis_method": "llm" if (llm_matrix and llm_matrix.get("method") == "llm") else "rule_based",
        "disclaimer": "ÂàÜÊûê‰ªÖÂü∫‰∫éËÅäÂ§©ËÆ∞ÂΩïËØÅÊçÆÔºå‰∏çÊûÑÊàêÊúÄÁªàÂÜ≥Á≠ñÂª∫ËÆÆ„ÄÇ",
    }
    meta = {
        "available": True,
        "source": "analysis",
        "llm_used": bool(llm_matrix and llm_matrix.get("method") == "llm"),
        "timing_ms": int((time.time() - started) * 1000),
    }
    return _success(data, meta=meta, tool_name="analyze_evidence")


async def _search_person_impl(args: dict) -> dict:
    """Internal implementation of search_person."""
    person = args.get("person", "")
    include_context = bool(args.get("include_context", False))
    max_messages = min(int(args.get("max_messages", _CHATLOG_MAX_LIST_ITEMS)), 200)
    context_before = min(int(args.get("context_before", 1)), 3)
    context_after = min(int(args.get("context_after", 1)), 3)

    if not person:
        return _error(
            "ÈîôËØØÔºöËØ∑Êèê‰æõ‰∫∫Áâ©ÂêçÁß∞„ÄÇ",
            meta={"source": "search_person"},
            tool_name="search_person"
        )

    loader = _get_loader()

    if not loader.is_loaded:
        if not loader.load():
            return _error(
                "ÈîôËØØÔºöÊó†Ê≥ïÂä†ËΩΩËÅäÂ§©ËÆ∞ÂΩïÊñá‰ª∂",
                meta={"source": "search_person"},
                tool_name="search_person"
            )

    person_messages = loader.get_messages_by_sender(person)

    if not person_messages:
        return _success(
            {
                "person": person,
                "messages": [],
                "total_messages": 0,
                "returned_count": 0,
                "omitted_count": 0,
                "next_cursor": None,
            },
            meta={"source": "search_person", "available": True},
            tool_name="search_person"
        )

    line_numbers: List[int] = []
    if include_context:
        line_set = set()
        for msg in person_messages[:max_messages]:
            for ln in range(max(1, msg.line_number - context_before), msg.line_number + context_after + 1):
                line_set.add(ln)
        line_numbers = sorted(line_set)
    else:
        line_numbers = [msg.line_number for msg in person_messages[:max_messages]]

    items = []
    for ln in line_numbers:
        msg = loader.get_message(ln)
        if msg:
            items.append({
                "line": msg.line_number,
                "time": (msg.timestamp or "")[:19],
                "sender": msg.sender or "Êú™Áü•",
                "content": _build_snippet(msg.content or "", _CHATLOG_SNIPPET_CHARS),
                "is_match": msg.sender == person,
            })

    limited, omitted_count, next_cursor = _truncate_list(
        items,
        max_messages,
        cursor_prefix=f"person:{person}"
    )

    data = {
        "person": person,
        "messages": limited,
        "total_messages": len(person_messages),
        "returned_count": len(limited),
        "omitted_count": omitted_count,
        "next_cursor": next_cursor,
        "include_context": include_context,
    }
    meta = {
        "available": True,
        "source": "search_person",
    }
    return _success(data, meta=meta, tool_name="search_person")


# Tool-decorated versions (for MCP)
@tool(
    "get_chatlog_stats",
    "Ëé∑ÂèñËÅäÂ§©ËÆ∞ÂΩïÁöÑÁªüËÆ°‰ø°ÊÅØÔºåÂåÖÊã¨ÊÄªÊ∂àÊÅØÊï∞„ÄÅÂèëÈÄÅËÄÖÂàóË°®Á≠â„ÄÇ",
    {}
)
async def get_chatlog_stats(args: dict) -> dict:
    """Get statistics about the loaded chatlog."""
    return await _get_chatlog_stats_impl(args)


@tool(
    "search_person",
    "ÊêúÁ¥¢ÁâπÂÆö‰∫∫Áâ©ÁöÑÊâÄÊúâÁõ∏ÂÖ≥Ê∂àÊÅØËÆ∞ÂΩï„ÄÇ",
    {
        "person": str,            # ‰∫∫Áâ©ÂêçÁß∞
        "include_context": bool,  # ÂèØÈÄâÔºöÊòØÂê¶ÂåÖÂê´‰∏ä‰∏ãÊñáÔºàÈªòËÆ§falseÔºâ
        "max_messages": int,
        "context_before": int,
        "context_after": int
    }
)
async def search_person(args: dict) -> dict:
    """Search for all messages related to a specific person."""
    return await _search_person_impl(args)


@tool(
    "list_topics",
    "ÂàóÂá∫ËÅäÂ§©ËÆ∞ÂΩïÁ¥¢Âºï‰∏≠ÁöÑËØùÈ¢òÊ†áÁ≠æ„ÄÇ",
    {
        "limit": int,
        "pattern": str
    }
)
async def list_topics(args: dict) -> dict:
    return await _list_topics_impl(args)


@tool(
    "search_by_topics",
    "Ê†πÊçÆËØùÈ¢òÊ†áÁ≠æÊ£ÄÁ¥¢Ê∂àÊÅØË°åÂè∑„ÄÇ",
    {
        "topics": list,
        "max_results": int
    }
)
async def search_by_topics(args: dict) -> dict:
    return await _search_by_topics_impl(args)


@tool(
    "search_by_keywords",
    "Ê†πÊçÆÂÖ≥ÈîÆËØçÂÖ®ÊñáÊ£ÄÁ¥¢Ê∂àÊÅØË°åÂè∑„ÄÇÂèØÈôêÂÆöÂèëÈÄÅËÄÖ„ÄÇ",
    {
        "keywords": list,
        "target_person": str,
        "max_results": int,
        "match_all": bool
    }
)
async def search_by_keywords(args: dict) -> dict:
    return await _search_by_keywords_impl(args)


@tool(
    "load_messages",
    "Ê†πÊçÆË°åÂè∑Âä†ËΩΩÊ∂àÊÅØÂÜÖÂÆπÔºåÂèØÈÄâÂåÖÂê´‰∏ä‰∏ãÊñá‰∏éÂÖÉÊï∞ÊçÆ„ÄÇ",
    {
        "line_numbers": list,
        "context_before": int,
        "context_after": int,
        "include_metadata": bool,
        "max_messages": int,
        "max_content_chars": int
    }
)
async def load_messages(args: dict) -> dict:
    return await _load_messages_impl(args)


@tool(
    "expand_query",
    "Â∞ÜÈóÆÈ¢òÊâ©Â±ï‰∏∫ÂÖ≥ÈîÆËØçÂíåËØùÈ¢òÊ†áÁ≠æÔºàLLM ÂèØÈÄâÔºâ„ÄÇ",
    {
        "question": str,
        "target_person": str,
        "use_llm": bool
    }
)
async def expand_query(args: dict) -> dict:
    return await _expand_query_impl(args)


@tool(
    "search_semantic",
    "‰ΩøÁî®ËØ≠‰πâÂêëÈáèÂè¨ÂõûÁõ∏‰ººÊ∂àÊÅØ„ÄÇ",
    {
        "query": str,
        "top_k": int
    }
)
async def search_semantic(args: dict) -> dict:
    return await _search_semantic_impl(args)


@tool(
    "filter_by_person",
    "ËøáÊª§Ê∂àÊÅØÔºåÁ°Æ‰øùÂÜÖÂÆπ‰∏éÁõÆÊ†á‰∫∫Áâ©Áõ∏ÂÖ≥„ÄÇ",
    {
        "messages": list,
        "target_person": str,
        "use_llm": bool
    }
)
async def filter_by_person(args: dict) -> dict:
    return await _filter_by_person_impl(args)


@tool(
    "format_messages",
    "Ê†ºÂºèÂåñÊ∂àÊÅØÂàóË°®‰∏∫ÊñáÊú¨„ÄÇ",
    {
        "messages": list,
        "format": str,
        "max_chars": int
    }
)
async def format_messages(args: dict) -> dict:
    return await _format_messages_impl(args)


@tool(
    "parse_task",
    "Ëß£ÊûêÁî®Êà∑ÈóÆÈ¢ò‰∏∫‰ªªÂä°Á±ªÂûã‰∏éËØÅÊçÆÁª¥Â∫¶ËÆ°Âàí„ÄÇ",
    {
        "question": str,
        "target_person": str,
        "use_llm": bool,
        "max_dimensions": int
    }
)
async def parse_task(args: dict) -> dict:
    return await _parse_task_impl(args)


@tool(
    "retrieve_evidence",
    "ÊåâÁª¥Â∫¶Ê£ÄÁ¥¢ËØÅÊçÆÔºåËøîÂõûËØÅÊçÆÊëòË¶Å‰∏é evidence_id„ÄÇ",
    {
        "question": str,
        "target_person": str,
        "dimensions": list,
        "max_per_dimension": int,
        "max_total_messages": int,
        "snippet_chars": int,
        "context_before": int,
        "context_after": int,
        "use_semantic": bool,
        "use_llm_plan": bool
    }
)
async def retrieve_evidence(args: dict) -> dict:
    return await _retrieve_evidence_impl(args)


@tool(
    "analyze_evidence",
    "Âü∫‰∫é evidence_id ÊàñËØÅÊçÆÂàóË°®ËæìÂá∫ËØÅÊçÆÁü©Èòµ„ÄÇ",
    {
        "evidence_id": str,
        "messages": list,
        "question": str,
        "target_person": str,
        "dimensions": list,
        "max_examples": int
    }
)
async def analyze_evidence(args: dict) -> dict:
    return await _analyze_evidence_impl(args)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MCP Server Creation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_chatlog_mcp_server(chatlog_path: Optional[str] = None):
    """
    Create the Chatlog MCP server.
    
    Args:
        chatlog_path: Optional path to chatlog JSONL file
        
    Returns:
        An MCP server that can be passed to ClaudeAgentOptions.mcp_servers
    """
    global _chatlog_loader
    
    # Initialize loader with custom path if provided
    if chatlog_path:
        _chatlog_loader = ChatlogLoader(chatlog_path)
    
    tool_profile = os.getenv("CHATLOG_TOOL_PROFILE", "slim").lower()
    core_tools = [parse_task, retrieve_evidence, analyze_evidence]
    if tool_profile in ("full", "debug"):
        tools = [
            get_chatlog_stats,
            search_person,
            list_topics,
            search_by_topics,
            search_by_keywords,
            load_messages,
            expand_query,
            search_semantic,
            filter_by_person,
            format_messages,
            *core_tools,
        ]
    elif tool_profile == "stats":
        tools = [get_chatlog_stats, *core_tools]
    else:
        tools = core_tools

    return create_sdk_mcp_server(
        name="chatlog",
        version="1.0.0",
        tools=tools,
    )


def get_chatlog_tools_info() -> List[Dict[str, str]]:
    """Get information about available chatlog tools for documentation."""
    tool_profile = os.getenv("CHATLOG_TOOL_PROFILE", "slim").lower()
    tools = [
        {
            "name": "mcp__chatlog__parse_task",
            "description": "Ëß£ÊûêÈóÆÈ¢ò‰∏∫‰ªªÂä°Á±ªÂûã‰∏éËØÅÊçÆÁª¥Â∫¶ËÆ°Âàí",
            "usage": "ÂÖ•Âè£ÔºöÁîüÊàêËØÅÊçÆÁª¥Â∫¶ËÆ°Âàí"
        },
        {
            "name": "mcp__chatlog__retrieve_evidence",
            "description": "ÊåâÁª¥Â∫¶Ê£ÄÁ¥¢ËØÅÊçÆÂπ∂ËøîÂõû evidence_id",
            "usage": "Ê£ÄÁ¥¢ËØÅÊçÆÊëòË¶ÅÔºåÈÅøÂÖçÂ§ßÊñáÊú¨Âõû‰º†"
        },
        {
            "name": "mcp__chatlog__analyze_evidence",
            "description": "Âü∫‰∫é evidence_id ‰∫ßÂá∫ËØÅÊçÆÁü©Èòµ",
            "usage": "ËæìÂá∫Áª¥Â∫¶ÁªìËÆ∫„ÄÅËØÅÊçÆ‰∏éÂèçËØÅ"
        },
    ]

    if tool_profile in ("full", "debug", "stats"):
        tools = [
            {
                "name": "mcp__chatlog__get_chatlog_stats",
                "description": "Ëé∑ÂèñËÅäÂ§©ËÆ∞ÂΩïÁªüËÆ°‰ø°ÊÅØ",
                "usage": "Êü•ÁúãËÅäÂ§©ËÆ∞ÂΩïÊ¶ÇÂÜµÊó∂Ë∞ÉÁî®"
            },
            *tools,
        ]

    if tool_profile in ("full", "debug"):
        tools = [
            *tools,
            {
                "name": "mcp__chatlog__search_person",
                "description": "ÊêúÁ¥¢ÁâπÂÆö‰∫∫Áâ©ÁöÑÊ∂àÊÅØËÆ∞ÂΩï",
                "usage": "ÈúÄË¶Å‰∫ÜËß£Êüê‰∏™‰∫∫ÁöÑÂéÜÂè≤Ê∂àÊÅØÊó∂Ë∞ÉÁî®"
            },
            {
                "name": "mcp__chatlog__list_topics",
                "description": "ÂàóÂá∫ËÅäÂ§©ËÆ∞ÂΩïÁ¥¢Âºï‰∏≠ÁöÑËØùÈ¢òÊ†áÁ≠æ",
                "usage": "Ë∞ÉËØïÂèØÁî®ËØùÈ¢òËåÉÂõ¥"
            },
            {
                "name": "mcp__chatlog__search_by_topics",
                "description": "ÊåâËØùÈ¢òÊ†áÁ≠æËøîÂõûÂåπÈÖçË°åÂè∑",
                "usage": "Ë∞ÉËØïËØùÈ¢òÁ¥¢ÂºïÂè¨Âõû"
            },
            {
                "name": "mcp__chatlog__search_by_keywords",
                "description": "ÊåâÂÖ≥ÈîÆËØçÊ£ÄÁ¥¢Ê∂àÊÅØË°åÂè∑",
                "usage": "Ë∞ÉËØïÂÖ≥ÈîÆËØçÂè¨Âõû"
            },
            {
                "name": "mcp__chatlog__load_messages",
                "description": "ÊåâË°åÂè∑Âä†ËΩΩÊ∂àÊÅØ‰∏é‰∏ä‰∏ãÊñá",
                "usage": "Ë∞ÉËØïÊ∂àÊÅØÂä†ËΩΩ"
            },
            {
                "name": "mcp__chatlog__expand_query",
                "description": "Â∞ÜÈóÆÈ¢òÊâ©Â±ï‰∏∫ÂÖ≥ÈîÆËØçÂíåËØùÈ¢ò",
                "usage": "Ë∞ÉËØïÂÖ≥ÈîÆËØç/ËØùÈ¢òÊâ©Â±ï"
            },
            {
                "name": "mcp__chatlog__search_semantic",
                "description": "ËØ≠‰πâÂêëÈáèÂè¨ÂõûÁõ∏‰ººÊ∂àÊÅØ",
                "usage": "Ë∞ÉËØïËØ≠‰πâÂè¨Âõû"
            },
            {
                "name": "mcp__chatlog__filter_by_person",
                "description": "ËøáÊª§‰∏éÁõÆÊ†á‰∫∫Áâ©Áõ∏ÂÖ≥ÁöÑÊ∂àÊÅØ",
                "usage": "Ë∞ÉËØï‰∫∫ÂêçÂΩíÂõ†"
            },
            {
                "name": "mcp__chatlog__format_messages",
                "description": "Ê†ºÂºèÂåñÊ∂àÊÅØÂàóË°®‰∏∫ÊñáÊú¨",
                "usage": "Ë∞ÉËØïÊ†ºÂºèÂåñËæìÂá∫"
            },
        ]

    return tools


async def close_chatlog_clients() -> None:
    """Close any chatlog-related async clients (e.g., Poe session)."""
    global _chatlog_cleaner
    if _chatlog_cleaner is None:
        return
    try:
        await _chatlog_cleaner.close()
    except Exception:
        pass
    _chatlog_cleaner = None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Synchronous API for direct usage
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def compose_chatlog_query_sync(
    question: str,
    target_person: Optional[str] = None,
    max_results: int = 100
) -> str:
    """Synchronous wrapper for composed chatlog query (internal use)."""
    args = {
        "question": question,
        "target_person": target_person,
        "max_results": max_results
    }
    
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_query_chatlog_composed_impl(args))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)


def compose_chatlog_analysis_sync(
    question: str,
    target_person: Optional[str] = None,
    max_dimensions: int = 4
) -> str:
    """Synchronous wrapper for the parse->retrieve->analyze flow."""
    args = {
        "question": question,
        "target_person": target_person,
        "max_dimensions": max_dimensions,
    }

    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    parse_result = loop.run_until_complete(_parse_task_impl(args))
    parse_payload = _extract_payload(parse_result)
    parse_data = parse_payload.get("data", {})
    dimensions = parse_data.get("dimensions", []) or []

    retrieve_result = loop.run_until_complete(_retrieve_evidence_impl({
        "question": question,
        "target_person": target_person,
        "dimensions": dimensions,
    }))
    retrieve_payload = _extract_payload(retrieve_result)
    retrieve_data = retrieve_payload.get("data", {})
    evidence_id = retrieve_data.get("evidence_id")

    analyze_result = loop.run_until_complete(_analyze_evidence_impl({
        "evidence_id": evidence_id,
        "question": question,
        "target_person": target_person,
        "dimensions": dimensions,
    }))
    analyze_payload = _extract_payload(analyze_result)
    analyze_data = analyze_payload.get("data", {})

    lines: List[str] = []
    lines.append("## ËØÅÊçÆÂàÜÊûê")
    lines.append("")
    lines.append(f"**ÈóÆÈ¢ò**: {question}")
    if target_person:
        lines.append(f"**ÁõÆÊ†á‰∫∫Áâ©**: {target_person}")
    lines.append(f"**evidence_id**: {evidence_id or 'Êó†'}")
    lines.append("")

    for item in analyze_data.get("matrix", []):
        lines.append(f"### {item.get('dimension', 'Êú™ÂëΩÂêçÁª¥Â∫¶')}")
        lines.append(f"- ÁªìËÆ∫: {item.get('conclusion', '')}")
        lines.append(f"- ÁΩÆ‰ø°Â∫¶: {item.get('confidence', '')}")
        lines.append(f"- Êé®Êñ≠: {item.get('reasoning', '')}")
        gaps = item.get("gaps") or []
        if gaps:
            lines.append(f"- Áº∫Âè£: {', '.join(gaps)}")

        evidence = item.get("evidence") or []
        if evidence:
            lines.append("- ËØÅÊçÆ:")
            for ev in evidence:
                snippet = ev.get("snippet", "")
                sender = ev.get("sender", "Êú™Áü•")
                line_no = ev.get("line")
                lines.append(f"  - [{line_no}] {sender}: {snippet}")

        counter = item.get("counter_evidence") or []
        if counter:
            lines.append("- ÂèçËØÅ:")
            for ev in counter:
                snippet = ev.get("snippet", "")
                sender = ev.get("sender", "Êú™Áü•")
                line_no = ev.get("line")
                lines.append(f"  - [{line_no}] {sender}: {snippet}")

        lines.append("")

    return "\n".join(lines).strip()


def get_chatlog_stats_sync() -> str:
    """Synchronous wrapper for get_chatlog_stats."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    result = loop.run_until_complete(_get_chatlog_stats_impl({}))
    
    # Extract text from result
    if "content" in result and result["content"]:
        return result["content"][0].get("text", "")
    return str(result)

