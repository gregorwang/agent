# Kimi ä¸ Claude Agent SDK æŠ€æœ¯å®ç°æŠ¥å‘Š
## æµå¼è¾“å‡ºã€æ€ç»´é“¾ä¸Tokenç»Ÿè®¡

> **ç”Ÿæˆæ—¥æœŸ**: 2026-01-12  
> **ç‰ˆæœ¬**: 1.0  
> **ç›®çš„**: è¯¦ç»†åˆ†æKimi APIå’ŒClaude Agent SDKå¦‚ä½•å®ç°æµå¼è¾“å‡ºã€æ€ç»´é“¾å±•ç¤ºå’ŒTokenä½¿ç”¨ç»Ÿè®¡

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šè¯¦ç»†å¯¹æ¯”äº† **Kimi API** å’Œ **Claude Agent SDK** åœ¨ä¸‰ä¸ªå…³é”®åŠŸèƒ½ä¸Šçš„å®ç°æ–¹æ¡ˆï¼š
1. **æµå¼è¾“å‡ºï¼ˆStreaming Outputï¼‰**
2. **æ€ç»´é“¾å±•ç¤ºï¼ˆThinking/Reasoning Chainï¼‰**
3. **Tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆToken Usage Statisticsï¼‰**

ä¸¤ä¸ªSDKéƒ½æä¾›äº†æˆç†Ÿçš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨å®ç°ç»†èŠ‚å’Œæ¶æ„è®¾è®¡ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

---

## ä¸€ã€æµå¼è¾“å‡ºï¼ˆStreaming Outputï¼‰

### 1.1 Kimi API æµå¼è¾“å‡º

#### æ ¸å¿ƒæœºåˆ¶
- **OpenAIå…¼å®¹**: Kimi API å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼ï¼Œå¯ç›´æ¥ä½¿ç”¨ OpenAI SDK
- **SSEåè®®**: ä½¿ç”¨ Server-Sent Events (SSE) å®ç°å¢é‡æµå¼ä¼ è¾“
- **å³æ—¶å“åº”**: Token ç”Ÿæˆåç«‹å³å‘é€ç»™å®¢æˆ·ç«¯ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´å›å¤

#### å®ç°æ–¹å¼

**Python ç¤ºä¾‹**:
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_KIMI_API_KEY",
    base_url="https://api.moonshot.cn/v1"
)

# å¼€å¯æµå¼è¾“å‡º
response = client.chat.completions.create(
    model="moonshot-v1-8k",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True  # å…³é”®å‚æ•°ï¼šå¯ç”¨æµå¼è¾“å‡º
)

# å¤„ç†æµå¼å“åº”
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

**Node.js ç¤ºä¾‹**:
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'YOUR_KIMI_API_KEY',
  baseURL: 'https://api.moonshot.cn/v1',
});

const stream = await client.chat.completions.create({
  model: 'moonshot-v1-8k',
  messages: [{ role: 'user', content: 'ä½ å¥½' }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

#### å…³é”®ç‰¹æ€§
- âœ… **å‚æ•°ç®€å•**: ä»…éœ€è®¾ç½® `stream=True`
- âœ… **å…¼å®¹æ€§å¼º**: æ”¯æŒæ‰€æœ‰ Kimi æ¨¡å‹ï¼ˆmoonshot-v1-8k/32k/128kï¼‰
- âœ… **ç¨³å®šæ€§é«˜**: åŸºäºæˆç†Ÿçš„ SSE åè®®
- âš ï¸ **ç½‘ç»œæ•æ„Ÿ**: é•¿æ—¶é—´ç”Ÿæˆå¯èƒ½éœ€è¦è¶…æ—¶ä¿æŠ¤

---

### 1.2 Claude Agent SDK æµå¼è¾“å‡º

#### æ ¸å¿ƒæœºåˆ¶
- **åŒæ¨¡å¼æ”¯æŒ**: åŒæ—¶æ”¯æŒ Streaming Mode å’Œ Single Input Mode
- **äº‹ä»¶é©±åŠ¨**: é€šè¿‡äº‹ä»¶ç±»å‹ï¼ˆmessage_start, content_block_deltaç­‰ï¼‰ç»“æ„åŒ–è¾“å‡º
- **å¼‚æ­¥è¿­ä»£å™¨**: Pythonä½¿ç”¨å¼‚æ­¥è¿­ä»£å™¨ï¼ŒTypeScriptä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨

#### å®ç°æ–¹å¼

**Python SDK (æ¨èStreaming Mode)**:
```python
from claude_agent_sdk import query

# æ–¹å¼1: ä½¿ç”¨ query() å‡½æ•° (å•æ¬¡äº¤äº’)
async for message in query(
    prompt="è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
    tools=[...],  # å¯é€‰å·¥å…·åˆ—è¡¨
):
    # message æ˜¯æµå¼è¿”å›çš„å¢é‡å†…å®¹
    if message.type == "content_block_delta":
        print(message.delta.text, end="", flush=True)
    elif message.type == "thinking":
        print(f"[æ€è€ƒä¸­: {message.content}]")
    elif message.type == "tool_use":
        print(f"[è°ƒç”¨å·¥å…·: {message.tool_name}]")

# æ–¹å¼2: ä½¿ç”¨ ClaudeSDKClient (æŒç»­ä¼šè¯)
from claude_agent_sdk import ClaudeSDKClient

client = ClaudeSDKClient()
async for event in client.stream_message(
    prompt="ç»§ç»­ä¸Šæ¬¡çš„è®¨è®º",
    preserve_history=True
):
    # å¤„ç†æµå¼äº‹ä»¶
    match event.type:
        case "message_start":
            print("\n--- æ–°æ¶ˆæ¯å¼€å§‹ ---")
        case "content_block_delta":
            print(event.delta.text, end="")
        case "message_stop":
            print("\n--- æ¶ˆæ¯ç»“æŸ ---")
```

**TypeScript SDK**:
```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

async function* streamResponse() {
  for await (const message of query({
    prompt: "åˆ†æè¿™æ®µä»£ç ",
    tools: [...],
  })) {
    yield message;
  }
}

// ä½¿ç”¨ç¤ºä¾‹
for await (const msg of streamResponse()) {
  if (msg.type === 'content_block_delta') {
    process.stdout.write(msg.delta.text);
  }
}
```

#### äº‹ä»¶ç±»å‹è¯¦è§£

| äº‹ä»¶ç±»å‹ | è¯´æ˜ | åŒ…å«å­—æ®µ |
|---------|------|---------|
| `message_start` | æ¶ˆæ¯å¼€å§‹ | `message.id`, `message.role` |
| `content_block_start` | å†…å®¹å—å¼€å§‹ | `content_block.type`, `index` |
| `content_block_delta` | å¢é‡æ–‡æœ¬æ›´æ–° | `delta.text`, `index` |
| `content_block_stop` | å†…å®¹å—ç»“æŸ | `index` |
| `message_delta` | æ¶ˆæ¯å…ƒæ•°æ®æ›´æ–° | `delta.stop_reason`, `usage` |
| `message_stop` | æ¶ˆæ¯å®Œå…¨ç»“æŸ | - |

#### é«˜çº§ç‰¹æ€§ï¼šç»†ç²’åº¦å·¥å…·æµå¼ä¼ è¾“

**é—®é¢˜**: ä¼ ç»Ÿæ–¹å¼éœ€è¦ç­‰å¾…å®Œæ•´ JSON å·¥å…·å‚æ•°ç”Ÿæˆå®Œæˆ  
**è§£å†³æ–¹æ¡ˆ**: Claude æ”¯æŒå·¥å…·å‚æ•°å¢é‡æµå¼ä¼ è¾“

```python
async for event in client.stream_message(prompt="..."):
    if event.type == "tool_use_delta":
        # æ— éœ€ç­‰å¾…å®Œæ•´ JSONï¼Œç›´æ¥å¤„ç†å¢é‡å‚æ•°
        partial_params = event.delta.partial_json
        # å¯ä»¥å¼€å§‹é¢„å¤„ç†æˆ–æ˜¾ç¤ºè¿›åº¦
        print(f"å·¥å…·å‚æ•°è¿›å±•: {partial_params}")
```

**ä¼˜åŠ¿**:
- âš¡ å‡å°‘é¦–å­—èŠ‚å»¶è¿Ÿ
- ğŸ“Š å®æ—¶æ˜¾ç¤ºå·¥å…·è°ƒç”¨è¿›åº¦
- ğŸ”„ æ”¯æŒå¤§å‹å‚æ•°ä¼ é€’çš„æ¸è¿›å¼å¤„ç†

---

### 1.3 æµå¼è¾“å‡ºå¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | Kimi API | Claude Agent SDK |
|-----|---------|-----------------|
| **å®ç°å¤æ‚åº¦** | â­ ç®€å• (`stream=True`) | â­â­ ä¸­ç­‰ï¼ˆéœ€å¤„ç†å¤šç§äº‹ä»¶ç±»å‹ï¼‰ |
| **åè®®** | SSEï¼ˆServer-Sent Eventsï¼‰ | SSE + ç»“æ„åŒ–äº‹ä»¶ |
| **å·¥å…·è°ƒç”¨æµå¼** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒç»†ç²’åº¦æµå¼å·¥å…·å‚æ•° |
| **ä¸Šä¸‹æ–‡æŒä¹…åŒ–** | éœ€æ‰‹åŠ¨ç®¡ç† | âœ… å†…ç½®ä¼šè¯ç®¡ç† |
| **æ€ç»´é“¾æµå¼** | âœ… é€šè¿‡ `reasoning_content` | âœ… é€šè¿‡ `ThinkingBlock` |
| **é”™è¯¯æ¢å¤** | éœ€è‡ªè¡Œå®ç° | âœ… å†…ç½®é‡è¿æœºåˆ¶ |

---

## äºŒã€æ€ç»´é“¾å±•ç¤ºï¼ˆThinking/Reasoning Chainï¼‰

### 2.1 Kimi K2 Thinking æ¨¡å‹

#### æ¨¡å‹æ¦‚è¿°
- **æ¨¡å‹åç§°**: `kimi-k2-thinking`
- **ä¸“é•¿**: å¤æ‚æ¨ç†ã€å¤šæ­¥é—®é¢˜è§£å†³ã€Agenticå·¥ä½œæµ
- **æ ¸å¿ƒèƒ½åŠ›**: æ·±åº¦æ¨ç†ã€å·¥å…·ç¼–æ’ï¼ˆ200-300æ¬¡è¿ç»­è°ƒç”¨ï¼‰ã€è‡ªä¸»å¯¼èˆª

#### æ€ç»´é“¾å®ç°

**API å“åº”ç»“æ„**:
```json
{
  "id": "cmpl-xxx",
  "choices": [{
    "message": {
      "role": "assistant",
      "reasoning_content": "æˆ‘éœ€è¦å…ˆåˆ†æé—®é¢˜çš„ä¸‰ä¸ªç»´åº¦ï¼š1) æŠ€æœ¯å¯è¡Œæ€§... 2) æˆæœ¬æ•ˆç›Š... 3) é£é™©è¯„ä¼°...",
      "content": "åŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘çš„å»ºè®®æ˜¯..."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 300,
    "total_tokens": 450
  }
}
```

**Python å®ç°ï¼ˆå«æ€ç»´é“¾æå–ï¼‰**:
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_KIMI_API_KEY",
    base_url="https://api.moonshot.cn/v1"
)

response = client.chat.completions.create(
    model="kimi-k2-thinking",
    messages=[
        {"role": "user", "content": "è¯æ˜è´¹é©¬å¤§å®šç†"}
    ],
    stream=True,
    max_tokens=16000,      # å»ºè®®>=16000ä»¥ç¡®ä¿å®Œæ•´è¾“å‡º
    temperature=1.0        # å»ºè®®è®¾ä¸º1.0ä»¥è·å¾—æœ€ä½³æ¨ç†æ€§èƒ½
)

reasoning_parts = []
content_parts = []

for chunk in response:
    delta = chunk.choices[0].delta
    
    # æå–æ€ç»´é“¾ï¼ˆä½¿ç”¨ hasattr æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§ï¼‰
    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
        reasoning_parts.append(delta.reasoning_content)
        print(f"ğŸ’­ {delta.reasoning_content}", end="", flush=True)
    
    # æå–æœ€ç»ˆå›å¤
    if hasattr(delta, 'content') and delta.content:
        content_parts.append(delta.content)
        print(f"ğŸ“ {delta.content}", end="", flush=True)

# å®Œæ•´æ€ç»´é“¾
full_reasoning = ''.join(reasoning_parts)
full_content = ''.join(content_parts)
```

#### æµå¼è¾“å‡ºä¸­çš„é¡ºåº
```
1. reasoning_content å…ˆè¾“å‡º â†’ "æˆ‘éœ€è¦æ€è€ƒ..."
2. content åè¾“å‡º â†’ "ç­”æ¡ˆæ˜¯..."
```

**éæµå¼è¾“å‡ºæå–**:
```python
response = client.chat.completions.create(
    model="kimi-k2-thinking",
    messages=[...],
    stream=False
)

message = response.choices[0].message

# ä½¿ç”¨ getattr å®‰å…¨æå–
reasoning = getattr(message, 'reasoning_content', None)
content = message.content

if reasoning:
    print(f"ğŸ§  æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n")
print(f"âœ… æœ€ç»ˆç­”æ¡ˆ:\n{content}")
```

#### å…³é”®é…ç½®å»ºè®®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|-----|-------|------|
| `max_tokens` | >=16000 | ç¡®ä¿æ€ç»´é“¾å’Œå›å¤å®Œæ•´è¾“å‡º |
| `temperature` | 1.0 | è·å¾—æœ€ä½³æ¨ç†æ€§èƒ½ |
| `stream` | True | é¿å…ç½‘ç»œè¶…æ—¶ï¼Œæå‡ç”¨æˆ·ä½“éªŒ |

#### Tokenè®¡è´¹è¯´æ˜
âš ï¸ **é‡è¦**: `reasoning_content` ä¸­çš„Token **ä¼šè®¡å…¥** `max_tokens` æ¶ˆè€—å’Œè®¡è´¹

---

### 2.2 Claude Agent SDK æ€ç»´æ¨¡å¼

#### æ ¸å¿ƒæ¦‚å¿µ

**1. Extended Thinking (æ‰©å±•æ€è€ƒ)**
- æ¨¡å‹å†…éƒ¨ç”Ÿæˆ `ThinkingBlock` è¯¦ç»†é˜è¿°æ¨ç†æ­¥éª¤
- æ”¯æŒæ¨¡å‹ï¼šClaude Opus 4.5+
- ç”¨é€”ï¼šè°ƒè¯•ã€å¼•å¯¼ã€é€æ˜åŒ–å†³ç­–

**2. Interleaved Thinking (äº¤é”™æ€è€ƒ)**
- åœ¨å¤šä¸ªå·¥å…·è°ƒç”¨ä¹‹é—´æ’å…¥æ¨ç†æ­¥éª¤
- å…è®¸é“¾å¼å·¥å…·è°ƒç”¨ + ä¸­é—´æ¨ç†
- å®ç°å¤æ‚çš„å¤šæ­¥éª¤å†³ç­–

#### å®ç°ç¤ºä¾‹

**å¯ç”¨æ‰©å±•æ€è€ƒ**:
```python
from anthropic import Anthropic

client = Anthropic(api_key="YOUR_API_KEY")

response = client.messages.create(
    model="claude-opus-4.5",
    max_tokens=4096,
    thinking={
        "type": "enabled",
        "budget_tokens": 2000  # ä¸ºæ€è€ƒåˆ†é…çš„Tokené¢„ç®—
    },
    messages=[
        {"role": "user", "content": "è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨åˆ†å¸ƒå¼ç³»ç»Ÿ"}
    ]
)

# å¤„ç†å“åº”ä¸­çš„æ€ç»´å—
for block in response.content:
    if block.type == "thinking":
        print(f"ğŸ§  å†…éƒ¨æ¨ç†:\n{block.thinking}\n")
    elif block.type == "text":
        print(f"ğŸ“„ è¾“å‡º:\n{block.text}\n")
```

**æµå¼è¾“å‡ºä¸­çš„æ€ç»´å—**:
```python
async for event in client.messages.stream(
    model="claude-opus-4.5",
    thinking={"type": "enabled", "budget_tokens": 1500},
    messages=[...]
):
    if event.type == "content_block_start":
        if event.content_block.type == "thinking":
            print("\n--- æ€è€ƒå¼€å§‹ ---")
    
    elif event.type == "content_block_delta":
        if hasattr(event.delta, 'thinking'):
            print(event.delta.thinking, end="", flush=True)
        elif hasattr(event.delta, 'text'):
            print(event.delta.text, end="", flush=True)
    
    elif event.type == "content_block_stop":
        print("\n--- å—ç»“æŸ ---")
```

#### é«˜çº§ç‰¹æ€§ï¼šæ€ç»´é“¾å¼•å¯¼

**é€šè¿‡ç¤ºä¾‹å¼•å¯¼æ€ç»´æ¨¡å¼**:
```python
messages = [
    {
        "role": "user",
        "content": "åˆ†æè¿™æ®µä»£ç çš„æ—¶é—´å¤æ‚åº¦"
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "thinking",
                "thinking": "æˆ‘åº”è¯¥ï¼š1) è¯†åˆ«å¾ªç¯ç»“æ„ 2) åˆ†æåµŒå¥—æ·±åº¦ 3) è€ƒè™‘é€’å½’å¤æ‚åº¦"
            },
            {
                "type": "text",
                "text": "åŸºäºä»¥ä¸Šæ€è€ƒï¼Œæ—¶é—´å¤æ‚åº¦ä¸º..."
            }
        ]
    },
    {
        "role": "user",
        "content": "ç°åœ¨åˆ†æè¿™æ®µæ–°ä»£ç "  # Claudeå°†é‡‡ç”¨ç±»ä¼¼çš„æ€è€ƒæ¨¡å¼
    }
]
```

#### äº¤é”™æ€è€ƒç¤ºä¾‹ï¼ˆå·¥å…·è°ƒç”¨ + æ¨ç†ï¼‰

```python
response = client.messages.create(
    model="claude-opus-4.5",
    thinking={"type": "enabled"},
    tools=[
        {
            "name": "search_database",
            "description": "æœç´¢æ•°æ®åº“",
            "input_schema": {...}
        },
        {
            "name": "analyze_data",
            "description": "åˆ†ææ•°æ®",
            "input_schema": {...}
        }
    ],
    messages=[
        {"role": "user", "content": "æ‰¾å‡ºé”€å”®é¢ä¸‹é™çš„åŸå› "}
    ]
)

# å“åº”å¯èƒ½åŒ…å«ï¼š
# 1. ThinkingBlock: "æˆ‘éœ€è¦å…ˆæŸ¥è¯¢æœ€è¿‘çš„é”€å”®æ•°æ®"
# 2. ToolUseBlock: search_database(query="last_30_days_sales")
# 3. ThinkingBlock: "æ•°æ®æ˜¾ç¤ºå‘¨æœ«é”€å”®é¢å¼‚å¸¸ä½ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ"
# 4. ToolUseBlock: analyze_data(segment="weekend")
# 5. TextBlock: "åŸå› æ˜¯å‘¨æœ«é…é€æœåŠ¡æš‚åœå¯¼è‡´..."
```

#### æ€ç»´é“¾æŒä¹…åŒ–

**Opus 4.5+ è‡ªåŠ¨ä¿ç•™å†å²æ€ç»´å—**:
```python
# é¦–è½®å¯¹è¯
response1 = client.messages.create(
    model="claude-opus-4.5",
    thinking={"type": "enabled"},
    messages=[{"role": "user", "content": "è®¾è®¡æ•°æ®åº“schema"}]
)

# åç»­å¯¹è¯ä¼šè‡ªåŠ¨åŒ…å«ä¹‹å‰çš„æ€ç»´é“¾
# æœ‰åŠ©äºæ¨ç†è¿ç»­æ€§å’Œç¼“å­˜ä¼˜åŒ–
response2 = client.messages.create(
    model="claude-opus-4.5",
    thinking={"type": "enabled"},
    messages=[
        {"role": "user", "content": "è®¾è®¡æ•°æ®åº“schema"},
        {"role": "assistant", "content": response1.content},
        {"role": "user", "content": "ç°åœ¨æ·»åŠ ç´¢å¼•ä¼˜åŒ–"}
    ]
)
```

---

### 2.3 æ€ç»´é“¾åŠŸèƒ½å¯¹æ¯”

| ç‰¹æ€§ | Kimi K2 Thinking | Claude Opus 4.5 |
|-----|------------------|-----------------|
| **å­—æ®µåç§°** | `reasoning_content` | `thinking` (ThinkingBlock) |
| **æ¨¡å‹è¦æ±‚** | kimi-k2-thinking | claude-opus-4.5+ |
| **å¯ç”¨æ–¹å¼** | æ¨¡å‹è‡ªåŠ¨å¯ç”¨ | éœ€æ˜¾å¼è®¾ç½® `thinking` å‚æ•° |
| **Tokené¢„ç®—æ§åˆ¶** | é€šè¿‡ `max_tokens` | é€šè¿‡ `budget_tokens` |
| **å†å²ä¿ç•™** | éœ€æ‰‹åŠ¨ç®¡ç† | âœ… è‡ªåŠ¨ä¿ç•™ |
| **å·¥å…·è°ƒç”¨é—´æ¨ç†** | âœ… æ”¯æŒ | âœ… æ”¯æŒï¼ˆInterleaved Thinkingï¼‰ |
| **å¼•å¯¼èƒ½åŠ›** | âŒ | âœ… å¯é€šè¿‡ç¤ºä¾‹å¼•å¯¼ |
| **æµå¼è¾“å‡º** | âœ… å…ˆäºcontentè¾“å‡º | âœ… ç‹¬ç«‹content_block |

---

## ä¸‰ã€Tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆToken Usage Statisticsï¼‰

### 3.1 Kimi API Tokenç»Ÿè®¡

#### ä¸‰ç§ç»Ÿè®¡æ–¹å¼

**1. é¢„ä¼°Tokenæ•°é‡ï¼ˆè°ƒç”¨å‰ï¼‰**

**ç«¯ç‚¹**: `POST /v1/tokenizers/estimate-token-count`

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_KIMI_API_KEY",
    base_url="https://api.moonshot.cn/v1"
)

# ä¼°ç®—Tokenæ¶ˆè€—
estimate_response = client.post(
    "/v1/tokenizers/estimate-token-count",
    json={
        "model": "moonshot-v1-32k",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
            {"role": "user", "content": "è¯·åˆ†æè¿™æ®µé•¿è¾¾5000å­—çš„æ–‡æœ¬..."}
        ]
    }
)

total_tokens_estimate = estimate_response.json()["total_tokens"]
print(f"é¢„ä¼°æ¶ˆè€—: {total_tokens_estimate} tokens")

# æ ¹æ®æ¨¡å‹æœ€å¤§Tokenæ•°è®¾ç½® max_tokens
max_output = 32000 - total_tokens_estimate - 100  # ç•™100 buffer
```

**2. å®é™…ä½¿ç”¨ç»Ÿè®¡ï¼ˆéæµå¼ï¼‰**

```python
response = client.chat.completions.create(
    model="moonshot-v1-8k",
    messages=[...],
    stream=False
)

usage = response.usage
print(f"è¾“å…¥: {usage.prompt_tokens} tokens")
print(f"è¾“å‡º: {usage.completion_tokens} tokens")
print(f"æ€»è®¡: {usage.total_tokens} tokens")
```

**3. æµå¼è¾“å‡ºä¸­çš„Tokenç»Ÿè®¡**

```python
total_tokens = 0
stream = client.chat.completions.create(
    model="moonshot-v1-8k",
    messages=[...],
    stream=True,
    stream_options={"include_usage": True}  # å…³é”®ï¼šå¯ç”¨usageç»Ÿè®¡
)

for chunk in stream:
    # å¤„ç†å†…å®¹
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
    
    # æå–æœ€ç»ˆçš„usageä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªchunkï¼‰
    if hasattr(chunk, 'usage') and chunk.usage:
        total_tokens = chunk.usage.total_tokens
        print(f"\næ€»Tokenæ¶ˆè€—: {total_tokens}")
```

#### Tokenè®¡è´¹è¯´æ˜

**è®¡è´¹å…¬å¼**:
```
æ€»è´¹ç”¨ = (è¾“å…¥Tokens Ã— è¾“å…¥å•ä»·) + (è¾“å‡ºTokens Ã— è¾“å‡ºå•ä»·)
```

**ä»·æ ¼å‚è€ƒ** (æˆªè‡³2026-01):
| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | æœ€å¤§ä¸Šä¸‹æ–‡ |
|-----|---------|---------|-----------|
| moonshot-v1-8k | Â¥0.012/1K | Â¥0.012/1K | 8K |
| moonshot-v1-32k | Â¥0.024/1K | Â¥0.024/1K | 32K |
| moonshot-v1-128k | Â¥0.060/1K | Â¥0.060/1K | 128K |
| kimi-k2-thinking | (åŒmoonshot-v1-128k) | - | 256K |

âš ï¸ **æ³¨æ„**: `reasoning_content` çš„Token **è®¡å…¥** `completion_tokens`

---

### 3.2 Claude Agent SDK Tokenç»Ÿè®¡

#### æ ¸å¿ƒç»Ÿè®¡ç»“æ„

**1. Messageçº§åˆ«çš„Usageå¯¹è±¡**

```python
from anthropic import Anthropic

client = Anthropic()
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "è§£é‡Šé‡å­çº ç¼ "}
    ]
)

# æå–ä½¿ç”¨ç»Ÿè®¡
usage = response.usage
print(f"è¾“å…¥Tokens: {usage.input_tokens}")
print(f"è¾“å‡ºTokens: {usage.output_tokens}")

# å¦‚æœä½¿ç”¨äº†ç¼“å­˜
if hasattr(usage, 'cache_creation_input_tokens'):
    print(f"ç¼“å­˜åˆ›å»º: {usage.cache_creation_input_tokens}")
if hasattr(usage, 'cache_read_input_tokens'):
    print(f"ç¼“å­˜è¯»å–: {usage.cache_read_input_tokens}")
```

**2. æµå¼è¾“å‡ºä¸­çš„Tokenç»Ÿè®¡**

```python
async with client.messages.stream(
    model="claude-3-7-sonnet-20250219",
    max_tokens=1024,
    messages=[...]
) as stream:
    async for event in stream:
        if event.type == "content_block_delta":
            print(event.delta.text, end="", flush=True)
    
    # æµç»“æŸåè·å–å®Œæ•´usage
    final_message = await stream.get_final_message()
    usage = final_message.usage
    print(f"\næ¶ˆè€—: {usage.input_tokens + usage.output_tokens} tokens")
```

**3. å¤šæ¨¡å‹ä½¿ç”¨åœºæ™¯çš„ç»Ÿè®¡ï¼ˆSubagentsï¼‰**

```python
# ä¸»Agentè°ƒç”¨å¤šä¸ªSubagentæ—¶
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    tools=[...],  # åŒ…å«subagentå·¥å…·
    messages=[...]
)

# modelUsageå­—æ®µæä¾›æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†ç»Ÿè®¡
if hasattr(response, 'model_usage'):
    for model_name, usage_data in response.model_usage.items():
        print(f"æ¨¡å‹: {model_name}")
        print(f"  è¾“å…¥: {usage_data['input_tokens']}")
        print(f"  è¾“å‡º: {usage_data['output_tokens']}")
        print(f"  æˆæœ¬: ${usage_data.get('total_cost_usd', 'N/A')}")
```

#### é«˜çº§ç‰¹æ€§ï¼šToken Counting API

**é¢„ä¼°Tokenæ¶ˆè€—ï¼ˆè°ƒç”¨å‰ï¼‰**:
```python
# ä½¿ç”¨ä¸“é—¨çš„Tokenè®¡æ•°ç«¯ç‚¹
count_response = client.messages.count_tokens(
    model="claude-3-7-sonnet-20250219",
    system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥åŠ©æ‰‹",
    messages=[
        {"role": "user", "content": "å®¡æŸ¥è¿™æ®µä»£ç ..."}
    ],
    tools=[
        {
            "name": "run_linter",
            "description": "è¿è¡Œä»£ç æ£€æŸ¥å·¥å…·",
            "input_schema": {...}
        }
    ]
)

estimated_input_tokens = count_response.input_tokens
print(f"é¢„ä¼°è¾“å…¥Token: {estimated_input_tokens}")

# æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£è°ƒæ•´ max_tokens
context_window = 200000  # Claude 3.7 Sonnet
max_output = min(4096, context_window - estimated_input_tokens - 1000)
```

**æ³¨æ„äº‹é¡¹**:
- è¿”å›çš„æ˜¯**ä¼°ç®—å€¼**ï¼Œå®é™…æ¶ˆè€—å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼ˆÂ±2%ï¼‰
- Anthropicä¸ä¼šå¯¹ç³»ç»Ÿä¼˜åŒ–è‡ªåŠ¨æ·»åŠ çš„Tokenè®¡è´¹
- åŒ…æ‹¬system promptã€toolsã€imagesã€PDFsåœ¨å†…çš„æ‰€æœ‰è¾“å…¥

#### è¯¦ç»†Usageå­—æ®µè¯´æ˜

```python
class UsageInfo:
    input_tokens: int                    # åŸºç¡€è¾“å…¥Tokenæ•°
    output_tokens: int                   # è¾“å‡ºTokenæ•°
    cache_creation_input_tokens: int     # åˆ›å»ºç¼“å­˜æ¶ˆè€—çš„Token
    cache_read_input_tokens: int         # ä»ç¼“å­˜è¯»å–èŠ‚çœçš„Token
    service_tier: str                    # æœåŠ¡ç­‰çº§ï¼ˆscale/defaultï¼‰
    total_cost_usd: float                # æ€»æˆæœ¬ï¼ˆç¾å…ƒï¼‰
```

**ç¼“å­˜ä¼˜åŒ–çš„Tokenè®¡ç®—**:
```python
# å¯ç”¨Prompt Caching
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    system=[
        {
            "type": "text",
            "text": "é•¿ç³»ç»Ÿæç¤º...",
            "cache_control": {"type": "ephemeral"}  # å¯ç”¨ç¼“å­˜
        }
    ],
    messages=[...]
)

usage = response.usage

# é¦–æ¬¡è°ƒç”¨
print(f"ç¼“å­˜åˆ›å»º: {usage.cache_creation_input_tokens}")
# åç»­è°ƒç”¨
print(f"ç¼“å­˜è¯»å–: {usage.cache_read_input_tokens}")  # é€šå¸¸èŠ‚çœ90%æˆæœ¬
```

---

### 3.3 Tokenç»Ÿè®¡å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | Kimi API | Claude Agent SDK |
|-----|---------|-----------------|
| **é¢„ä¼°API** | `/v1/tokenizers/estimate-token-count` | `messages.count_tokens()` |
| **å®é™…ç»Ÿè®¡ä½ç½®** | `response.usage` | `message.usage` |
| **æµå¼ç»Ÿè®¡** | éœ€ `stream_options={"include_usage": True}` | `stream.get_final_message().usage` |
| **ç¼“å­˜ç»Ÿè®¡** | âŒ ä¸æ”¯æŒ | âœ… `cache_creation/read_input_tokens` |
| **å¤šæ¨¡å‹ç»Ÿè®¡** | éœ€æ‰‹åŠ¨ç´¯åŠ  | âœ… `modelUsage` å­—æ®µ |
| **æˆæœ¬è®¡ç®—** | éœ€è‡ªè¡Œè®¡ç®— | âœ… å¯é€‰ `total_cost_usd` |
| **æ€ç»´é“¾Token** | è®¡å…¥ `completion_tokens` | è®¡å…¥ `output_tokens` |
| **ç²¾ç¡®åº¦** | é«˜ï¼ˆä¸€è‡´æ€§å¼ºï¼‰ | ä¼°ç®—å€¼ï¼ˆÂ±2%ï¼‰ |

---

## å››ã€ç»¼åˆå®ç°å»ºè®®

### 4.1 é€‰æ‹©æµå¼è¾“å‡ºçš„åœºæ™¯

**æ¨èä½¿ç”¨æµå¼è¾“å‡º**:
- âœ… é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆ>500 tokensï¼‰
- âœ… å®æ—¶äº¤äº’åº”ç”¨ï¼ˆèŠå¤©æœºå™¨äººã€ä»£ç åŠ©æ‰‹ï¼‰
- âœ… éœ€è¦æ˜¾ç¤ºè¿›åº¦çš„ä»»åŠ¡
- âœ… é¿å…ç½‘ç»œè¶…æ—¶ï¼ˆç”Ÿæˆæ—¶é—´>30ç§’ï¼‰

**å¯é€‰æ‹©éæµå¼**:
- æ‰¹å¤„ç†ä»»åŠ¡
- éœ€è¦åŸå­æ€§äº‹åŠ¡ï¼ˆå…¨éƒ¨æˆåŠŸæˆ–å…¨éƒ¨å¤±è´¥ï¼‰
- ç®€çŸ­å›å¤ï¼ˆ<100 tokensï¼‰

### 4.2 æ€ç»´é“¾å±•ç¤ºçš„æœ€ä½³å®è·µ

**Kimi K2 Thinking**:
```python
# æœ€ä½³é…ç½®
config = {
    "model": "kimi-k2-thinking",
    "stream": True,              # é¿å…è¶…æ—¶
    "max_tokens": 16000,         # ç¡®ä¿å®Œæ•´è¾“å‡º
    "temperature": 1.0,          # æœ€ä½³æ¨ç†æ€§èƒ½
}

# UIå±•ç¤ºå»ºè®®
def display_with_thinking(reasoning, content):
    print("="*50)
    print("ğŸ’­ æ€è€ƒè¿‡ç¨‹:")
    print("-"*50)
    print(reasoning)
    print("="*50)
    print("âœ… ç»“è®º:")
    print(content)
```

**Claude Thinking Blocks**:
```python
# å¯ç”¨æ‰©å±•æ€è€ƒ
thinking_config = {
    "type": "enabled",
    "budget_tokens": 2000  # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
}

# åŒºåˆ†æ˜¾ç¤º
def render_content_blocks(blocks):
    for block in blocks:
        if block.type == "thinking":
            print(f"ğŸ§  [å†…éƒ¨æ¨ç†]\n{block.thinking}\n")
        elif block.type == "text":
            print(f"ğŸ“ [å›å¤]\n{block.text}\n")
        elif block.type == "tool_use":
            print(f"ğŸ”§ [å·¥å…·] {block.name}: {block.input}\n")
```

### 4.3 Tokenä¼˜åŒ–ç­–ç•¥

**1. ä½¿ç”¨é¢„ä¼°APIé¿å…è¶…é™**
```python
# Kimiæ–¹å¼
estimate = kimi_client.estimate_tokens(messages)
if estimate > 30000:  # moonshot-v1-32kä¸Šé™
    # å‹ç¼©æ¶ˆæ¯å†å²
    messages = compact_messages(messages)

# Claudeæ–¹å¼
estimate = claude_client.messages.count_tokens(...)
if estimate > 190000:  # Claude 3.7ä¸Šä¸‹æ–‡200K
    # è§¦å‘æ€»ç»“æœºåˆ¶
    messages = summarize_conversation(messages)
```

**2. æµå¼è¾“å‡ºä¸­å®æ—¶ç›‘æ§**
```python
token_count = 0
for chunk in stream:
    token_count += len(chunk.choices[0].delta.content or "")
    
    # åŠ¨æ€è°ƒæ•´ç­–ç•¥
    if token_count > threshold:
        print("[è­¦å‘Š] Tokenæ¶ˆè€—æ¥è¿‘ä¸Šé™")
```

**3. åˆ©ç”¨ç¼“å­˜ï¼ˆClaudeï¼‰**
```python
# å°†é•¿æ–‡æ¡£è®¾ä¸ºå¯ç¼“å­˜
system_prompt = [{
    "type": "text",
    "text": long_documentation,
    "cache_control": {"type": "ephemeral"}  # 90%æˆæœ¬èŠ‚çœ
}]
```

### 4.4 é”™è¯¯å¤„ç†å’Œé‡è¯•

**æµå¼è¾“å‡ºçš„é”™è¯¯æ¢å¤**:
```python
import asyncio

async def resilient_stream(client, **kwargs):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async for chunk in client.stream(**kwargs):
                yield chunk
            break  # æˆåŠŸå®Œæˆ
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"æµå¼è¾“å‡ºä¸­æ–­ï¼Œ{wait_time}ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
            else:
                raise  # æœ€ç»ˆå¤±è´¥
```

---

## äº”ã€å®é™…åº”ç”¨ä»£ç æ¨¡æ¿

### 5.1 å®Œæ•´çš„Kimi K2 Thinkingåº”ç”¨

```python
from openai import OpenAI
from typing import Dict, List

class KimiThinkingAgent:
    def __init__(self, api_key: str):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.moonshot.cn/v1"
        )
    
    def estimate_cost(self, messages: List[Dict]) -> Dict:
        """é¢„ä¼°Tokenæ¶ˆè€—å’Œæˆæœ¬"""
        response = self.client.post(
            "/v1/tokenizers/estimate-token-count",
            json={"model": "kimi-k2-thinking", "messages": messages}
        )
        total_tokens = response.json()["total_tokens"]
        
        # å‡è®¾è¾“å‡ºä¸è¾“å…¥ç›¸å½“
        estimated_total = total_tokens * 2
        cost = (estimated_total / 1000) * 0.060  # Â¥0.060/1K
        
        return {
            "input_tokens": total_tokens,
            "estimated_total": estimated_total,
            "estimated_cost_cny": cost
        }
    
    def query_with_thinking(
        self, 
        prompt: str,
        show_thinking: bool = True
    ) -> Dict:
        """æ‰§è¡Œæ¨ç†æŸ¥è¯¢å¹¶åˆ†ç¦»æ€ç»´é“¾"""
        messages = [{"role": "user", "content": prompt}]
        
        # é¢„ä¼°æˆæœ¬
        estimate = self.estimate_cost(messages)
        print(f"é¢„ä¼°æ¶ˆè€—: {estimate['estimated_total']} tokens (çº¦Â¥{estimate['estimated_cost_cny']:.4f})")
        
        stream = self.client.chat.completions.create(
            model="kimi-k2-thinking",
            messages=messages,
            stream=True,
            max_tokens=16000,
            temperature=1.0,
            stream_options={"include_usage": True}
        )
        
        reasoning_parts = []
        content_parts = []
        usage = None
        
        for chunk in stream:
            delta = chunk.choices[0].delta
            
            # æå–æ€ç»´é“¾
            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                reasoning_parts.append(delta.reasoning_content)
                if show_thinking:
                    print(f"ğŸ’­ {delta.reasoning_content}", end="", flush=True)
            
            # æå–å›å¤
            if hasattr(delta, 'content') and delta.content:
                content_parts.append(delta.content)
                print(f"{delta.content}", end="", flush=True)
            
            # æå–usage
            if hasattr(chunk, 'usage') and chunk.usage:
                usage = chunk.usage
        
        print()  # æ¢è¡Œ
        
        return {
            "reasoning": ''.join(reasoning_parts),
            "content": ''.join(content_parts),
            "usage": {
                "prompt_tokens": usage.prompt_tokens if usage else 0,
                "completion_tokens": usage.completion_tokens if usage else 0,
                "total_tokens": usage.total_tokens if usage else 0
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
agent = KimiThinkingAgent(api_key="YOUR_API_KEY")
result = agent.query_with_thinking(
    "è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ç™¾ä¸‡çº§å¹¶å‘çš„å¾®æœåŠ¡æ¶æ„ï¼Œå¹¶åˆ†ææ½œåœ¨çš„å•ç‚¹æ•…éšœ"
)

print("\n" + "="*60)
print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
print(f"æ€»Token: {result['usage']['total_tokens']}")
print(f"æ€ç»´é“¾é•¿åº¦: {len(result['reasoning'])} å­—ç¬¦")
print(f"å›å¤é•¿åº¦: {len(result['content'])} å­—ç¬¦")
```

### 5.2 å®Œæ•´çš„Claude Agent SDKåº”ç”¨

```python
from anthropic import Anthropic
from typing import List, Dict, AsyncIterator

class ClaudeThinkingAgent:
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
        self.model = "claude-opus-4.5"
    
    def estimate_tokens(
        self, 
        messages: List[Dict],
        system: str = None,
        tools: List[Dict] = None
    ) -> int:
        """é¢„ä¼°Tokenæ¶ˆè€—"""
        response = self.client.messages.count_tokens(
            model=self.model,
            system=system,
            messages=messages,
            tools=tools or []
        )
        return response.input_tokens
    
    async def stream_with_thinking(
        self,
        prompt: str,
        thinking_budget: int = 2000,
        show_thinking: bool = True
    ) -> Dict:
        """æµå¼è¾“å‡ºå«æ€ç»´é“¾çš„å“åº”"""
        
        messages = [{"role": "user", "content": prompt}]
        
        # é¢„ä¼°
        estimated = self.estimate_tokens(messages)
        print(f"é¢„ä¼°è¾“å…¥: {estimated} tokens\n")
        
        thinking_parts = []
        text_parts = []
        tool_uses = []
        
        async with self.client.messages.stream(
            model=self.model,
            max_tokens=4096,
            thinking={
                "type": "enabled",
                "budget_tokens": thinking_budget
            },
            messages=messages
        ) as stream:
            async for event in stream:
                if event.type == "content_block_start":
                    if event.content_block.type == "thinking":
                        if show_thinking:
                            print("\nğŸ§  [æ€è€ƒä¸­]")
                    elif event.content_block.type == "text":
                        print("\nğŸ“ [å›å¤]")
                
                elif event.type == "content_block_delta":
                    if hasattr(event.delta, 'thinking'):
                        thinking_parts.append(event.delta.thinking)
                        if show_thinking:
                            print(event.delta.thinking, end="", flush=True)
                    
                    elif hasattr(event.delta, 'text'):
                        text_parts.append(event.delta.text)
                        print(event.delta.text, end="", flush=True)
                
                elif event.type == "content_block_stop":
                    print()  # æ¢è¡Œ
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_message = await stream.get_final_message()
            usage = final_message.usage
        
        return {
            "thinking": ''.join(thinking_parts),
            "content": ''.join(text_parts),
            "tool_uses": tool_uses,
            "usage": {
                "input_tokens": usage.input_tokens,
                "output_tokens": usage.output_tokens,
                "total_tokens": usage.input_tokens + usage.output_tokens,
                "cache_read": getattr(usage, 'cache_read_input_tokens', 0)
            }
        }
    
    def query_with_tools_and_thinking(
        self,
        prompt: str,
        tools: List[Dict]
    ) -> Dict:
        """å¸¦å·¥å…·è°ƒç”¨çš„æ¨ç†"""
        response = self.client.messages.create(
            model=self.model,
            max_tokens=4096,
            thinking={"type": "enabled", "budget_tokens": 1500},
            tools=tools,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # è§£æå“åº”
        thinking_blocks = []
        text_blocks = []
        tool_blocks = []
        
        for block in response.content:
            if block.type == "thinking":
                thinking_blocks.append(block.thinking)
            elif block.type == "text":
                text_blocks.append(block.text)
            elif block.type == "tool_use":
                tool_blocks.append({
                    "name": block.name,
                    "input": block.input,
                    "id": block.id
                })
        
        return {
            "thinking": "\n".join(thinking_blocks),
            "content": "\n".join(text_blocks),
            "tool_uses": tool_blocks,
            "usage": {
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
import asyncio

async def main():
    agent = ClaudeThinkingAgent(api_key="YOUR_API_KEY")
    
    result = await agent.stream_with_thinking(
        prompt="åˆ†æé‡å­è®¡ç®—å¯¹ç°ä»£å¯†ç å­¦çš„å¨èƒï¼Œå¹¶æå‡ºåé‡å­å¯†ç å­¦è§£å†³æ–¹æ¡ˆ",
        thinking_budget=3000,
        show_thinking=True
    )
    
    print("\n" + "="*60)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»Token: {result['usage']['total_tokens']}")
    print(f"  - è¾“å…¥: {result['usage']['input_tokens']}")
    print(f"  - è¾“å‡º: {result['usage']['output_tokens']}")
    print(f"  - ç¼“å­˜è¯»å–: {result['usage']['cache_read']}")

asyncio.run(main())
```

---

## å…­ã€æ€»ç»“ä¸å»ºè®®

### 6.1 åŠŸèƒ½å¯¹æ¯”çŸ©é˜µ

| ç»´åº¦ | Kimi API | Claude Agent SDK | æ¨èåœºæ™¯ |
|-----|---------|-----------------|---------|
| **æµå¼è¾“å‡ºæ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | Kimiæ›´ç®€å• |
| **æ€ç»´é“¾æ§åˆ¶** | â­â­â­ | â­â­â­â­â­ | Claudeæ›´çµæ´» |
| **Tokenç»Ÿè®¡ç²¾åº¦** | â­â­â­â­â­ | â­â­â­â­ | Kimiæ›´ç²¾ç¡® |
| **ç¼“å­˜ä¼˜åŒ–** | âŒ | âœ… | Claudeæˆæœ¬ä¼˜åŠ¿ |
| **å·¥å…·è°ƒç”¨æµå¼** | âŒ | âœ… | Claude Agentåœºæ™¯ |
| **ä¸­æ–‡æ”¯æŒ** | â­â­â­â­â­ | â­â­â­â­ | KimiåŸç”Ÿä¼˜åŠ¿ |
| **é•¿ä¸Šä¸‹æ–‡** | 256K | 200K | Kimiç•¥èƒœ |
| **ä»·æ ¼** | Â¥0.06/1K | ~$3/1M (~Â¥0.02/1K) | Claudeæ›´ä¾¿å®œ |

### 6.2 å®æ–½è·¯çº¿å›¾

**Phase 1: åŸºç¡€æµå¼è¾“å‡º (Week 1)**
- [ ] é›†æˆKimi/Claudeæµå¼API
- [ ] å®ç°åŸºç¡€äº‹ä»¶å¤„ç†
- [ ] æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶

**Phase 2: æ€ç»´é“¾å±•ç¤º (Week 2)**
- [ ] è§£æ `reasoning_content` / `ThinkingBlock`
- [ ] è®¾è®¡UIåˆ†ç¦»å±•ç¤ºæ€ç»´ä¸å›å¤
- [ ] å®ç°æ€ç»´é“¾æŠ˜å /å±•å¼€

**Phase 3: Tokenä¼˜åŒ– (Week 3)**
- [ ] é›†æˆTokené¢„ä¼°API
- [ ] å®ç°æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿
- [ ] æ·»åŠ è‡ªåŠ¨å‹ç¼©æœºåˆ¶

**Phase 4: é«˜çº§ç‰¹æ€§ (Week 4+)**
- [ ] å®ç°Prompt Cachingï¼ˆClaudeï¼‰
- [ ] å¤šæ¨¡å‹ç»Ÿè®¡èšåˆ
- [ ] ç»†ç²’åº¦å·¥å…·æµå¼ï¼ˆClaudeï¼‰

### 6.3 æŠ€æœ¯é€‰å‹å»ºè®®

**é€‰æ‹©Kimiå¦‚æœ**:
- âœ… ä¸»è¦å¤„ç†ä¸­æ–‡ä»»åŠ¡
- âœ… éœ€è¦è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ256Kï¼‰
- âœ… è¿½æ±‚Tokenç»Ÿè®¡ç²¾ç¡®æ€§
- âœ… å›¢é˜Ÿç†Ÿæ‚‰OpenAI SDK
- âœ… éœ€è¦å¼ºAgentèƒ½åŠ›ï¼ˆK2 Thinkingï¼‰

**é€‰æ‹©Claudeå¦‚æœ**:
- âœ… éœ€è¦å¤æ‚çš„å·¥å…·ç¼–æ’
- âœ… é‡è§†æˆæœ¬ä¼˜åŒ–ï¼ˆç¼“å­˜ï¼‰
- âœ… éœ€è¦å¼•å¯¼æ€ç»´æ¨¡å¼
- âœ… å¤šæ¨¡å‹åä½œåœºæ™¯
- âœ… éœ€è¦ç»†ç²’åº¦æ§åˆ¶ï¼ˆthinking budgetï¼‰

---

## é™„å½•

### A. å®Œæ•´APIå‚è€ƒ

**Kimi APIæ–‡æ¡£**: [https://platform.moonshot.cn/docs](https://platform.moonshot.cn/docs)  
**Claude APIæ–‡æ¡£**: [https://docs.anthropic.com/claude/docs](https://docs.anthropic.com/claude/docs)  
**Claude Agent SDK**: [https://docs.anthropic.com/claude/docs/claude-agent-sdk](https://docs.anthropic.com/claude/docs/claude-agent-sdk)

### B. å¸¸è§é—®é¢˜

**Q: æµå¼è¾“å‡ºä¼šå¢åŠ æˆæœ¬å—ï¼Ÿ**  
A: ä¸ä¼šã€‚Tokenè®¡è´¹ä¸æ˜¯å¦æµå¼æ— å…³ï¼Œåªä¸å®é™…ç”Ÿæˆçš„Tokenæ•°é‡æœ‰å…³ã€‚

**Q: æ€ç»´é“¾çš„Tokenå¯ä»¥ä¸è®¡è´¹å—ï¼Ÿ**  
A: ä¸å¯ä»¥ã€‚æ— è®ºæ˜¯Kimiçš„`reasoning_content`è¿˜æ˜¯Claudeçš„`ThinkingBlock`ï¼Œå…¶Tokenéƒ½ä¼šè®¡å…¥æ€»æ¶ˆè€—ã€‚

**Q: å¦‚ä½•æœ€å¤§åŒ–ç¼“å­˜æ•ˆç›Šï¼ˆClaudeï¼‰ï¼Ÿ**  
A: å°†ä¸å˜çš„é•¿æ–‡æœ¬ï¼ˆå¦‚system promptã€æ–‡æ¡£ï¼‰æ ‡è®°ä¸º`cache_control: ephemeral`ï¼Œå¹¶åœ¨åç»­è¯·æ±‚ä¸­ä¿æŒä¸€è‡´ã€‚

**Q: Kimi K2 Thinkingçš„max_tokensä¸ºä»€ä¹ˆå»ºè®®>=16000ï¼Ÿ**  
A: å› ä¸ºæ¨¡å‹éœ€è¦è¶³å¤Ÿç©ºé—´åŒæ—¶è¾“å‡ºæ€ç»´é“¾å’Œæœ€ç»ˆå›å¤ï¼Œè¿‡å°ä¼šå¯¼è‡´æˆªæ–­ã€‚

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2026-01-12 08:05  
**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**: æ ¹æ®æœ¬æŠ¥å‘Šé€‰æ‹©é€‚åˆçš„SDKå¹¶å¼€å§‹é›†æˆPOC
